{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Cloud Space","text":"<p>Website : www.cloudspaceacademy.com   info@cloudspaceacademy.com   Phone: +1-855-200-7653   WhatsApp: +1-571-454-4691  Facebook </p> <p>At CloudSpace, our students' success is our top priority. We understand it can be difficult to study alone once the BootCamp is over. CloudSpace is committed to providing you with the necessary support until you land that dream job. The FREE follow-up program is a dedicated place to keep learning with an industry leader DevOps instructor until you land a job. All this free of charge as long as you have completed the AWS Cloud &amp; DevOps BootCamp with us. So after completing the 6 months AWS &amp; DevOps Bootcamp, you can attend the Free Follow-up Program free of charge on a weekly basis to keep working on interview preparation and DevOps projects with an experienced instructor. So you will be allowed to attend this post-bootcamp until you land a job.</p> <p>Follow-Up Program Schedule: Thursdays (8PM-9PM) Interview Prep - Sundays (1PM-3PM) Projects Hands On</p>"},{"location":"3-tier-terraform-on-aws/","title":"Multi-Tier Architecture on AWS using Terraform","text":""},{"location":"3-tier-terraform-on-aws/#overview","title":"\ud83d\ude80 Overview:","text":"<p>The Multi-Tier Architecture project on AWS using Terraform aims to create a scalable and resilient infrastructure that leverages the power of Amazon Web Services (AWS) cloud platform. This project utilizes Terraform, an Infrastructure as Code (IaC) tool, to provision and manage the infrastructure components, enabling automation, repeatability, and scalability. The primary objective of this project is to design and deploy a multi-tier architecture on AWS that consists of multiple layers, including presentation, application, and database tiers. Each tier is deployed across multiple Availability Zones (AZs) for high availability and fault tolerance.</p>"},{"location":"3-tier-terraform-on-aws/#problem-statement","title":"\ud83d\udd27 Problem Statement","text":"<p>Terraform is an IaC software tool that provides a consistent command line interface (CLI) workflow to manage hundreds of cloud services. Terraform codifies cloud APIs into declarative configuration files. In this specific case you need to create foundation Networking(VPC, Subnets, route table, IGW, NAT Gateway...), virtual machines (EC2 instances), databases (RDS), distribution of traffic (ELB) and Auto-scaling (ASG). Terraform will automatically use the configuration files to provide the infrastructure resources and run application needed. Terraform will use his deployment to provide all AWS needed elements avoiding us to use the console and it will automate the setup, ensuring consistency and reducing human error.</p>"},{"location":"3-tier-terraform-on-aws/#techonology-stack","title":"\ud83d\udcbd Techonology Stack","text":"<p>The architecture consists of the following three tiers:</p> <ul> <li> <p>VPC: AWS VPC</p> </li> <li> <p>AutoScaling: AWS ASG</p> </li> <li> <p>Elastic Load Balancer: AWS ELB</p> </li> <li> <p>Database: AWS RDS</p> </li> <li> <p>File Configuration: Terraform</p> </li> </ul>"},{"location":"3-tier-terraform-on-aws/#architecture-diagram","title":"\ud83d\udccc Architecture Diagram","text":""},{"location":"3-tier-terraform-on-aws/#project-requirements","title":"\ud83c\udf1f Project Requirements","text":"<p>Before you get started, make sure you have the following prerequisites in place:</p> <ul> <li>Terraform installed on your local machine.</li> <li>AWS IAM credentials configured in your text editor. In this case we will use VSCODE.</li> <li>Git installed on your local machine and Github account set up Github</li> <li>Git for cloning the repository.</li> </ul> <p>You must know and understand:</p> <ul> <li>High Availability: The architecture is designed for fault tolerance and redundancy. Reason why resources will be deploy across two avaibility zones to ensure resilience to failures. </li> <li>Scalability: Easily scale the web and application tiers to handle varying workloads automaticaly based on demand.</li> <li>Security: Implementing security best practices such as Security groups and network ACLs are configured to ensure a secure and protected environment.</li> </ul> <p>You must also know Terraform workflow</p> <p></p>"},{"location":"3-tier-terraform-on-aws/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<p>I - Terraform Configuration files</p> <p>Step 1: Provider Configuration</p> <p>Step 2: Variables Configuration</p> <p>Step 3: VPC Configuration</p> <p>Step 4: web tier Configuration</p> <p>Step 5: Application tier Configuration</p> <p>Step 6: Database tier Configuration</p> <p>Step 7: Output Configuration</p> <p>II - Instructions of Deployment</p> <p>Step 8: Clone Repository</p> <p>Step 9: Initialize Folder</p> <p>Step 10: Format Files</p> <p>Step 11: Validate Files</p> <p>Step 12: Plan</p> <p>Step 13: Apply</p> <p>Step 14: Review of Resources</p> <p>Step 15: Destroy</p>"},{"location":"3-tier-terraform-on-aws/#terraform-configuration-files","title":"\u2728Terraform Configuration files","text":"<p>You need to write different files generating resources</p> <p>Step 1:  Provider Configuration</p> <p>Here we declare our cloud provider and we specify the region where we will be launching resources</p> <ul> <li>provider Configuration</li> </ul> <p>Step 2:  Variables Configuration</p> <p>This is where we declare all variables and thier value. It includes</p> <ul> <li>Variables: List of element that can vary or change. They can be reuse values throughout our code without repeating ourselves and help make the code dynamic</li> <li>values: values attributed to each variables.</li> <li>secrets: username and Password for the Database</li> </ul> <p>Reminder: Never push terraform.tfvars and secrets.tfvars file on Github</p> <p>We have </p> <ul> <li>variables Configuration</li> <li>value Configuration</li> <li>Secrets Configuration</li> </ul> <p>Step 3: VPC Configuration</p> <p>This is where you create the basement, foundation and networking where all the resources will be launch. It includes VPC, Subnets, IGW, NatGateway, EIP and Route tables</p> <ul> <li>VPC Configuration</li> </ul> <p>Step 4:  Web Tier Configuration</p> <p>The Web Tier is the entry point for incoming user requests. Resources are launched in the public subnets. It typically includes:</p> <ul> <li>Web Servers: These run your application code that contains the apache which will deploy the index.html located in the user data.</li> <li>Load Balancer: Distributes traffic across multiple web servers running in the public subnets.</li> <li>Auto Scaling: Automatically adjusts the number of web servers based on traffic.</li> <li>Security Groups: Controls incoming and outgoing traffic from outside to the web servers.</li> </ul> <p>Web tier configuration files are :</p> <ul> <li>Web ASG Configuration</li> <li>Web ELB Configuration</li> </ul> <p>Step 5: Application Tier Configuration</p> <p>The Application Tier hosts the application servers responsible for running business logic and interacting with the database tier. Key components include:</p> <ul> <li>Application Servers: These run your application code and can be horizontally scaled.</li> <li>Load Balancer: Distributes traffic to the application servers running in the private subnets.</li> <li>Auto Scaling: Automatically adjusts the number of web servers based on traffic.</li> <li>Security Groups: Controls incoming and outgoing traffic from the web servers to the application servers.</li> </ul> <p>Application Tier Configuration files are: </p> <ul> <li>App ASG Configuration</li> <li>App ELB Configuration</li> </ul> <p>Step 6:  Database Tier Configuration</p> <p>The Database Tier stores and manages our application data. We use Amazon RDS for a managed, a highly available and scalable database to store application data. Key components include:</p> <ul> <li>Subnets groups: List of subnets wherether Server databases will run.</li> <li>Amazon RDS: A managed database service for MySQL/PostgreSQL/SQL Server databases.</li> <li>Security Groups: Control incoming and outgoing traffic to the database.</li> </ul> <p>Database Tier Configuration file:</p> <ul> <li>DB Configuration</li> </ul> <p>Step 7: Output Configuration</p> <p>Know as Output Value : it is a convenient way to get useful information about your infranstructure printed on the CLI. It is showing the ARN, name or ID of a resource. In this case we are bringing out the DNS name of the web application Load balancer.  </p> <ul> <li>Output Configuration</li> </ul>"},{"location":"3-tier-terraform-on-aws/#instructions-of-deployment","title":"\ud83d\udcbc Instructions of Deployment","text":"<p>Follow these steps to deploy the architecture:</p> <p>Step 8: Clone Repository:</p> <p>Clone the repository in your local machine using the command \"git clone\" </p> <p><code>bash    git clone https://github.com/cloudspaceacademy/terraform-on-aws.git</code></p> <p>Step 9: Initialize Folder</p> <p>Initialize the folder containing configuation files that were clone to Terraform and apply the configuration by typing  the following command</p> <p><code>bash    terraform init</code></p> <p>You must see this image</p> <p></p> <p>Step 10: Format Files</p> <p>Apply any changes on files and Review the changes and confirm the good format with command:</p> <p><code>bash    terraform fmt</code></p> <p>Step 11: Validate Files</p> <p>Ensure that every files are syntactically valid and ready to go with the command: </p> <p><code>bash    terraform validate</code></p> <p>If everything is good you will have something like this </p> <p> </p> <p>Step 12: Plan</p> <p>Create an excution plan to provide the achievement of the desired state. It Check and confirm the numbers of resources that will be create. Use command:</p> <p><code>bash    terraform plan</code></p> <p>The list of all resources in stage of creation will appear and you can see all properties(arguments and attributs) of each resouces</p> <p> </p> <p>Step 13: Apply</p> <p>Bring all desired state resources on life. It Launch and create all resources listed in the configuration files. The command to perform the task is:  </p> <p><code>bash    terraform apply -auto-approve</code></p> <p>You will be prompt to type the username and password for the database. After you enter those criticals data the process of creation will start and you will be able to see which resourse is on the way to be create and the time it taking to create.</p> <p> </p> <p>At the end you will recieve a prompt message showing all resources status: created, changed and the numbers of them. </p> <p> </p> <p>Step 14: Review of resources</p> <p>Go back on the console and check all actual state resources one by one to see. You will have</p> <p>VPC</p> <p> </p> <p>Instances running</p> <p> </p> <p>Application Load Balancer</p> <p> </p> <p>Autoscaling groups </p> <p> </p> <p>Database </p> <p> </p> <p>Web page </p> <p> </p> <p>Step 15: Destroy</p> <p>Destroy the terraform managed infrastructure meaning all resourcescreated will be shut down. This action can be done with the command \"terraform destroy\" </p> <p><code>bash    terraform destroy -auto-approve</code></p> <p>At the end you will recieve a prompt message showing all resources has been destroyed</p> <p> </p>"},{"location":"3-tier-terraform-on-aws/#license","title":"\ud83d\udcc4 License","text":"<p>This project is licensed under the CloudSpace Academy License</p>"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/","title":"Creating a Kubernetes Cluster with Minikube","text":"<p>Introduction</p> <p>Minikube is a tool that allows you to run Kubernetes clusters on your local machine. It's great for development, testing, and learning Kubernetes concepts. This guide will walk you through the process of setting up a Kubernetes cluster using Minikube.</p> <p>Prerequisites</p> <p>Hypervisor: Minikube requires a hypervisor to run the virtual machines. Common choices include VirtualBox, Hyper-V, KVM, or Docker. Ensure your preferred hypervisor is installed and properly configured.We use Docker for that </p> <p>Docker | installation guide Click Here</p> <p>kubectl: The kubectl command-line tool is used to interact with your Kubernetes cluster. Make sure it's installed on your machine.</p> <p>Kubectl | installation guide Click Here</p>"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#install-minikube","title":"Install Minikube","text":"<p>Visit the Minikube Installation Guide and follow the instructions for your operating system to install Minikube.</p> <p>Minikube | installation guide Click Here</p>"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#start-minikube","title":"Start Minikube","text":"<p>Open a terminal and run the following command to start Minikube:</p> <p><code>minikube start</code></p> <p>This command will download the Minikube ISO, start a virtual machine, and set up the Kubernetes cluster.</p>"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#verify-cluster-status","title":"Verify Cluster Status","text":"<p>After Minikube has started, you can check the cluster status using:</p> <p><code>kubectl cluster-info</code></p> <p>This command will display information about the cluster, including the Kubernetes master and services.</p>"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#check-nodes","title":"Check Nodes","text":"<p>Verify that Minikube has created a node for your cluster:</p> <p><code>kubectl get nodes</code></p> <p>This command should show the Minikube node with a status of Ready.</p>"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#kubernetes-dashboard-optional","title":"Kubernetes Dashboard (Optional)","text":"<p>If you want to use the Kubernetes Dashboard, you can start it with:</p> <p><code>minikube dashboard</code></p> <p>This will open a web browser with the Kubernetes Dashboard, providing a visual interface to manage your cluster.</p>"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#interact-with-kubernetes","title":"Interact with Kubernetes","text":"<p>Now that your Minikube cluster is running, you can start deploying applications and managing your Kubernetes cluster using kubectl.</p>"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#example-deployment","title":"Example Deployment","text":"<p>As a simple test, let's deploy a sample Nginx application. Save the following YAML as nginx-deployment.yaml:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n        ports:\n        - containerPort: 80\n</code></pre> <p>Apply the configuration:</p> <p><code>kubectl apply -f nginx-deployment.yaml</code></p> <p>This will deploy two replicas of the Nginx web server.</p>"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#conclusion","title":"Conclusion","text":"<p>Congratulations! You've successfully created a Kubernetes cluster using Minikube and deployed a sample application. This is just the beginning\u2014explore more Kubernetes features and concepts as you continue your journey with container orchestration.</p>"},{"location":"Real-Time%20Chat%20Application/","title":"Real-Time Chat Application DevOps","text":"<p>Description</p> <p>Create a real-time chat application using message brokers and set up Application Performance Metrics (APM) in a Kubernetes cluster.</p> <p>Tools</p> <p>\u25cf Container Orchestration: Kubernetes for managing containers and applications.</p> <p>\u25cf Message Brokers: Utilize message brokers like Apache Kafka or RabbitMQ for real-time messaging.</p>"},{"location":"Scan-Docker-Container/","title":"Scan Docker Container","text":"<p>This project aims to elucidate the crucial practices of security and vulnerability checking within Docker containers through the lens of CI/CD pipelines. By integrating automated testing and security scanning into the development workflow, developers can proactively identify and address security vulnerabilities, thereby bolstering the resilience of their applications.</p> <p>The pipeline will be designed to execute automated tests, validating the functional aspects of the application, and concurrently performing security scans to scrutinize the containerized environment for potential vulnerabilities. This dual-pronged approach ensures that the development process not only meets functional requirements but also adheres to fundamental security principles.</p> <p>By the culmination of this project, participants will have a practical understanding of how to implement an end-to-end CI/CD pipeline that includes automated testing for functional correctness and security scans to fortify Docker containers against potential vulnerabilities. Through this exploration, developers will be equipped with the knowledge and tools to embed security into their development workflows, fostering a proactive and secure approach to containerized application deployment. Let's embark on this journey to integrate security seamlessly into the CI/CD pipeline, fortifying our applications in the ever-evolving landscape of software development.</p>"},{"location":"Solution-Architecht/","title":"Solution Architecht Projects","text":"<p>Description</p> <p>Create a real-time chat application using message brokers and set up Application Performance Metrics (APM) in a Kubernetes cluster.</p> <p>Tools</p> <p>\u25cf Container Orchestration: Kubernetes for managing containers and applications.</p> <p>\u25cf Message Brokers: Utilize message brokers like Apache Kafka or RabbitMQ for real-time messaging.</p>"},{"location":"Terraform-Starter/","title":"Terraform Starter","text":""},{"location":"Terraform-Starter/#coming-soon","title":"Coming Soon !","text":""},{"location":"about/","title":"About","text":""},{"location":"about/#who-we-are","title":"Who We Are","text":"<p>Our career training turns ambitions into job-ready skills and business goals into tangible results. We follow the teaching method that helps students to understand the concepts and implement it by themselves.</p> <p>You can choose technology career track that includes Cloud Computing, Linux Administration, Network Engineering and Cybersecurity.</p> <p>The CloudSpace Academy training aims to make you an expert in your chosen career track and make you capable of implementing your skills in a job. Our training program has been recognized for empowering and teaching underprivileged communities.</p>"},{"location":"aws-static-web-hosting/","title":"AWS Static Web Hosting","text":""},{"location":"aws-static-web-hosting/#uh-oh","title":"Uh oh :(","text":""},{"location":"aws-static-web-hosting/#httpsgithubcomcloudspaceacademyaws-static-web-hostinggit","title":"https://github.com/cloudspaceacademy/aws-static-web-hosting.git","text":""},{"location":"aws-static-web-hosting/#overview","title":"Overview:","text":"<p>For this project we will have a static website hosted on S3 and will be utilizing CodePipeline to monitor and automatically deploy changes made from our CodeCommit repository where our index.html is hosted. Then we\u2019ll setup CloudFront as a CDN that will redirect HTTP requests to HTTPS.</p>"},{"location":"aws-static-web-hosting/#problem-statement","title":"Problem Statement","text":""},{"location":"aws-static-web-hosting/#techonology-stack","title":"Techonology Stack","text":""},{"location":"aws-static-web-hosting/#architecture-diagram","title":"Architecture Diagram","text":""},{"location":"aws-static-web-hosting/#project-requirements","title":"Project Requirements:","text":"<p>Your team has asked you to create a way to automate the deployment of a website. Currently your developers have to go through the process manually to test each new update to their code. You\u2019ll need to provide the static site URL to the developers and also make a modification to the code in the GitHub repo to verify the pipeline is working.</p> <ol> <li>Create a new repository in GitHub or CodeCommit and load the attached HTML.</li> <li>Create and configure a S3 bucket to host your static website.</li> <li>Create a CI/CD pipeline using the AWS Codepipeline service .</li> <li>Set your repo as the Source Stage of the Codepipeline that is triggered when an update is made to a GitHub repo.</li> <li>For the deploy stage select your S3 bucket.</li> <li>Deploy the pipeline and verify that you can reach the static website.</li> <li>Make an update to the code in your github to verify that the codepipeline is triggered. This can be as simple as a change to the Readme file because any change to the files should trigger the workflow.</li> </ol> <p>Note: you can skip the Build stage for this project.</p> <p>Your app is very popular all around the world but some users are complaining about slow load times in some Regions. You have been asked to add CloudFront as a CDN for your static website. CloudFront should allow caching of your static webpage and only allow HTTPS traffic to your site.</p>"},{"location":"aws-static-web-hosting/#instructions","title":"Instructions","text":"<p>Create New Repository and Clone it.</p> <p>First we need to create a repository.</p> <p>Navigate to GitHub -&gt; Repositories -&gt; Create Repository and give it a name.</p> <p></p> <p>Use the Clone URL to clone it to your local system.</p> <p>Add your files to your local repository, commit your changes, and push your changes.</p> <p></p> <p>File has been pushed from our local repo to CodeCommit.</p> <p>Create S3 Bucket</p> <p>Navigate to S3 -&gt; Create Bucket.</p> <p>Uncheck \u201cBlock all Public Access\u201d and acknowledge.</p> <p>Navigate to your bucket -&gt; Properties -&gt; Edit Static website hosting</p> <p>Enable Static website hosting and add your index document</p> <p>Now we need to create a bucket policy. Got to Permissions and edit the bucket policy.</p> <p>The following will allow everyone to access the bucket using the GetObject command,</p> <p></p> <p>Setup Pipeline.</p> <p>Navigate to CodePipeline -&gt; Create pipeline provide a name and click next.</p> <p>Source Provider = AWS CodeCommit</p> <p>Repository name = \u201cSelect your repo from the list\u201d</p> <p>Branch Name = Master</p> <p></p>"},{"location":"aws-vpc-flow-logs/","title":"AWS Vpc Flow Logs","text":""},{"location":"aws-vpc-flow-logs/#coming-soon","title":"Coming Soon !","text":""},{"location":"dockerbeginning/","title":"Flask Calculator Web Application","text":"<p>Introduction</p> <p>This documentation provides an overview of the Flask calculator web application and guides you on how to set up and run the project. This simple web application allows users to perform basic arithmetic calculations such as addition, subtraction, multiplication, and division.</p> <p>Tools Requirements</p> <p>Python | installation guide Click Here </p> <p>Docker | installation guide Click Here</p>"},{"location":"dockerbeginning/#getting-started","title":"Getting Started","text":"<p>Open terminal and run this command to clone the repository of Flask Calculator Web Application.</p> <p>Command:</p> <pre><code>git clone https://github.com/cloudspaceacademy/Flask-Calculator-app.git\n</code></pre> <p>Now navigate to the directory Flask-Calculator-app. You will see a cloudspace directory now open that directory using the terminal. </p> <p>You can simply use this command:</p> <p><code>cd Flask-Calculator-app &amp;&amp; cd cloudspace</code></p> <p>Now run  <code>ls</code>  command and Now what are the scenarios there you will see app.py,Dockerfile,requirements.txt file and a directory named templates.</p> <p>Let's elaborate why we use that Dockerfile for this application and how we write that Dockerfile.</p>"},{"location":"dockerbeginning/#why-do-we-use-docker","title":"Why do we use docker?","text":"<p>Docker provides a way to package, distribute, and run applications consistently across various environments. It simplifies the process of creating, deploying, and maintaining applications while improving portability, scalability, and resource efficiency. The Dockerfile is at the heart of this process, defining how the container image is built and what software is included, making it a valuable tool for modern software development and deployment.</p> <p>Now let's elaborate how we write that Dockerfile to package our application.</p>"},{"location":"dockerbeginning/#dockerfile","title":"Dockerfile","text":"<pre><code>FROM python:3.9-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\n\nRUN pip install -r requirements.txt\n\nCOPY . .\n\nEXPOSE 5000\n\nCMD [\"python\", \"app.py\"] \n</code></pre> <p>Now just break down all the code and elaborate why they use for </p> <p>FROM python:3.9-slim</p> <p>This line specifies the base image for our Docker container. In this case, it's using the official Python 3.9 slim image. The \"slim\" variant provides a smaller image size by excluding some non-essential components, making it suitable for many Python applications.</p> <p>WORKDIR /app</p> <p>This line sets the working directory inside the container to /app. This is where the application code and files will be placed, and it's the directory where commands will be executed by default. It's a good practice to organize your application within a dedicated directory.</p> <p>COPY requirements.txt .</p> <p>This line copies the requirements.txt file from the host system into the container. The . at the end specifies the destination path, which is the current directory within the container (i.e., /app).</p> <p>RUN pip install -r requirements.txt</p> <p>This line uses the RUN instruction to execute a command within the container. In this case, it runs pip install -r requirements.txt to install all the Python packages listed in the requirements.txt file. This step ensures that the necessary dependencies are installed in the container.</p> <p>COPY . .</p> <p>This line copies all the files and directories from the current directory (the directory where the Dockerfile is located) on our host system to the /app directory in the container. This includes our application code (app.py) and any other files or directories in the project folder.</p> <p>EXPOSE 5000</p> <p>This line informs Docker that the container will listen on port 5000. While this line is not necessary for port mapping, it serves as a form of documentation to indicate which ports the container expects to use. It's good practice for clarity.</p> <p>CMD [\"python\", \"app.py\"]</p> <p>This line defines the default command to execute when the container is started. It runs the Flask application by executing python app.py. The Flask development server will start, and our application will be served on port 5000 as specified earlier.</p> <p>Now lets learn how to run the application by using just two docker commands from your local machine. </p>"},{"location":"dockerbeginning/#running-the-application-with-docker","title":"Running the Application with Docker","text":""},{"location":"dockerbeginning/#building-a-docker-image","title":"Building a Docker Image","text":"<p>To build a Docker image, you use the docker build command. </p> <p><code>docker build -t flask-calculator .</code></p> <p>docker build -t flask-calculator .  Run this command where the Dockerfile is located. Our Dockerfile is located in the root directory cloudspace so that we use ( . )  </p> <p>Now break down the code </p> <p>docker build: This is the Docker command for building an image.</p> <p>-t flask-calculator: This part specifies the image name and tag. In this example, the image is named \"flask-calculator,\" and it has no specific tag. The tag is used to version your images; if you don't specify one, it defaults to \"latest.\"</p> <p>.: The dot (.) at the end of the command tells Docker to use the current directory as the build context. It means that Docker will look in the current directory for a Dockerfile to use as the basis for building the image.</p>"},{"location":"dockerbeginning/#docker-image-used-for","title":"Docker image used for :","text":"<p>A Docker image is a lightweight, standalone, executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, system libraries, and settings. Images are used as the basis for creating Docker containers.</p>"},{"location":"dockerbeginning/#running-a-docker-container","title":"Running a Docker Container","text":"<p>Once we have built our Docker image, we can create and run containers from it using the docker run command. </p> <p><code>docker run -p 5000:80 -d flask-calculator</code></p> <p>Now break down the code </p> <p>docker run: This is the Docker command for creating and running a container.</p> <p>-p 5000:80: This part maps port 5000 from our host machine to port 80 inside the container. It allows us to access our Flask application running in the container on port 5000.</p> <p>-d: this defines the container will run in detached mode, which means it runs in the background.</p> <p>flask-calculator: This is the name of the Docker image you want to use to create the container. The image name corresponds to the one you specified when building the image.</p>"},{"location":"dockerbeginning/#docker-container-used-for","title":"Docker Container used for :","text":"<p>A Docker container is a runnable instance of a Docker image. It is an isolated environment where our application runs. Containers share the same OS kernel as the host system but are isolated from each other. They are ephemeral and can be started, stopped, and removed as needed.</p>"},{"location":"dockerbeginning/#access-the-application","title":"Access the application","text":"<p>Now you will see the app is running from your local machine on port 5000. Open your web browser and go to localhost:5000 to access the Flask calculator web application within the Docker container.</p>"},{"location":"dockercomposebeginning/","title":"Flask Calculator Web Application","text":"<p>Introduction</p> <p>This documentation provides an overview of the Flask calculator web application and guides you on how to set up and run the project. The Flask calculator is a straightforward web application designed for basic arithmetic calculations, including addition, subtraction, multiplication, and division.</p> <p>Tools Requirements</p> <p>Python | installation guide Click Here </p> <p>Docker | installation guide Click Here</p>"},{"location":"dockercomposebeginning/#getting-started","title":"Getting Started","text":"<p>Open your terminal and clone the Flask Calculator Web Application repository using the following command:</p> <pre><code>git clone https://github.com/cloudspaceacademy/Flask-Calculator-app.git\n</code></pre> <p>Navigate to the \"Flask-Calculator-app\" directory By running this command :</p> <p><code>cd Flask-Calculator-app</code></p> <p>Inside the \"Flask-Calculator-app\" directory, you will find a \"cloudspace\" directory. Enter this directory using the following command:</p> <p><code>cd cloudspace</code></p> <p>In Previous session \"Docker Beginner\" you have learned about how to run this \"Flask Calculator Web Application\" using Dockerfile.Now we are gonna learn how to run this application using docker-compose.yml file.</p> <p>In Previous session \"Docker Beginner\" you have learned about Dockerfile.How To write a Dockerfile,why its used for,how to run the application using Dockerfile etc.Now let's jump up to docker-compose.yml file.</p>"},{"location":"dockercomposebeginning/#docker-composeyml-file-used-for","title":"docker-compose.yml file used for","text":"<p>A docker-compose.yml file is used to define a multi-container Docker application. It allows you to define and configure multiple services, networks, and volumes in a single file. Docker Compose simplifies the process of orchestrating complex applications by providing a way to manage multiple containers as a single application stack.</p> <p>Simply Dockerfile is used for single service or to run the application using one container and docker-compose.yml file is used for multiple services or to packaging and run the application using multi containers.</p>"},{"location":"dockercomposebeginning/#docker-composeyml","title":"docker-compose.yml","text":"<pre><code>version: '3.8'\n\nservices:\n  flask-calculator:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - \"8080:80\"\n\n</code></pre> <p>Let's Break down the code </p> <p>version: '3.8':Specifies the version of the Docker Compose file syntax. In this case, it's using version 3.8.</p> <p>services:Defines the services that make up the Docker application.</p> <p>flask-calculator:The name of the service. In this case, it's named flask-calculator.</p> <p>build:Specifies how to build the Docker image for the service.</p> <p>context: . :Specifies the build context, which is the path to the directory containing the Dockerfile and any files needed for the build. In this case, it's set to the current directory (.)</p> <p>dockerfile: Dockerfile:Specifies the name of the Dockerfile to use for building the image. In this case, it's named Dockerfile.</p> <p>ports:Specifies the ports to expose from the container. - \"8080:80\":</p> <p>Maps port 80 from the container to port 8080 on the host machine. The format is host_port:container_port.</p>"},{"location":"dockercomposebeginning/#running-the-application-with-docker-compose","title":"Running the Application with Docker Compose","text":"<p>To run the application run this command where the docker-compose.yml file is located.</p> <p>Command :</p> <p><code>docker-compose up -d</code> </p> <p>Break down Codes:</p> <p>docker-compose: The Docker Compose command-line tool.</p> <p>up: This command is used to create and start containers based on the configurations specified in the docker-compose.yml file.</p> <p>-d: Stands for \"detached mode.\" When you run Docker containers in detached mode, the containers run in the background, and you get your terminal prompt back. This allows you to continue using the same terminal session for other tasks.</p> <p>Now Run <code>docker ps</code> command to see the running container id that you have just created.</p>"},{"location":"dockercomposebeginning/#access-the-application","title":"Access the application","text":"<p>Now you will see the app is running from your local machine on port 8080. Open your web browser and go to localhost:8080 to access the Flask calculator web application within the Docker container.</p>"},{"location":"serverless-on-aws/","title":"Lambda and S3Events DEMO","text":""},{"location":"serverless-on-aws/#video-guides-for-this-mini-project","title":"Video Guides for this Mini Project","text":"<ul> <li>PART1</li> <li>PART2</li> <li>PLAYLIST</li> </ul> <p>This content is free, but I have a full range of training courses HERE</p> <p>In this demo lesson you're going to create a simple event-driven image processing pipeline. The pipeline uses two S3 buckets, a source bucket and a processed bucket. When images are added to the source bucket a lambda function is triggered based on the PUT.  When invoked the lambda function receives the <code>event</code> and extracts the bucket and object information. Once those details are known, the lambda function, using the <code>PIL</code> module pixelates the image with <code>5</code> different variations (8x8, 16x16, 32x32, 48x48 and 64x64) and uploads them to the processed bucket.</p>"},{"location":"serverless-on-aws/#stage-1-create-the-s3-buckets","title":"Stage 1 - Create the S3 Buckets","text":"<p>Move to the S3 Console https://s3.console.aws.amazon.com/s3/home?region=us-east-1# We will be creating <code>2</code> buckets, both with the same name, but each suffixed with a functional title (see below) , all settings apart from region and bucket name can be left as default. Click <code>Create Bucket</code> and create a bucket in the format of unique-name-<code>source</code> in the <code>us-east-1</code> region Click <code>Create Bucket</code> and create a another bucket in the format of unique-name-<code>processed</code> also in the <code>us-east-1</code> region These names will need to be unique, but as an example  </p> <p>Bucket 1 : <code>dontusethisname-source</code> Bucket 2 : <code>dontusethisname-processed</code> </p>"},{"location":"serverless-on-aws/#stage-2-create-the-lambda-role","title":"Stage 2 - Create the Lambda Role","text":"<p>Move to the IAM Console https://console.aws.amazon.com/iamv2/home?#/home Click Roles, then Create Role For <code>Trusted entity type</code>, pick <code>AWS service</code> For the service to trust pick <code>Lambda</code>  then click <code>Next</code> , <code>Next</code> again For <code>Role name</code> put <code>PixelatorRole</code>  then Create the role  </p> <p>Click <code>PixelatorRole</code> Under <code>Permissions Policy</code> we need to add permissions and it will be an <code>inline policy</code> Click <code>JSON</code>  and delete the contents of the code box entirely. Load this link in a new tab (https://raw.githubusercontent.com/acantril/learn-cantrill-io-labs/master/00-aws-simple-demos/aws-lambda-s3-events/01_LABSETUP/policy/s3pixelator.json) Copy the entire contents into your clipboard and paste into the previous permissions policy code editor box Locate the words <code>REPLACEME</code> there should be <code>4</code> occurrences, 2 each for the source and processed buckets .. and for each of those one for the bucket and another for the objects in that bucket. Replace the term <code>REPLACEME</code> with the name you picked for your buckets above, in my example it is <code>dontusethisname</code> You should end with 4 lines looking like this, only with <code>YOUR</code> bucket names  </p> <pre><code>\"Resource\":[\n    \"arn:aws:s3:::dontusethisname-processed\",\n    \"arn:aws:s3:::dontusethisname-processed/*\",\n    \"arn:aws:s3:::dontusethisname-source/*\",\n    \"arn:aws:s3:::dontusethisname-source\"\n]\n</code></pre> <p>Locate the two occurrences of <code>YOURACCOUNTID</code>, you need to replace both of these words with your AWS account ID To get that, click the account dropdown at the top right  click the small icon to copy down the <code>Account ID</code> and replace the <code>YOURACCOUNTID</code> in the policy code editor. important if you use the 'icon' to copy this number, it will remove the <code>-</code> in the account number for you :) you need to paste <code>123456789000</code> rather than <code>1234-5678-9000</code> </p> <p>You should have something which looks like this, only with your account ID:  </p> <pre><code>{\n      \"Effect\": \"Allow\",\n      \"Action\": \"logs:CreateLogGroup\",\n      \"Resource\": \"arn:aws:logs:us-east-1:123456789000:*\"\n  },\n  {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n          \"logs:CreateLogStream\",\n          \"logs:PutLogEvents\"\n      ],\n      \"Resource\": [\n          \"arn:aws:logs:us-east-1:123456789000:log-group:/aws/lambda/pixelator:*\"\n      ]\n  }\n</code></pre> <p>Click <code>Review Policy</code> For name put <code>pixelator_access_inline</code>  and create the policy.  </p>"},{"location":"serverless-on-aws/#stage-3-pre-only-do-this-part-if-you-want-to-get-experience-of-creating-a-lambda-zip","title":"Stage 3 (pre) - ONLY DO THIS PART IF YOU WANT TO GET EXPERIENCE OF CREATING A LAMBDA ZIP","text":""},{"location":"serverless-on-aws/#this-guide-is-only-tested-on-macos-it-should-work-on-linux-windows-may-require-different-tools","title":"this guide is only tested on macOS, it should work on linux, windows may require different tools.","text":""},{"location":"serverless-on-aws/#if-in-doubt-skip-to-step-3-below","title":"if in doubt, skip to step 3 below","text":"<p>From the CLI/Terminal Create a folder my_lambda_deployment Move into that folder create a folder called lambda Move into that folder Create a file called <code>lambda_function.py</code> and paste in the code for the lambda <code>pixelator</code> function (https://raw.githubusercontent.com/acantril/learn-cantrill-io-labs/master/00-aws-simple-demos/aws-lambda-s3-events/01_LABSETUP/lambda/lambda_function.py) then save Download this file (https://files.pythonhosted.org/packages/f3/3b/d7bb231b3bc1414252e77463dc63554c1aeccffe0798524467aca7bad089/Pillow-9.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl) into that folder run <code>unzip Pillow-9.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl</code> and then <code>rm Pillow-9.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl</code> These are the Pillow module files ... required for image manipulation in Python 3.9 (which is what the lambda function will be using) From the same folder, run <code>zip -r ../my-deployment-package.zip .</code> which will create a lambda function zip, containing all these files in the parent directory.  </p> <p>This zip will be the same zip which i link below, so if you do have any issues with the lambda function, you can use the one i've pre-created.</p>"},{"location":"serverless-on-aws/#stage-3-create-the-lambda-function","title":"Stage 3 - Create the Lambda Function","text":"<p>Move to the lambda console (https://console.aws.amazon.com/lambda/home?region=us-east-1#/functions) Click <code>Create Function</code> We're going to be <code>Authoring from Scratch</code> For <code>Function name</code> enter <code>pixelator</code> for <code>Runtime</code> select <code>Python 3.9</code> For <code>Architecture</code> select <code>x86_64</code> For <code>Permissions</code> expand <code>Change default execution role</code> pick <code>Use an existing role</code> and in the <code>Existing role</code> dropdown, pick <code>PixelatorRole</code> Then <code>Create Function</code> Close down any <code>notifcation</code> dialogues/popups Click <code>Upload from</code> and select <code>.zip file</code> Either 1, download this zip to your local machine (https://github.com/acantril/learn-cantrill-io-labs/blob/master/00-aws-simple-demos/aws-lambda-s3-events/01_LABSETUP/my-deployment-package.zip, click Download) or 2, locate the .zip you created yourself in the <code>Stage 3(pre)</code> above - they will be identical On the lambda screen, click <code>Upload</code> locate and select that .zip, and then click the <code>Save</code> button This upload will take a few minutes, but once complete you might see something saying <code>The deployment package of your Lambda function \"pixelator\" is too large to enable inline code editing. However, you can still invoke your function.</code> which is OK :)  </p>"},{"location":"serverless-on-aws/#stage-4-configure-the-lambda-function-trigger","title":"Stage 4 - Configure the Lambda Function &amp; Trigger","text":"<p>Click <code>Configuration</code> tab and then <code>Environment variables</code> We need to add an environment variable telling the pixelator function which processed bucket to use, it will know the source bucket because it's told about that in the event data. Click <code>Edit</code> then <code>Add environment variable</code>, under Key put <code>processed_bucket</code> and for <code>Value</code> put the bucket name of your processed bucket.  As an example <code>dontusethisname-processed</code>  (but use your bucket name) Be <code>really really really</code> sure you put your <code>processed</code> bucket here and NOT your source bucket. if you use the source bucket here, the output images will be stored in the source bucket, this will cause the lambda function to run over and over again ... bad be super-sure to put your processed bucket Click <code>Save</code> </p> <p>Click <code>General configuration</code> then click <code>Edit</code> and change the timeout to <code>1</code> minutes and <code>0</code> seconds, then click <code>Save</code> </p> <p>Click <code>Add trigger</code> In the dropdown pick <code>S3</code> Under <code>Bucket</code> pick your source bucket ... AGAIN be really really sure this is your source bucket and NOT your destination bucket and NOT any other bucket. Only pick your SOURCE bucket here. You will need to check the <code>Recursive invocation</code> acknowledgment box, this is because this lambda function is invoked every time anything is added to the source bucket, if you configure this wrongly, or configure the environment variable above wrongly ... it will run the lambda function over and over again for ever.  Once checked, click <code>Add</code> </p>"},{"location":"serverless-on-aws/#stage-5-test-and-monitor","title":"Stage 5 - Test and Monitor","text":"<p>open a tab to the <code>cloudwatch logs</code> console (https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logsV2:log-groups) make sure you have two tabs open to the <code>s3 console</code> (https://s3.console.aws.amazon.com/s3/home?region=us-east-1)  In one tab open your <code>-source</code> bucket &amp; in the other open the `-processed' bucket  </p> <p>In the <code>-source</code> bucket tab, make sure to select the <code>Objects</code> tab and click <code>Upload</code> Add some files and click <code>Upload</code>  (use your own, or these https://github.com/acantril/learn-cantrill-io-labs/tree/master/00-aws-simple-demos/aws-lambda-s3-events/01_LABSETUP/media) Once finished, click <code>Close</code> Move to the <code>CloudWatch Logs</code> tab Click the <code>Refresh</code> icon, locate and click <code>/aws/lambda/pixelator</code> If there is a log stream in there, click the most recent one, if not, keep clicking the <code>Refresh</code> icon and then click the most recent log stream Expand the line which begins with <code>{'Records': [{'eventVersion':</code> and you can see all of the event information about the lambda invocation, you should see the object name listed in <code>'object': {'key'</code> ... Go to the S3 Console tab for the <code>-processed</code> bucket Click the <code>Refresh</code> icon Select each of the pixelated versions of the image ... you should have 5 (<code>8x8</code>, <code>16x16</code>, <code>32x32</code>, <code>48x48</code> and <code>64x64</code>) Click <code>Open</code> You browser will either open or save all of the images Open them one by one, starting with <code>8x8</code> and finally <code>64x64</code> in order ... notice how they are the same image, but less and less pixelated :)  </p>"},{"location":"serverless-on-aws/#stage-6-cleanup","title":"Stage 6 - Cleanup","text":"<p>Open the <code>pixelator</code> lambda function (https://console.aws.amazon.com/lambda/home?region=us-east-1#/functions/pixelator?tab=code) Delete the function Move to the IAM Roles console (https://console.aws.amazon.com/iamv2/home#/roles)  Click <code>PixelatorRole</code>, then <code>Delete</code> the role, then confirm the deletion. Go to the <code>S3 Console</code> (https://s3.console.aws.amazon.com/s3/home?region=us-east-1&amp;region=us-east-1) For each of the <code>source</code> and <code>processed</code> buckets do:</p> <ul> <li>Select the bucket.  </li> <li>Click <code>Empty</code>.  </li> <li>Type <code>permanently delete</code>, and <code>Empty</code>.  </li> <li>Close the dialogue and move back to the main S3 Console.  </li> <li>Make sure the bucket is still selected, click <code>Delete</code>.   </li> <li>Type the name of the bucket then delete the bucket.  </li> </ul> <p>That's all folks! :)  </p>"}]}