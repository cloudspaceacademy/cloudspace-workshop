{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Cloud Space Website : www.cloudspaceacademy.com info@cloudspaceacademy.com Phone: +1-855-200-7653 WhatsApp: +1-571-454-4691 Facebook At CloudSpace , our students' success is our top priority. We understand it can be difficult to study alone once the BootCamp is over. CloudSpace is committed to providing you with the necessary support until you land that dream job. The FREE follow-up program is a dedicated place to keep learning with an industry leader DevOps instructor until you land a job. All this free of charge as long as you have completed the AWS Cloud & DevOps BootCamp with us. So after completing the 6 months AWS & DevOps Bootcamp, you can attend the Free Follow-up Program free of charge on a weekly basis to keep working on interview preparation and DevOps projects with an experienced instructor. So you will be allowed to attend this post-bootcamp until you land a job. Follow-Up Program Schedule: Thursdays (8PM-9PM) Interview Prep - Sundays (1PM-3PM) Projects Hands On","title":"Home"},{"location":"#welcome-to-cloud-space","text":"Website : www.cloudspaceacademy.com info@cloudspaceacademy.com Phone: +1-855-200-7653 WhatsApp: +1-571-454-4691 Facebook At CloudSpace , our students' success is our top priority. We understand it can be difficult to study alone once the BootCamp is over. CloudSpace is committed to providing you with the necessary support until you land that dream job. The FREE follow-up program is a dedicated place to keep learning with an industry leader DevOps instructor until you land a job. All this free of charge as long as you have completed the AWS Cloud & DevOps BootCamp with us. So after completing the 6 months AWS & DevOps Bootcamp, you can attend the Free Follow-up Program free of charge on a weekly basis to keep working on interview preparation and DevOps projects with an experienced instructor. So you will be allowed to attend this post-bootcamp until you land a job. Follow-Up Program Schedule: Thursdays (8PM-9PM) Interview Prep - Sundays (1PM-3PM) Projects Hands On","title":"Welcome to Cloud Space"},{"location":"AWS%20E-Commerce%20Application/","text":"AWS E-Commerce Application DevOps Description Develop and deploy a full-fledged e-commerce application in an AWS environment. This includes a database, S3 bucket for storage, ECS for deployment, and infrastructure as code with CloudFormation. Tools \u25cf Infrastructure as Code: AWS CloudFormation to define the infrastructure. \u25cf Database: AWS RDS or DynamoDB for database management. \u25cf Storage: AWS S3 for file storage. \u25cf Deployment: AWS ECS for container-based deployment. \u25cf CI/CD: Automate deployment using GitHub Actions.","title":"Full Stack DevOps Expert"},{"location":"AWS%20E-Commerce%20Application/#aws-e-commerce-application-devops","text":"Description Develop and deploy a full-fledged e-commerce application in an AWS environment. This includes a database, S3 bucket for storage, ECS for deployment, and infrastructure as code with CloudFormation. Tools \u25cf Infrastructure as Code: AWS CloudFormation to define the infrastructure. \u25cf Database: AWS RDS or DynamoDB for database management. \u25cf Storage: AWS S3 for file storage. \u25cf Deployment: AWS ECS for container-based deployment. \u25cf CI/CD: Automate deployment using GitHub Actions.","title":"AWS E-Commerce Application DevOps"},{"location":"AWS%20Photo%20Gallery%20Application/","text":"AWS Photo Gallery Application DevOps Description This application is a photo gallery platform similar to Pexels . Users can freely upload and download images. It leverages AWS services for scalability and reliability. Tools \u25cf Version Control: AWS CodeCommit for code storage. \u25cf CI/CD: AWS CodePipeline for automating deployment. \u25cf Infrastructure as Code: Terraform to define infrastructure components such as S3 buckets, ECS clusters, EC2 instances for MongoDB databases. \u25cf Monitoring and Alerting: Amazon CloudWatch for monitoring and alerting.","title":"AWS DevOps Expert"},{"location":"AWS%20Photo%20Gallery%20Application/#aws-photo-gallery-application-devops","text":"Description This application is a photo gallery platform similar to Pexels . Users can freely upload and download images. It leverages AWS services for scalability and reliability. Tools \u25cf Version Control: AWS CodeCommit for code storage. \u25cf CI/CD: AWS CodePipeline for automating deployment. \u25cf Infrastructure as Code: Terraform to define infrastructure components such as S3 buckets, ECS clusters, EC2 instances for MongoDB databases. \u25cf Monitoring and Alerting: Amazon CloudWatch for monitoring and alerting.","title":"AWS Photo Gallery Application DevOps"},{"location":"Ansible-Playbook-Library/","text":"Ansible Playbook Library Coming Soon !","title":"Ansible Playbook Library"},{"location":"Ansible-Playbook-Library/#ansible-playbook-library","text":"","title":"Ansible Playbook Library"},{"location":"Ansible-Playbook-Library/#coming-soon","text":"","title":"Coming Soon !"},{"location":"Banking%20Web%20Application/","text":"Banking Web Application DevOps Create a web-based banking application that includes load balancing using Nginx and microservice tracking using Application Gateway with distributed tracing. Tools \u25cf Web Server: Nginx for load balancing. \u25cf Microservice Tracking: AWS Elastic Load Balancing for managing web traffic and enabling distributed tracing.","title":"Load Balancing Expert"},{"location":"Banking%20Web%20Application/#banking-web-application-devops","text":"Create a web-based banking application that includes load balancing using Nginx and microservice tracking using Application Gateway with distributed tracing. Tools \u25cf Web Server: Nginx for load balancing. \u25cf Microservice Tracking: AWS Elastic Load Balancing for managing web traffic and enabling distributed tracing.","title":"Banking Web Application DevOps"},{"location":"CI-CD-Beginner/","text":"CI/CD Beginner In the ever-evolving landscape of software development, the seamless integration of efficient and reliable Continuous Integration/Continuous Deployment (CI/CD) pipelines is imperative for delivering high-quality applications with speed and consistency. This project embarks on the journey of demystifying the CI/CD process using GitHub Actions and orchestrating the deployment of a Dockerized application to both development and production environments within the Amazon Web Services (AWS) ecosystem. The core objective of this project is to provide a clear understanding of the fundamental concepts surrounding CI/CD, containerization with Docker, and deployment in AWS environments. By leveraging GitHub Actions, a powerful and flexible automation tool integrated into the GitHub platform, developers gain the ability to automate workflows, conduct automated testing, and seamlessly deploy applications, all within the same version control environment. The chosen technology stack revolves around Docker, offering containerization to encapsulate the application and its dependencies, ensuring consistency across various environments. AWS serves as the deployment platform, with a focus on providing scalable and resilient infrastructure to host Dockerized applications. Throughout this project, we will explore the step-by-step process of building a CI/CD pipeline, containerizing an application, and deploying it to both a development and a production environment. By the end, developers will not only have a functional CI/CD pipeline but will also grasp the principles and practices essential for streamlining software development workflows in a modern, cloud-native context. Let's embark on this journey to unlock the potential of CI/CD, Docker, and AWS for a more efficient and robust software development lifecycle.","title":"CI/CD Pipeline"},{"location":"CI-CD-Beginner/#cicd-beginner","text":"In the ever-evolving landscape of software development, the seamless integration of efficient and reliable Continuous Integration/Continuous Deployment (CI/CD) pipelines is imperative for delivering high-quality applications with speed and consistency. This project embarks on the journey of demystifying the CI/CD process using GitHub Actions and orchestrating the deployment of a Dockerized application to both development and production environments within the Amazon Web Services (AWS) ecosystem. The core objective of this project is to provide a clear understanding of the fundamental concepts surrounding CI/CD, containerization with Docker, and deployment in AWS environments. By leveraging GitHub Actions, a powerful and flexible automation tool integrated into the GitHub platform, developers gain the ability to automate workflows, conduct automated testing, and seamlessly deploy applications, all within the same version control environment. The chosen technology stack revolves around Docker, offering containerization to encapsulate the application and its dependencies, ensuring consistency across various environments. AWS serves as the deployment platform, with a focus on providing scalable and resilient infrastructure to host Dockerized applications. Throughout this project, we will explore the step-by-step process of building a CI/CD pipeline, containerizing an application, and deploying it to both a development and a production environment. By the end, developers will not only have a functional CI/CD pipeline but will also grasp the principles and practices essential for streamlining software development workflows in a modern, cloud-native context. Let's embark on this journey to unlock the potential of CI/CD, Docker, and AWS for a more efficient and robust software development lifecycle.","title":"CI/CD Beginner"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/","text":"Creating a Kubernetes Cluster with Minikube Introduction Minikube is a tool that allows you to run Kubernetes clusters on your local machine. It's great for development, testing, and learning Kubernetes concepts. This guide will walk you through the process of setting up a Kubernetes cluster using Minikube. Prerequisites Hypervisor: Minikube requires a hypervisor to run the virtual machines. Common choices include VirtualBox, Hyper-V, KVM, or Docker. Ensure your preferred hypervisor is installed and properly configured.We use Docker for that Docker | installation guide Click Here kubectl: The kubectl command-line tool is used to interact with your Kubernetes cluster. Make sure it's installed on your machine. Kubectl | installation guide Click Here Install Minikube Visit the Minikube Installation Guide and follow the instructions for your operating system to install Minikube. Minikube | installation guide Click Here Start Minikube Open a terminal and run the following command to start Minikube: minikube start This command will download the Minikube ISO, start a virtual machine, and set up the Kubernetes cluster. Verify Cluster Status After Minikube has started, you can check the cluster status using: kubectl cluster-info This command will display information about the cluster, including the Kubernetes master and services. Check Nodes Verify that Minikube has created a node for your cluster: kubectl get nodes This command should show the Minikube node with a status of Ready. Kubernetes Dashboard (Optional) If you want to use the Kubernetes Dashboard, you can start it with: minikube dashboard This will open a web browser with the Kubernetes Dashboard, providing a visual interface to manage your cluster. Interact with Kubernetes Now that your Minikube cluster is running, you can start deploying applications and managing your Kubernetes cluster using kubectl. Example Deployment As a simple test, let's deploy a sample Nginx application. Save the following YAML as nginx-deployment.yaml: apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80 Apply the configuration: kubectl apply -f nginx-deployment.yaml This will deploy two replicas of the Nginx web server. Conclusion Congratulations! You've successfully created a Kubernetes cluster using Minikube and deployed a sample application. This is just the beginning\u2014explore more Kubernetes features and concepts as you continue your journey with container orchestration.","title":"Kubernetes-Minikube Cluster Beginner"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#creating-a-kubernetes-cluster-with-minikube","text":"Introduction Minikube is a tool that allows you to run Kubernetes clusters on your local machine. It's great for development, testing, and learning Kubernetes concepts. This guide will walk you through the process of setting up a Kubernetes cluster using Minikube. Prerequisites Hypervisor: Minikube requires a hypervisor to run the virtual machines. Common choices include VirtualBox, Hyper-V, KVM, or Docker. Ensure your preferred hypervisor is installed and properly configured.We use Docker for that Docker | installation guide Click Here kubectl: The kubectl command-line tool is used to interact with your Kubernetes cluster. Make sure it's installed on your machine. Kubectl | installation guide Click Here","title":"Creating a Kubernetes Cluster with Minikube"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#install-minikube","text":"Visit the Minikube Installation Guide and follow the instructions for your operating system to install Minikube. Minikube | installation guide Click Here","title":"Install Minikube"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#start-minikube","text":"Open a terminal and run the following command to start Minikube: minikube start This command will download the Minikube ISO, start a virtual machine, and set up the Kubernetes cluster.","title":"Start Minikube"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#verify-cluster-status","text":"After Minikube has started, you can check the cluster status using: kubectl cluster-info This command will display information about the cluster, including the Kubernetes master and services.","title":"Verify Cluster Status"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#check-nodes","text":"Verify that Minikube has created a node for your cluster: kubectl get nodes This command should show the Minikube node with a status of Ready.","title":"Check Nodes"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#kubernetes-dashboard-optional","text":"If you want to use the Kubernetes Dashboard, you can start it with: minikube dashboard This will open a web browser with the Kubernetes Dashboard, providing a visual interface to manage your cluster.","title":"Kubernetes Dashboard (Optional)"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#interact-with-kubernetes","text":"Now that your Minikube cluster is running, you can start deploying applications and managing your Kubernetes cluster using kubectl.","title":"Interact with Kubernetes"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#example-deployment","text":"As a simple test, let's deploy a sample Nginx application. Save the following YAML as nginx-deployment.yaml: apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80 Apply the configuration: kubectl apply -f nginx-deployment.yaml This will deploy two replicas of the Nginx web server.","title":"Example Deployment"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#conclusion","text":"Congratulations! You've successfully created a Kubernetes cluster using Minikube and deployed a sample application. This is just the beginning\u2014explore more Kubernetes features and concepts as you continue your journey with container orchestration.","title":"Conclusion"},{"location":"Logging-with-ELK-Stack/","text":"Logging with ELK Stack Coming Soon !","title":"Logging with ELK Stack"},{"location":"Logging-with-ELK-Stack/#logging-with-elk-stack","text":"","title":"Logging with ELK Stack"},{"location":"Logging-with-ELK-Stack/#coming-soon","text":"","title":"Coming Soon !"},{"location":"Monitoring-with-Prometheus/","text":"Monitoring with Prometheus In the intricate tapestry of modern software architecture, the ability to monitor and analyze the performance and health metrics of applications is pivotal. This project embarks on a journey to demystify the realm of monitoring by introducing Prometheus, a powerful open-source monitoring and alerting toolkit, and Grafana, a versatile platform for visualizing time-series data. Monitoring applications in real-time is indispensable for identifying bottlenecks, detecting anomalies, and ensuring optimal performance. Prometheus, with its robust data model and flexible querying language, has emerged as a cornerstone in the domain of monitoring, providing developers and operators with the tools needed to gain insights into the intricate workings of their systems. This project is designed to elucidate the fundamental concepts of Prometheus and Grafana by guiding participants through the process of setting up a monitoring infrastructure for their applications. By integrating Prometheus to collect and store time-series data and Grafana to create intuitive dashboards, developers can gain actionable insights into the health and performance of their systems. Throughout this project, we will delve into the intricacies of Prometheus, exploring its capabilities for metric collection, querying, and alerting. Grafana will then complement this monitoring setup by providing a visually appealing and customizable interface to represent and analyze the collected metrics. By the culmination of this project, participants will have a solid understanding of how to implement Prometheus for monitoring and Grafana for visualization, empowering them to make informed decisions based on real-time data. Let's embark on this exploration of monitoring and visualization to unlock the potential of Prometheus and Grafana in enhancing the observability of your applications.","title":"Monitoring with Prometheus"},{"location":"Monitoring-with-Prometheus/#monitoring-with-prometheus","text":"In the intricate tapestry of modern software architecture, the ability to monitor and analyze the performance and health metrics of applications is pivotal. This project embarks on a journey to demystify the realm of monitoring by introducing Prometheus, a powerful open-source monitoring and alerting toolkit, and Grafana, a versatile platform for visualizing time-series data. Monitoring applications in real-time is indispensable for identifying bottlenecks, detecting anomalies, and ensuring optimal performance. Prometheus, with its robust data model and flexible querying language, has emerged as a cornerstone in the domain of monitoring, providing developers and operators with the tools needed to gain insights into the intricate workings of their systems. This project is designed to elucidate the fundamental concepts of Prometheus and Grafana by guiding participants through the process of setting up a monitoring infrastructure for their applications. By integrating Prometheus to collect and store time-series data and Grafana to create intuitive dashboards, developers can gain actionable insights into the health and performance of their systems. Throughout this project, we will delve into the intricacies of Prometheus, exploring its capabilities for metric collection, querying, and alerting. Grafana will then complement this monitoring setup by providing a visually appealing and customizable interface to represent and analyze the collected metrics. By the culmination of this project, participants will have a solid understanding of how to implement Prometheus for monitoring and Grafana for visualization, empowering them to make informed decisions based on real-time data. Let's embark on this exploration of monitoring and visualization to unlock the potential of Prometheus and Grafana in enhancing the observability of your applications.","title":"Monitoring with Prometheus"},{"location":"Open%20Source%20Photo%20Gallery%20Application/","text":"Open Source Photo Gallery Application DevOps Description An open-source version of the photo gallery application. Tools \u25cf Version Control: GitHub for code storage. \u25cf CI/CD: GitHub Actions for automating deployment. \u25cf Infrastructure as Code: Terraform for defining infrastructure elements. \u25cf Database and Log Monitoring: ELK Stack (Elasticsearch Logstash, Kibana) for log monitoring. \u25cf Alerting: Prometheus and Grafana for alerting. \u25cf Automatic Deployment: Deploy the application automatically.","title":"Open Source DevOps Expert"},{"location":"Open%20Source%20Photo%20Gallery%20Application/#open-source-photo-gallery-application-devops","text":"Description An open-source version of the photo gallery application. Tools \u25cf Version Control: GitHub for code storage. \u25cf CI/CD: GitHub Actions for automating deployment. \u25cf Infrastructure as Code: Terraform for defining infrastructure elements. \u25cf Database and Log Monitoring: ELK Stack (Elasticsearch Logstash, Kibana) for log monitoring. \u25cf Alerting: Prometheus and Grafana for alerting. \u25cf Automatic Deployment: Deploy the application automatically.","title":"Open Source Photo Gallery Application DevOps"},{"location":"Real-Time%20Chat%20Application/","text":"Real-Time Chat Application DevOps Description Create a real-time chat application using message brokers and set up Application Performance Metrics (APM) in a Kubernetes cluster. Tools \u25cf Container Orchestration: Kubernetes for managing containers and applications. \u25cf Message Brokers: Utilize message brokers like Apache Kafka or RabbitMQ for real-time messaging.","title":"Real-Time Application DevOps Expert"},{"location":"Real-Time%20Chat%20Application/#real-time-chat-application-devops","text":"Description Create a real-time chat application using message brokers and set up Application Performance Metrics (APM) in a Kubernetes cluster. Tools \u25cf Container Orchestration: Kubernetes for managing containers and applications. \u25cf Message Brokers: Utilize message brokers like Apache Kafka or RabbitMQ for real-time messaging.","title":"Real-Time Chat Application DevOps"},{"location":"Scan-Docker-Container/","text":"Scan Docker Container This project aims to elucidate the crucial practices of security and vulnerability checking within Docker containers through the lens of CI/CD pipelines. By integrating automated testing and security scanning into the development workflow, developers can proactively identify and address security vulnerabilities, thereby bolstering the resilience of their applications. The pipeline will be designed to execute automated tests, validating the functional aspects of the application, and concurrently performing security scans to scrutinize the containerized environment for potential vulnerabilities. This dual-pronged approach ensures that the development process not only meets functional requirements but also adheres to fundamental security principles. By the culmination of this project, participants will have a practical understanding of how to implement an end-to-end CI/CD pipeline that includes automated testing for functional correctness and security scans to fortify Docker containers against potential vulnerabilities. Through this exploration, developers will be equipped with the knowledge and tools to embed security into their development workflows, fostering a proactive and secure approach to containerized application deployment. Let's embark on this journey to integrate security seamlessly into the CI/CD pipeline, fortifying our applications in the ever-evolving landscape of software development.","title":"Scan Docker container"},{"location":"Scan-Docker-Container/#scan-docker-container","text":"This project aims to elucidate the crucial practices of security and vulnerability checking within Docker containers through the lens of CI/CD pipelines. By integrating automated testing and security scanning into the development workflow, developers can proactively identify and address security vulnerabilities, thereby bolstering the resilience of their applications. The pipeline will be designed to execute automated tests, validating the functional aspects of the application, and concurrently performing security scans to scrutinize the containerized environment for potential vulnerabilities. This dual-pronged approach ensures that the development process not only meets functional requirements but also adheres to fundamental security principles. By the culmination of this project, participants will have a practical understanding of how to implement an end-to-end CI/CD pipeline that includes automated testing for functional correctness and security scans to fortify Docker containers against potential vulnerabilities. Through this exploration, developers will be equipped with the knowledge and tools to embed security into their development workflows, fostering a proactive and secure approach to containerized application deployment. Let's embark on this journey to integrate security seamlessly into the CI/CD pipeline, fortifying our applications in the ever-evolving landscape of software development.","title":"Scan Docker Container"},{"location":"Solution-Architecht/","text":"Solution Architecht Projects Description Create a real-time chat application using message brokers and set up Application Performance Metrics (APM) in a Kubernetes cluster. Tools \u25cf Container Orchestration: Kubernetes for managing containers and applications. \u25cf Message Brokers: Utilize message brokers like Apache Kafka or RabbitMQ for real-time messaging.","title":"Solution Architecht Projects"},{"location":"Solution-Architecht/#solution-architecht-projects","text":"Description Create a real-time chat application using message brokers and set up Application Performance Metrics (APM) in a Kubernetes cluster. Tools \u25cf Container Orchestration: Kubernetes for managing containers and applications. \u25cf Message Brokers: Utilize message brokers like Apache Kafka or RabbitMQ for real-time messaging.","title":"Solution Architecht Projects"},{"location":"Terraform-Starter/","text":"Terraform Starter Creating AWS Infrastructure with Terraform Introduction This documentation provides step-by-step instructions on using Terraform to create an AWS EC2 instance in the eu-north-1 region. The example code specifies an AMI ID for ubuntu-focal-20.04-amd64 (ami-07ec4220c92589b40) and uses a t3.micro instance type. Prerequisites AWS Account: Ensure that you have an AWS account. If you don't have one, you can sign up at AWS Console. AWS CLI Installation: Install the AWS CLI on your local machine. You can download it from the AWS CLI official website. Terraform Installation: Install Terraform on your local machine. You can download Terraform from the official Terraform website. AWS Credentials: Configure your AWS credentials on your machine using the AWS CLI. Run aws configure and provide your AWS Access Key, Secret Key, region (eu-north-1), and output format. Terraform Configuration Create a Terraform Configuration File: Create a file named main.tf and paste the following Terraform configuration: # Specify the AWS provider and region provider \"aws\" { region = \"eu-north-1\" # Change this to your desired AWS region } # Define an AWS EC2 instance resource \"aws_instance\" \"example_instance\" { ami = \"ami-07ec4220c92589b40\" # Change this to a valid AMI ID for your region instance_type = \"t3.micro\" # Change this to your desired instance type tags = { Name = \"example-instance\" } } Initialize Terraform: Open a terminal, navigate to the directory containing main.tf, and run: terraform init Apply Terraform Configuration: Run the following command to apply the Terraform configuration: terraform apply Terraform will prompt you to confirm the plan. Type yes and press Enter. Access AWS Console: Visit the AWS Console and log in with your AWS credentials. Verify EC2 Instance: Navigate to the EC2 service in the AWS Console. You should see a new EC2 instance with the specified configuration. Conclusion You have successfully created an AWS EC2 instance using Terraform. This example can be expanded and customized based on your specific requirements.","title":"Terraform Starter"},{"location":"Terraform-Starter/#terraform-starter","text":"","title":"Terraform Starter"},{"location":"Terraform-Starter/#creating-aws-infrastructure-with-terraform","text":"Introduction This documentation provides step-by-step instructions on using Terraform to create an AWS EC2 instance in the eu-north-1 region. The example code specifies an AMI ID for ubuntu-focal-20.04-amd64 (ami-07ec4220c92589b40) and uses a t3.micro instance type.","title":"Creating AWS Infrastructure with Terraform"},{"location":"Terraform-Starter/#prerequisites","text":"AWS Account: Ensure that you have an AWS account. If you don't have one, you can sign up at AWS Console. AWS CLI Installation: Install the AWS CLI on your local machine. You can download it from the AWS CLI official website. Terraform Installation: Install Terraform on your local machine. You can download Terraform from the official Terraform website. AWS Credentials: Configure your AWS credentials on your machine using the AWS CLI. Run aws configure and provide your AWS Access Key, Secret Key, region (eu-north-1), and output format.","title":"Prerequisites"},{"location":"Terraform-Starter/#terraform-configuration","text":"Create a Terraform Configuration File: Create a file named main.tf and paste the following Terraform configuration: # Specify the AWS provider and region provider \"aws\" { region = \"eu-north-1\" # Change this to your desired AWS region } # Define an AWS EC2 instance resource \"aws_instance\" \"example_instance\" { ami = \"ami-07ec4220c92589b40\" # Change this to a valid AMI ID for your region instance_type = \"t3.micro\" # Change this to your desired instance type tags = { Name = \"example-instance\" } } Initialize Terraform: Open a terminal, navigate to the directory containing main.tf, and run: terraform init Apply Terraform Configuration: Run the following command to apply the Terraform configuration: terraform apply Terraform will prompt you to confirm the plan. Type yes and press Enter.","title":"Terraform Configuration"},{"location":"Terraform-Starter/#access-aws-console","text":"Visit the AWS Console and log in with your AWS credentials. Verify EC2 Instance: Navigate to the EC2 service in the AWS Console. You should see a new EC2 instance with the specified configuration.","title":"Access AWS Console:"},{"location":"Terraform-Starter/#conclusion","text":"You have successfully created an AWS EC2 instance using Terraform. This example can be expanded and customized based on your specific requirements.","title":"Conclusion"},{"location":"about/","text":"Who We Are Our career training turns ambitions into job-ready skills and business goals into tangible results. We follow the teaching method that helps students to understand the concepts and implement it by themselves. You can choose technology career track that includes Cloud Computing, Linux Administration, Network Engineering and Cybersecurity. The CloudSpace Academy training aims to make you an expert in your chosen career track and make you capable of implementing your skills in a job. Our training program has been recognized for empowering and teaching underprivileged communities.","title":"About"},{"location":"about/#who-we-are","text":"Our career training turns ambitions into job-ready skills and business goals into tangible results. We follow the teaching method that helps students to understand the concepts and implement it by themselves. You can choose technology career track that includes Cloud Computing, Linux Administration, Network Engineering and Cybersecurity. The CloudSpace Academy training aims to make you an expert in your chosen career track and make you capable of implementing your skills in a job. Our training program has been recognized for empowering and teaching underprivileged communities.","title":"Who We Are"},{"location":"aws-api-gateway/","text":"Aws Api Gateway Coming Soon !","title":"AWS Api Gateway"},{"location":"aws-api-gateway/#aws-api-gateway","text":"","title":"Aws Api Gateway"},{"location":"aws-api-gateway/#coming-soon","text":"","title":"Coming Soon !"},{"location":"aws-cloudtrail-log-file-integrity/","text":"Aws Cloudtrail Log File Integrity Coming Soon !","title":"AWS Cloudtrail Log File Integrity"},{"location":"aws-cloudtrail-log-file-integrity/#aws-cloudtrail-log-file-integrity","text":"","title":"Aws Cloudtrail Log File Integrity"},{"location":"aws-cloudtrail-log-file-integrity/#coming-soon","text":"","title":"Coming Soon !"},{"location":"aws-cognito-web-identity-federation/","text":"AWS Cognito Web Identity Federation Coming Soon !","title":"AWS Cognito Web Identity Federation"},{"location":"aws-cognito-web-identity-federation/#aws-cognito-web-identity-federation","text":"","title":"AWS Cognito Web Identity Federation"},{"location":"aws-cognito-web-identity-federation/#coming-soon","text":"","title":"Coming Soon !"},{"location":"aws-control-tower/","text":"AWS Control Tower Coming Soon !","title":"AWS Control Tower"},{"location":"aws-control-tower/#aws-control-tower","text":"","title":"AWS Control Tower"},{"location":"aws-control-tower/#coming-soon","text":"","title":"Coming Soon !"},{"location":"aws-dms-database-migration/","text":"AWS DMS Database Migration Coming Soon !","title":"AWS Dms Database Migration"},{"location":"aws-dms-database-migration/#aws-dms-database-migration","text":"","title":"AWS DMS Database Migration"},{"location":"aws-dms-database-migration/#coming-soon","text":"","title":"Coming Soon !"},{"location":"aws-dynamodb-lambda-trigger/","text":"AWS Dynamodb Lambda Trigger Coming Soon !","title":"AWS Dynamodb Lambda Trigger"},{"location":"aws-dynamodb-lambda-trigger/#aws-dynamodb-lambda-trigger","text":"","title":"AWS Dynamodb Lambda Trigger"},{"location":"aws-dynamodb-lambda-trigger/#coming-soon","text":"","title":"Coming Soon !"},{"location":"aws-efs/","text":"AWS EFS Coming Soon !","title":"AWS Efs"},{"location":"aws-efs/#aws-efs","text":"","title":"AWS EFS"},{"location":"aws-efs/#coming-soon","text":"","title":"Coming Soon !"},{"location":"aws-elastic-disaster-recovery/","text":"AWS Elastic Disaster Recovery Coming Soon !","title":"AWS Elastic Disaster Recovery"},{"location":"aws-elastic-disaster-recovery/#aws-elastic-disaster-recovery","text":"","title":"AWS Elastic Disaster Recovery"},{"location":"aws-elastic-disaster-recovery/#coming-soon","text":"","title":"Coming Soon !"},{"location":"aws-global-accelerator/","text":"AWS Global Accelerator Coming Soon !","title":"AWS Global Accelerator"},{"location":"aws-global-accelerator/#aws-global-accelerator","text":"","title":"AWS Global Accelerator"},{"location":"aws-global-accelerator/#coming-soon","text":"","title":"Coming Soon !"},{"location":"aws-iam-scp-permissions-boundary/","text":"AWS IAM Scp Permissions Boundary Coming Soon !","title":"AWS Iam Scp Permissions Boundary"},{"location":"aws-iam-scp-permissions-boundary/#aws-iam-scp-permissions-boundary","text":"","title":"AWS IAM Scp Permissions Boundary"},{"location":"aws-iam-scp-permissions-boundary/#coming-soon","text":"","title":"Coming Soon !"},{"location":"aws-lambda-s3-events/","text":"AWS Lambda S3 Events Coming Soon !","title":"AWS Lambda S3 Events"},{"location":"aws-lambda-s3-events/#aws-lambda-s3-events","text":"","title":"AWS Lambda S3 Events"},{"location":"aws-lambda-s3-events/#coming-soon","text":"","title":"Coming Soon !"},{"location":"aws-lambda-xray/","text":"AWS Lambda Xray Coming Soon !","title":"AWS Lambda Xray"},{"location":"aws-lambda-xray/#aws-lambda-xray","text":"","title":"AWS Lambda Xray"},{"location":"aws-lambda-xray/#coming-soon","text":"","title":"Coming Soon !"},{"location":"aws-lex-lambda-rds/","text":"AWS Lex Lambda RDS Coming Soon !","title":"AWS Lex Lambda Rds"},{"location":"aws-lex-lambda-rds/#aws-lex-lambda-rds","text":"","title":"AWS Lex Lambda RDS"},{"location":"aws-lex-lambda-rds/#coming-soon","text":"","title":"Coming Soon !"},{"location":"aws-macie/","text":"AWS Macie Coming Soon !","title":"AWS Macie"},{"location":"aws-macie/#aws-macie","text":"","title":"AWS Macie"},{"location":"aws-macie/#coming-soon","text":"","title":"Coming Soon !"},{"location":"aws-patch-manager/","text":"AWS Patch Manager Coming Soon !","title":"AWS Patch Manager"},{"location":"aws-patch-manager/#aws-patch-manager","text":"","title":"AWS Patch Manager"},{"location":"aws-patch-manager/#coming-soon","text":"","title":"Coming Soon !"},{"location":"aws-pet-rekognition-ecr/","text":"AWS Pet Rekognition ECR Coming Soon !","title":"AWS Pet Rekognition Ecr"},{"location":"aws-pet-rekognition-ecr/#aws-pet-rekognition-ecr","text":"","title":"AWS Pet Rekognition ECR"},{"location":"aws-pet-rekognition-ecr/#coming-soon","text":"","title":"Coming Soon !"},{"location":"aws-systems-manager/","text":"AWS Systems Manager Coming Soon !","title":"AWS Systems Manager"},{"location":"aws-systems-manager/#aws-systems-manager","text":"","title":"AWS Systems Manager"},{"location":"aws-systems-manager/#coming-soon","text":"","title":"Coming Soon !"},{"location":"aws-video-on-demand/","text":"AWS Video On Demand Coming Soon !","title":"AWS Video On Demand"},{"location":"aws-video-on-demand/#aws-video-on-demand","text":"","title":"AWS Video On Demand"},{"location":"aws-video-on-demand/#coming-soon","text":"","title":"Coming Soon !"},{"location":"aws-vpc-flow-logs/","text":"AWS Vpc Flow Logs Coming Soon !","title":"AWS Vpc Flow Logs"},{"location":"aws-vpc-flow-logs/#aws-vpc-flow-logs","text":"","title":"AWS Vpc Flow Logs"},{"location":"aws-vpc-flow-logs/#coming-soon","text":"","title":"Coming Soon !"},{"location":"aws-waf/","text":"AWS Waf Coming Soon !","title":"AWS Waf"},{"location":"aws-waf/#aws-waf","text":"","title":"AWS Waf"},{"location":"aws-waf/#coming-soon","text":"","title":"Coming Soon !"},{"location":"dockerbeginning/","text":"Flask Calculator Web Application Introduction This documentation provides an overview of the Flask calculator web application and guides you on how to set up and run the project. This simple web application allows users to perform basic arithmetic calculations such as addition, subtraction, multiplication, and division. Tools Requirements Python | installation guide Click Here Docker | installation guide Click Here Getting Started Open terminal and run this command to clone the repository of Flask Calculator Web Application. Command: git clone https://github.com/cloudspaceacademy/Flask-Calculator-app.git Now navigate to the directory Flask-Calculator-app. You will see a cloudspace directory now open that directory using the terminal. You can simply use this command: cd Flask-Calculator-app && cd cloudspace Now run ls command and Now what are the scenarios there you will see app.py,Dockerfile,requirements.txt file and a directory named templates. Let's elaborate why we use that Dockerfile for this application and how we write that Dockerfile. Why do we use docker? Docker provides a way to package, distribute, and run applications consistently across various environments. It simplifies the process of creating, deploying, and maintaining applications while improving portability, scalability, and resource efficiency. The Dockerfile is at the heart of this process, defining how the container image is built and what software is included, making it a valuable tool for modern software development and deployment. Now let's elaborate how we write that Dockerfile to package our application. Dockerfile FROM python:3.9-slim WORKDIR /app COPY requirements.txt . RUN pip install -r requirements.txt COPY . . EXPOSE 5000 CMD [\"python\", \"app.py\"] Now just break down all the code and elaborate why they use for FROM python:3.9-slim This line specifies the base image for our Docker container. In this case, it's using the official Python 3.9 slim image. The \"slim\" variant provides a smaller image size by excluding some non-essential components, making it suitable for many Python applications. WORKDIR /app This line sets the working directory inside the container to /app. This is where the application code and files will be placed, and it's the directory where commands will be executed by default. It's a good practice to organize your application within a dedicated directory. COPY requirements.txt . This line copies the requirements.txt file from the host system into the container. The . at the end specifies the destination path, which is the current directory within the container (i.e., /app). RUN pip install -r requirements.txt This line uses the RUN instruction to execute a command within the container. In this case, it runs pip install -r requirements.txt to install all the Python packages listed in the requirements.txt file. This step ensures that the necessary dependencies are installed in the container. COPY . . This line copies all the files and directories from the current directory (the directory where the Dockerfile is located) on our host system to the /app directory in the container. This includes our application code (app.py) and any other files or directories in the project folder. EXPOSE 5000 This line informs Docker that the container will listen on port 5000. While this line is not necessary for port mapping, it serves as a form of documentation to indicate which ports the container expects to use. It's good practice for clarity. CMD [\"python\", \"app.py\"] This line defines the default command to execute when the container is started. It runs the Flask application by executing python app.py. The Flask development server will start, and our application will be served on port 5000 as specified earlier. Now lets learn how to run the application by using just two docker commands from your local machine. Running the Application with Docker Building a Docker Image To build a Docker image, you use the docker build command. docker build -t flask-calculator . docker build -t flask-calculator . Run this command where the Dockerfile is located. Our Dockerfile is located in the root directory cloudspace so that we use ( . ) Now break down the code docker build: This is the Docker command for building an image. -t flask-calculator: This part specifies the image name and tag. In this example, the image is named \"flask-calculator,\" and it has no specific tag. The tag is used to version your images; if you don't specify one, it defaults to \"latest.\" .: The dot (.) at the end of the command tells Docker to use the current directory as the build context. It means that Docker will look in the current directory for a Dockerfile to use as the basis for building the image. Docker image used for : A Docker image is a lightweight, standalone, executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, system libraries, and settings. Images are used as the basis for creating Docker containers. Running a Docker Container Once we have built our Docker image, we can create and run containers from it using the docker run command. docker run -p 5000:80 -d flask-calculator Now break down the code docker run: This is the Docker command for creating and running a container. -p 5000:80: This part maps port 5000 from our host machine to port 80 inside the container. It allows us to access our Flask application running in the container on port 5000. -d: this defines the container will run in detached mode, which means it runs in the background. flask-calculator: This is the name of the Docker image you want to use to create the container. The image name corresponds to the one you specified when building the image. Docker Container used for : A Docker container is a runnable instance of a Docker image. It is an isolated environment where our application runs. Containers share the same OS kernel as the host system but are isolated from each other. They are ephemeral and can be started, stopped, and removed as needed. Access the application Now you will see the app is running from your local machine on port 5000. Open your web browser and go to localhost:5000 to access the Flask calculator web application within the Docker container.","title":"Docker Beginner"},{"location":"dockerbeginning/#flask-calculator-web-application","text":"Introduction This documentation provides an overview of the Flask calculator web application and guides you on how to set up and run the project. This simple web application allows users to perform basic arithmetic calculations such as addition, subtraction, multiplication, and division. Tools Requirements Python | installation guide Click Here Docker | installation guide Click Here","title":"Flask Calculator Web Application"},{"location":"dockerbeginning/#getting-started","text":"Open terminal and run this command to clone the repository of Flask Calculator Web Application. Command: git clone https://github.com/cloudspaceacademy/Flask-Calculator-app.git Now navigate to the directory Flask-Calculator-app. You will see a cloudspace directory now open that directory using the terminal. You can simply use this command: cd Flask-Calculator-app && cd cloudspace Now run ls command and Now what are the scenarios there you will see app.py,Dockerfile,requirements.txt file and a directory named templates. Let's elaborate why we use that Dockerfile for this application and how we write that Dockerfile.","title":"Getting Started"},{"location":"dockerbeginning/#why-do-we-use-docker","text":"Docker provides a way to package, distribute, and run applications consistently across various environments. It simplifies the process of creating, deploying, and maintaining applications while improving portability, scalability, and resource efficiency. The Dockerfile is at the heart of this process, defining how the container image is built and what software is included, making it a valuable tool for modern software development and deployment. Now let's elaborate how we write that Dockerfile to package our application.","title":"Why do we use docker?"},{"location":"dockerbeginning/#dockerfile","text":"FROM python:3.9-slim WORKDIR /app COPY requirements.txt . RUN pip install -r requirements.txt COPY . . EXPOSE 5000 CMD [\"python\", \"app.py\"] Now just break down all the code and elaborate why they use for FROM python:3.9-slim This line specifies the base image for our Docker container. In this case, it's using the official Python 3.9 slim image. The \"slim\" variant provides a smaller image size by excluding some non-essential components, making it suitable for many Python applications. WORKDIR /app This line sets the working directory inside the container to /app. This is where the application code and files will be placed, and it's the directory where commands will be executed by default. It's a good practice to organize your application within a dedicated directory. COPY requirements.txt . This line copies the requirements.txt file from the host system into the container. The . at the end specifies the destination path, which is the current directory within the container (i.e., /app). RUN pip install -r requirements.txt This line uses the RUN instruction to execute a command within the container. In this case, it runs pip install -r requirements.txt to install all the Python packages listed in the requirements.txt file. This step ensures that the necessary dependencies are installed in the container. COPY . . This line copies all the files and directories from the current directory (the directory where the Dockerfile is located) on our host system to the /app directory in the container. This includes our application code (app.py) and any other files or directories in the project folder. EXPOSE 5000 This line informs Docker that the container will listen on port 5000. While this line is not necessary for port mapping, it serves as a form of documentation to indicate which ports the container expects to use. It's good practice for clarity. CMD [\"python\", \"app.py\"] This line defines the default command to execute when the container is started. It runs the Flask application by executing python app.py. The Flask development server will start, and our application will be served on port 5000 as specified earlier. Now lets learn how to run the application by using just two docker commands from your local machine.","title":"Dockerfile"},{"location":"dockerbeginning/#running-the-application-with-docker","text":"","title":"Running the Application with Docker"},{"location":"dockerbeginning/#building-a-docker-image","text":"To build a Docker image, you use the docker build command. docker build -t flask-calculator . docker build -t flask-calculator . Run this command where the Dockerfile is located. Our Dockerfile is located in the root directory cloudspace so that we use ( . ) Now break down the code docker build: This is the Docker command for building an image. -t flask-calculator: This part specifies the image name and tag. In this example, the image is named \"flask-calculator,\" and it has no specific tag. The tag is used to version your images; if you don't specify one, it defaults to \"latest.\" .: The dot (.) at the end of the command tells Docker to use the current directory as the build context. It means that Docker will look in the current directory for a Dockerfile to use as the basis for building the image.","title":"Building a Docker Image"},{"location":"dockerbeginning/#docker-image-used-for","text":"A Docker image is a lightweight, standalone, executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, system libraries, and settings. Images are used as the basis for creating Docker containers.","title":"Docker image used for :"},{"location":"dockerbeginning/#running-a-docker-container","text":"Once we have built our Docker image, we can create and run containers from it using the docker run command. docker run -p 5000:80 -d flask-calculator Now break down the code docker run: This is the Docker command for creating and running a container. -p 5000:80: This part maps port 5000 from our host machine to port 80 inside the container. It allows us to access our Flask application running in the container on port 5000. -d: this defines the container will run in detached mode, which means it runs in the background. flask-calculator: This is the name of the Docker image you want to use to create the container. The image name corresponds to the one you specified when building the image.","title":"Running a Docker Container"},{"location":"dockerbeginning/#docker-container-used-for","text":"A Docker container is a runnable instance of a Docker image. It is an isolated environment where our application runs. Containers share the same OS kernel as the host system but are isolated from each other. They are ephemeral and can be started, stopped, and removed as needed.","title":"Docker Container used for :"},{"location":"dockerbeginning/#access-the-application","text":"Now you will see the app is running from your local machine on port 5000. Open your web browser and go to localhost:5000 to access the Flask calculator web application within the Docker container.","title":"Access the application"},{"location":"dockercomposebeginning/","text":"Flask Calculator Web Application Introduction This documentation provides an overview of the Flask calculator web application and guides you on how to set up and run the project. The Flask calculator is a straightforward web application designed for basic arithmetic calculations, including addition, subtraction, multiplication, and division. Tools Requirements Python | installation guide Click Here Docker | installation guide Click Here Getting Started Open your terminal and clone the Flask Calculator Web Application repository using the following command: git clone https://github.com/cloudspaceacademy/Flask-Calculator-app.git Navigate to the \"Flask-Calculator-app\" directory By running this command : cd Flask-Calculator-app Inside the \"Flask-Calculator-app\" directory, you will find a \"cloudspace\" directory. Enter this directory using the following command: cd cloudspace In Previous session \"Docker Beginner\" you have learned about how to run this \"Flask Calculator Web Application\" using Dockerfile .Now we are gonna learn how to run this application using docker-compose.yml file . In Previous session \"Docker Beginner\" you have learned about Dockerfile .How To write a Dockerfile,why its used for,how to run the application using Dockerfile etc.Now let's jump up to docker-compose.yml file. docker-compose.yml file used for A docker-compose.yml file is used to define a multi-container Docker application. It allows you to define and configure multiple services, networks, and volumes in a single file. Docker Compose simplifies the process of orchestrating complex applications by providing a way to manage multiple containers as a single application stack. Simply Dockerfile is used for single service or to run the application using one container and docker-compose.yml file is used for multiple services or to packaging and run the application using multi containers. docker-compose.yml version: '3.8' services: flask-calculator: build: context: . dockerfile: Dockerfile ports: - \"8080:80\" Let's Break down the code version: '3.8' :Specifies the version of the Docker Compose file syntax. In this case, it's using version 3.8. services :Defines the services that make up the Docker application. flask-calculator :The name of the service. In this case, it's named flask-calculator. build :Specifies how to build the Docker image for the service. context: . :Specifies the build context, which is the path to the directory containing the Dockerfile and any files needed for the build. In this case, it's set to the current directory (.) dockerfile: Dockerfile :Specifies the name of the Dockerfile to use for building the image. In this case, it's named Dockerfile. ports :Specifies the ports to expose from the container. - \"8080:80\": Maps port 80 from the container to port 8080 on the host machine. The format is host_port:container_port. Running the Application with Docker Compose To run the application run this command where the docker-compose.yml file is located. Command : docker-compose up -d Break down Codes: docker-compose : The Docker Compose command-line tool. up : This command is used to create and start containers based on the configurations specified in the docker-compose.yml file. -d : Stands for \"detached mode.\" When you run Docker containers in detached mode, the containers run in the background, and you get your terminal prompt back. This allows you to continue using the same terminal session for other tasks. Now Run docker ps command to see the running container id that you have just created. Access the application Now you will see the app is running from your local machine on port 8080. Open your web browser and go to localhost:8080 to access the Flask calculator web application within the Docker container.","title":"Docker Compose Beginner"},{"location":"dockercomposebeginning/#flask-calculator-web-application","text":"Introduction This documentation provides an overview of the Flask calculator web application and guides you on how to set up and run the project. The Flask calculator is a straightforward web application designed for basic arithmetic calculations, including addition, subtraction, multiplication, and division. Tools Requirements Python | installation guide Click Here Docker | installation guide Click Here","title":"Flask Calculator Web Application"},{"location":"dockercomposebeginning/#getting-started","text":"Open your terminal and clone the Flask Calculator Web Application repository using the following command: git clone https://github.com/cloudspaceacademy/Flask-Calculator-app.git Navigate to the \"Flask-Calculator-app\" directory By running this command : cd Flask-Calculator-app Inside the \"Flask-Calculator-app\" directory, you will find a \"cloudspace\" directory. Enter this directory using the following command: cd cloudspace In Previous session \"Docker Beginner\" you have learned about how to run this \"Flask Calculator Web Application\" using Dockerfile .Now we are gonna learn how to run this application using docker-compose.yml file . In Previous session \"Docker Beginner\" you have learned about Dockerfile .How To write a Dockerfile,why its used for,how to run the application using Dockerfile etc.Now let's jump up to docker-compose.yml file.","title":"Getting Started"},{"location":"dockercomposebeginning/#docker-composeyml-file-used-for","text":"A docker-compose.yml file is used to define a multi-container Docker application. It allows you to define and configure multiple services, networks, and volumes in a single file. Docker Compose simplifies the process of orchestrating complex applications by providing a way to manage multiple containers as a single application stack. Simply Dockerfile is used for single service or to run the application using one container and docker-compose.yml file is used for multiple services or to packaging and run the application using multi containers.","title":"docker-compose.yml file used for"},{"location":"dockercomposebeginning/#docker-composeyml","text":"version: '3.8' services: flask-calculator: build: context: . dockerfile: Dockerfile ports: - \"8080:80\" Let's Break down the code version: '3.8' :Specifies the version of the Docker Compose file syntax. In this case, it's using version 3.8. services :Defines the services that make up the Docker application. flask-calculator :The name of the service. In this case, it's named flask-calculator. build :Specifies how to build the Docker image for the service. context: . :Specifies the build context, which is the path to the directory containing the Dockerfile and any files needed for the build. In this case, it's set to the current directory (.) dockerfile: Dockerfile :Specifies the name of the Dockerfile to use for building the image. In this case, it's named Dockerfile. ports :Specifies the ports to expose from the container. - \"8080:80\": Maps port 80 from the container to port 8080 on the host machine. The format is host_port:container_port.","title":"docker-compose.yml"},{"location":"dockercomposebeginning/#running-the-application-with-docker-compose","text":"To run the application run this command where the docker-compose.yml file is located. Command : docker-compose up -d Break down Codes: docker-compose : The Docker Compose command-line tool. up : This command is used to create and start containers based on the configurations specified in the docker-compose.yml file. -d : Stands for \"detached mode.\" When you run Docker containers in detached mode, the containers run in the background, and you get your terminal prompt back. This allows you to continue using the same terminal session for other tasks. Now Run docker ps command to see the running container id that you have just created.","title":"Running the Application with Docker Compose"},{"location":"dockercomposebeginning/#access-the-application","text":"Now you will see the app is running from your local machine on port 8080. Open your web browser and go to localhost:8080 to access the Flask calculator web application within the Docker container.","title":"Access the application"}]}