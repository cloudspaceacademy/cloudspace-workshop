{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Cloud Space","text":"<p>Website : www.cloudspaceacademy.com   info@cloudspaceacademy.com   Phone: +1-855-200-7653   WhatsApp: +1-571-454-4691  Facebook </p> <p>At CloudSpace, our students' success is our top priority. We understand it can be difficult to study alone once the BootCamp is over. CloudSpace is committed to providing you with the necessary support until you land that dream job. The FREE follow-up program is a dedicated place to keep learning with an industry leader DevOps instructor until you land a job. All this free of charge as long as you have completed the AWS Cloud &amp; DevOps BootCamp with us. So after completing the 6 months AWS &amp; DevOps Bootcamp, you can attend the Free Follow-up Program free of charge on a weekly basis to keep working on interview preparation and DevOps projects with an experienced instructor. So you will be allowed to attend this post-bootcamp until you land a job.</p> <p>Follow-Up Program Schedule: Thursdays (8PM-9PM) Interview Prep - Sundays (1PM-3PM) Projects Hands On</p>"},{"location":"AWS%20E-Commerce%20Application/","title":"AWS E-Commerce Application DevOps","text":"<p>Description</p> <p>Develop and deploy a full-fledged e-commerce application in an AWS environment. This includes a database, S3 bucket for storage, ECS for deployment, and infrastructure as code with CloudFormation.</p> <p>Tools</p> <p>\u25cf Infrastructure as Code: AWS CloudFormation to define the infrastructure.</p> <p>\u25cf Database: AWS RDS or DynamoDB for database management.</p> <p>\u25cf Storage: AWS S3 for file storage.</p> <p>\u25cf Deployment: AWS ECS for container-based deployment.</p> <p>\u25cf CI/CD: Automate deployment using GitHub Actions.</p>"},{"location":"AWS%20Photo%20Gallery%20Application/","title":"AWS Photo Gallery Application DevOps","text":"<p>Description</p> <p>This application is a photo gallery platform similar to Pexels . Users can freely upload and download images. It leverages AWS services for scalability and reliability.</p> <p>Tools</p> <p>\u25cf Version Control: AWS CodeCommit for code storage.</p> <p>\u25cf CI/CD: AWS CodePipeline for automating deployment.</p> <p>\u25cf Infrastructure as Code: Terraform to define infrastructure components such as S3 buckets, ECS clusters, EC2 instances for MongoDB databases.</p> <p>\u25cf Monitoring and Alerting: Amazon CloudWatch for monitoring and alerting.</p>"},{"location":"Ansible-Playbook-Library/","title":"Ansible Playbook Library","text":""},{"location":"Ansible-Playbook-Library/#coming-soon","title":"Coming Soon !","text":""},{"location":"Banking%20Web%20Application/","title":"Banking Web Application DevOps","text":"<p>Create a web-based banking application that includes load balancing using Nginx and microservice tracking using Application Gateway with distributed tracing.</p> <p>Tools</p> <p>\u25cf Web Server: Nginx for load balancing.</p> <p>\u25cf Microservice Tracking: AWS Elastic Load Balancing for managing web traffic and enabling distributed tracing.</p>"},{"location":"CI-CD-Beginner/","title":"CI/CD Beginner","text":"<p>In the ever-evolving landscape of software development, the seamless integration of efficient and reliable Continuous Integration/Continuous Deployment (CI/CD) pipelines is imperative for delivering high-quality applications with speed and consistency. This project embarks on the journey of demystifying the CI/CD process using GitHub Actions and orchestrating the deployment of a Dockerized application to both development and production environments within the Amazon Web Services (AWS) ecosystem.</p> <p>The core objective of this project is to provide a clear understanding of the fundamental concepts surrounding CI/CD, containerization with Docker, and deployment in AWS environments. By leveraging GitHub Actions, a powerful and flexible automation tool integrated into the GitHub platform, developers gain the ability to automate workflows, conduct automated testing, and seamlessly deploy applications, all within the same version control environment.</p> <p>The chosen technology stack revolves around Docker, offering containerization to encapsulate the application and its dependencies, ensuring consistency across various environments. AWS serves as the deployment platform, with a focus on providing scalable and resilient infrastructure to host Dockerized applications.</p> <p>Throughout this project, we will explore the step-by-step process of building a CI/CD pipeline, containerizing an application, and deploying it to both a development and a production environment. By the end, developers will not only have a functional CI/CD pipeline but will also grasp the principles and practices essential for streamlining software development workflows in a modern, cloud-native context. Let's embark on this journey to unlock the potential of CI/CD, Docker, and AWS for a more efficient and robust software development lifecycle.</p>"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/","title":"Creating a Kubernetes Cluster with Minikube","text":"<p>Introduction</p> <p>Minikube is a tool that allows you to run Kubernetes clusters on your local machine. It's great for development, testing, and learning Kubernetes concepts. This guide will walk you through the process of setting up a Kubernetes cluster using Minikube.</p> <p>Prerequisites</p> <p>Hypervisor: Minikube requires a hypervisor to run the virtual machines. Common choices include VirtualBox, Hyper-V, KVM, or Docker. Ensure your preferred hypervisor is installed and properly configured.We use Docker for that </p> <p>Docker | installation guide Click Here</p> <p>kubectl: The kubectl command-line tool is used to interact with your Kubernetes cluster. Make sure it's installed on your machine.</p> <p>Kubectl | installation guide Click Here</p>"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#install-minikube","title":"Install Minikube","text":"<p>Visit the Minikube Installation Guide and follow the instructions for your operating system to install Minikube.</p> <p>Minikube | installation guide Click Here</p>"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#start-minikube","title":"Start Minikube","text":"<p>Open a terminal and run the following command to start Minikube:</p> <p><code>minikube start</code></p> <p>This command will download the Minikube ISO, start a virtual machine, and set up the Kubernetes cluster.</p>"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#verify-cluster-status","title":"Verify Cluster Status","text":"<p>After Minikube has started, you can check the cluster status using:</p> <p><code>kubectl cluster-info</code></p> <p>This command will display information about the cluster, including the Kubernetes master and services.</p>"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#check-nodes","title":"Check Nodes","text":"<p>Verify that Minikube has created a node for your cluster:</p> <p><code>kubectl get nodes</code></p> <p>This command should show the Minikube node with a status of Ready.</p>"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#kubernetes-dashboard-optional","title":"Kubernetes Dashboard (Optional)","text":"<p>If you want to use the Kubernetes Dashboard, you can start it with:</p> <p><code>minikube dashboard</code></p> <p>This will open a web browser with the Kubernetes Dashboard, providing a visual interface to manage your cluster.</p>"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#interact-with-kubernetes","title":"Interact with Kubernetes","text":"<p>Now that your Minikube cluster is running, you can start deploying applications and managing your Kubernetes cluster using kubectl.</p>"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#example-deployment","title":"Example Deployment","text":"<p>As a simple test, let's deploy a sample Nginx application. Save the following YAML as nginx-deployment.yaml:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n        ports:\n        - containerPort: 80\n</code></pre> <p>Apply the configuration:</p> <p><code>kubectl apply -f nginx-deployment.yaml</code></p> <p>This will deploy two replicas of the Nginx web server.</p>"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#conclusion","title":"Conclusion","text":"<p>Congratulations! You've successfully created a Kubernetes cluster using Minikube and deployed a sample application. This is just the beginning\u2014explore more Kubernetes features and concepts as you continue your journey with container orchestration.</p>"},{"location":"Logging-with-ELK-Stack/","title":"Logging with ELK Stack","text":""},{"location":"Logging-with-ELK-Stack/#coming-soon","title":"Coming Soon !","text":""},{"location":"Monitoring-with-Prometheus/","title":"Monitoring with Prometheus","text":"<p>In the intricate tapestry of modern software architecture, the ability to monitor and analyze the performance and health metrics of applications is pivotal. This project embarks on a journey to demystify the realm of monitoring by introducing Prometheus, a powerful open-source monitoring and alerting toolkit, and Grafana, a versatile platform for visualizing time-series data.</p> <p>Monitoring applications in real-time is indispensable for identifying bottlenecks, detecting anomalies, and ensuring optimal performance. Prometheus, with its robust data model and flexible querying language, has emerged as a cornerstone in the domain of monitoring, providing developers and operators with the tools needed to gain insights into the intricate workings of their systems.</p> <p>This project is designed to elucidate the fundamental concepts of Prometheus and Grafana by guiding participants through the process of setting up a monitoring infrastructure for their applications. By integrating Prometheus to collect and store time-series data and Grafana to create intuitive dashboards, developers can gain actionable insights into the health and performance of their systems.</p> <p>Throughout this project, we will delve into the intricacies of Prometheus, exploring its capabilities for metric collection, querying, and alerting. Grafana will then complement this monitoring setup by providing a visually appealing and customizable interface to represent and analyze the collected metrics.</p> <p>By the culmination of this project, participants will have a solid understanding of how to implement Prometheus for monitoring and Grafana for visualization, empowering them to make informed decisions based on real-time data. Let's embark on this exploration of monitoring and visualization to unlock the potential of Prometheus and Grafana in enhancing the observability of your applications.</p>"},{"location":"Open%20Source%20Photo%20Gallery%20Application/","title":"Open Source Photo Gallery Application DevOps","text":"<p>Description</p> <p>An open-source version of the photo gallery application.</p> <p>Tools</p> <p>\u25cf Version Control: GitHub for code storage.</p> <p>\u25cf CI/CD: GitHub Actions for automating deployment.</p> <p>\u25cf Infrastructure as Code: Terraform for defining infrastructure elements.</p> <p>\u25cf Database and Log Monitoring: ELK Stack (Elasticsearch Logstash, Kibana) for log monitoring.</p> <p>\u25cf Alerting: Prometheus and Grafana for alerting.</p> <p>\u25cf Automatic Deployment: Deploy the application automatically.</p>"},{"location":"Real-Time%20Chat%20Application/","title":"Real-Time Chat Application DevOps","text":"<p>Description</p> <p>Create a real-time chat application using message brokers and set up Application Performance Metrics (APM) in a Kubernetes cluster.</p> <p>Tools</p> <p>\u25cf Container Orchestration: Kubernetes for managing containers and applications.</p> <p>\u25cf Message Brokers: Utilize message brokers like Apache Kafka or RabbitMQ for real-time messaging.</p>"},{"location":"Scan-Docker-Container/","title":"Scan Docker Container","text":"<p>This project aims to elucidate the crucial practices of security and vulnerability checking within Docker containers through the lens of CI/CD pipelines. By integrating automated testing and security scanning into the development workflow, developers can proactively identify and address security vulnerabilities, thereby bolstering the resilience of their applications.</p> <p>The pipeline will be designed to execute automated tests, validating the functional aspects of the application, and concurrently performing security scans to scrutinize the containerized environment for potential vulnerabilities. This dual-pronged approach ensures that the development process not only meets functional requirements but also adheres to fundamental security principles.</p> <p>By the culmination of this project, participants will have a practical understanding of how to implement an end-to-end CI/CD pipeline that includes automated testing for functional correctness and security scans to fortify Docker containers against potential vulnerabilities. Through this exploration, developers will be equipped with the knowledge and tools to embed security into their development workflows, fostering a proactive and secure approach to containerized application deployment. Let's embark on this journey to integrate security seamlessly into the CI/CD pipeline, fortifying our applications in the ever-evolving landscape of software development.</p>"},{"location":"Solution-Architecht/","title":"Solution Architecht Projects","text":"<p>Description</p> <p>Create a real-time chat application using message brokers and set up Application Performance Metrics (APM) in a Kubernetes cluster.</p> <p>Tools</p> <p>\u25cf Container Orchestration: Kubernetes for managing containers and applications.</p> <p>\u25cf Message Brokers: Utilize message brokers like Apache Kafka or RabbitMQ for real-time messaging.</p>"},{"location":"Terraform-Starter/","title":"Terraform Starter","text":""},{"location":"Terraform-Starter/#coming-soon","title":"Coming Soon !","text":""},{"location":"about/","title":"About","text":""},{"location":"about/#who-we-are","title":"Who We Are","text":"<p>Our career training turns ambitions into job-ready skills and business goals into tangible results. We follow the teaching method that helps students to understand the concepts and implement it by themselves.</p> <p>You can choose technology career track that includes Cloud Computing, Linux Administration, Network Engineering and Cybersecurity.</p> <p>The CloudSpace Academy training aims to make you an expert in your chosen career track and make you capable of implementing your skills in a job. Our training program has been recognized for empowering and teaching underprivileged communities.</p>"},{"location":"aws-api-gateway/","title":"API Gateway integration with Lambda, Mock, and AWS Service integrations","text":""},{"location":"aws-api-gateway/#overview","title":"Overview","text":"<p>We\u2019re going to set up an API Gateway REST API, with a few different endpoints, including Mock, Lambda, and AWS service (SNS) integrations.</p> <p>You can read about the differences between REST and HTTP APIs here: https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-vs-rest.html</p> <p>For this demo we will use REST, because it allows the use of Mock integrations.</p> <p>I will be creating this in the ap-southeast-2 region, so all links to the console will be there. Make sure you change region if you\u2019re deploying elsewhere.</p>"},{"location":"aws-api-gateway/#instructions","title":"Instructions","text":""},{"location":"aws-api-gateway/#stage-1-setting-up-sns","title":"Stage 1 - Setting up SNS","text":"<p>Head to the SNS console: https://ap-southeast-2.console.aws.amazon.com/sns/v3/home?region=ap-southeast-2#/topics</p> <p>Click on Create topic</p> <p>Set the Type to \u201cStandard\u201d</p> <p>Set the Name to be \u201cAPI-Messages\u201d</p> <p>Under Access policy, leave the Method as \u201cBasic\u201d</p> <p>Change Define who can publish messages to the topic to \u201cOnly the specified AWS accounts\u201d and enter your account ID (found in the top right of the console)</p> <p>Change Define who can subscribe to this topic to \u201cOnly the specified AWS accounts\u201d and enter your account ID again</p> <p>In the real world, this should be locked down further to only the resources you want publishing to the topic, but in this temporary example set up, locking down to just the account is fine and safe enough</p> <p>Leave all other options as default</p> <p>Click on Create topic</p> <p>On the next page, click on Create subscription</p> <p>Change the Protocol to \u201cEmail\u201d</p> <p>In the Endpoint field, enter your personal email</p> <p>Click Create subscription</p> <p>You will receive a confirmation email shortly after, with a link you need to click on. This tells SNS that you\u2019re happy to receive emails from the topic, and prevents spam from being sent via SNS.</p> <p>Side note: While writing this, my confirmation went to Spam in Gmail, so don\u2019t forget to check there.</p> <p>Your subscription should now be in the Confirmed state:</p> <p></p>"},{"location":"aws-api-gateway/#stage-2-create-the-lambda","title":"Stage 2 - Create the Lambda","text":"<p>Head to the Lambda console: https://ap-southeast-2.console.aws.amazon.com/lambda/home?region=ap-southeast-2#/functions</p> <p>Click Create function</p> <p>Leave Author from scratch selected</p> <p>Set the Function name to <code>api-return-ip</code></p> <p>Set the Runtime to \u201cPython 3.9\u201d</p> <p>Leave the Architecture as \u201cx86_64\u201d</p> <p>Click Create function</p> <p>In the Code tab, enter the following code:</p> <pre><code>def lambda_handler(event, context):   \n    return {\n        'statusCode': 200,\n        'headers': {},\n        'body': event['requestContext']['identity']['sourceIp'],\n        'isBase64Encoded': False\n        }\n</code></pre> <p>This is an extremely basic function that just returns the source IP of the requester (you).</p> <p>Don\u2019t forget to click Deploy to save the function.</p> <p></p>"},{"location":"aws-api-gateway/#stage-3-create-the-api","title":"Stage 3 - Create the API","text":"<p>Head to the API Gateway console: https://ap-southeast-2.console.aws.amazon.com/apigateway/main/apis?region=ap-southeast-2</p> <p>Click Create API</p> <p>Select REST API \u2192 Build</p> <p>Make sure you don\u2019t select \u201cREST API Private\u201d.</p> <p></p> <p>Leave the Protocol and \u201cCreate new API\u201d options as is, and set your API name to whatever you like</p> <p></p> <p>Click Create API</p> <p>Once that\u2019s done you will see all of the \u201cResources\u201d (endpoints / API paths), right now we have none, so click on Actions and then \u201cCreate Resource\u201d</p> <p></p> <p>This first resource we create will be for the Mock integration, so we\u2019ll just name it \u201cMock\u201d. The Resource Path is the URL path you will use to call it, so in this case it would be something like</p> <p><code>https://abcdef1234.execute-api.ap-southeast-2.amazonaws.com/mock</code></p> <p></p> <p>Next we have to attach a Method. A Method is the HTTP method that the resource (path) will accept, such as \u201cGET\u201d, \u201cPOST\u201d, \u201cDELETE\u201d, etc.</p> <p>For the Mock integration, we will just use \u201cGET\u201d. </p> <p>Make sure you are in the /mock resource</p> <p></p> <p>Then click Actions then \u201cCreate Method\u201d</p> <p></p> <p>Select \u201cGET\u201d</p> <p></p> <p>Then click the tick to accept.</p> <p>Once that\u2019s done, API Gateway will present a list of possible integrations. For this one, select \u201cMock\u201d then click Save</p> <p></p> <p>Once that\u2019s done, click on \u201cIntegration Response\u201d</p> <p></p> <p>This is where we tell API Gateway what the Mock integration should respond with. </p> <p>Expand the 200 status line, then the Mapping Templates section, and set the Content-Type to <code>application/json</code></p> <p></p> <p>Click on the tick, then in the template section, enter the following (you can replace the message with whatever you like)</p> <pre><code>{\n    \"statusCode\": 200,\n    \"message\": \"This response is mocking you\"\n}\n</code></pre> <p>Then click Save (it won\u2019t give any feedback that it\u2019s saved, but it has)</p> <p></p> <p>Then click Save on the method response</p> <p></p> <p>That\u2019s all done. Now we\u2019ll set up the Lambda integration.</p> <p>Go back to the root (<code>/</code>)resource</p> <p></p> <p>Then click Actions then Create Resource</p> <p>For this one, set the resource name to \u201cLambda\u201d, and leave the Resource Path as \u201c/lambda\u201d</p> <p>Click Create Resource</p> <p>Click on Actions then Create Method. This will also be a \u201cGET\u201d</p> <p></p> <p>On the next page, set the \u201cIntegration type\u201d to \u201cLambda function\u201d</p> <p>Enable \u201cUse Lambda Proxy integration\u201d</p> <p>Once you click the \u201cLambda Function\u201d text field, it should drop down with a list of Lambda\u2019s in that region, select the one you created earlier</p> <p>Leave all other options as is, and click Save</p> <p></p> <p>You should see a popup telling you that you\u2019re about to give API Gateway permission to invoke your Lambda, click OK</p> <p>Lastly we\u2019ll set up another resource for SNS.</p> <p>For this one, we will need to set up an IAM role that API Gateway will use to publish messages to SNS. </p> <p>Head to the IAM Console: https://us-east-1.console.aws.amazon.com/iamv2/home?region=ap-southeast-2#/roles</p> <p>Go to the Roles page, and click Create Role</p> <p></p> <p>Under \u201cTrusted entity\u201d, select \u201cAWS Service\u201d, and in the drop-down, select API Gateway. Make sure you select the radio button for \u201cAPI Gateway\u201d as well.</p> <p></p> <p>Click Next</p> <p>On the Permissions page, click Next</p> <p>Lastly, set the role name to \u201capi-gw-sns-role\u201d then click Create role</p> <p>Now go into the role you just created</p> <p></p> <p>Click on Add permissions then Create inline policy</p> <p></p> <p>Go to the JSON tab and enter the following</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"sns:Publish\",\n            \"Resource\": \"*\"\n        }\n    ]\n}\n</code></pre> <p>Click Review policy</p> <p>Under \u201cReview policy\u201d set the name to \u201cSnsPublish\u201d and click Create policy</p> <p>On the summary page, copy the ARN of the role you just created, you will need it for the next step</p> <p></p> <p>Now head back to the API Gateway console: https://ap-southeast-2.console.aws.amazon.com/apigateway/main/apis?region=ap-southeast-2</p> <p>Go back into your REST API</p> <p>Go back to the root resource</p> <p></p> <p>Then click Actions then Create Resource</p> <p>For this one, set the resource name to \u201cSNS\u201d, and leave the Resource Path as \u201c/sns\u201d</p> <p>Click Create Resource</p> <p>Click on Actions then Create Method. </p> <p>This one will be a \u201cPOST\u201d.</p> <p>On the next page, set the \u201cIntegration type\u201d to \u201cAWS Service\u201d</p> <p>Set the AWS Region as the same region as your API / SNS (for me this is ap-southeast-2)</p> <p>Set the AWS Service as \u201cSimple Notification Service (SNS)\u201d</p> <p>Leave the AWS Subdomain blank</p> <p>Set the HTTP method to POST</p> <p>Leave the Action Type as \u201cUse action name\u201d</p> <p>Set the \u201cAction\u201d to \u201cPublish\u201d</p> <p>Under Execution Role you need to put the ARN of the IAM role you just created for SNS.  </p> <p>Leave the rest of the form as is, and click Save</p> <p></p> <p>If you\u2019re wondering where we got the \u201cPublish\u201d action, and where to find other Action types, you can view them all in the API Reference for the service: https://docs.aws.amazon.com/sns/latest/api/API_Operations.html</p> <p>There\u2019s a few ways to pass your message through API Gateway to the AWS service, but to make things easy for this example, we\u2019re going to use Query Strings. Query Strings are found in the URL of websites after a question mark, e.g. <code>https://google.com/search?q=hello</code>, where \u201cq=hello\u201d is the query string.</p> <p>Go to the \u201c/sns\u201d resource, and the \u201cPOST\u201d method, and click \u201cMethod Request\u201d </p> <p></p> <p>Under \u201cURL Query String Parameters\u201d click \u201cAdd query string\u201d and enter \u201cTopicArn\u201d and click the tick.</p> <p>Then click \u201cAdd query string\u201d and enter \u201cMessage\u201d and click the tick.</p> <p>Your query string should look like this</p> <p></p> <p>Go back to the \u201c/sns\u201d resource, and the \u201cPOST\u201d method, and click \u201cIntegration Request\u201d</p> <p></p> <p>Under \u201cURL Query String Parameters\u201d click \u201cAdd query string\u201d </p> <p>Set the Name to \u201cMessage\u201d and the Mapped from to <code>method.request.querystring.Message</code></p> <p>Click the tick to save</p> <p>Click \u201cAdd query string\u201d again</p> <p>Set the Name to \u201cTopicArn\u201d and the Mapped from to <code>method.request.querystring.TopicArn</code></p> <p>Click the tick to save</p> <p>Now we need to Deploy the API. </p> <p>Click on Actions, then Deploy API</p> <p></p> <p>In the pop up window, set the \u201cDeployment stage\u201d to \u201c[New Stage]\u201d, and the \u201cStage name\u201d to \u201cv1\u201d.</p> <p></p> <p>The \u201cStage name\u201d can really be anything, and is used to direct API requests to different \u201cversions\u201d of the API. So you could have a \u201cdev\u201d stage, and \u201cprod\u201d stage, or just use the standard \u201cv1\u201d, \u201cv2\u201d, etc.</p> <p>Click Deploy</p> <p>Once that\u2019s done, you will be sent to the Stage Editor page, where you can set things like Rate Limiting, WAF associations, caching, logs, etc. We don\u2019t need to change any of these for this demo.</p> <p>At the top of the screen you will see your API URL</p> <p></p> <p>Copy that URL for the next step.</p>"},{"location":"aws-api-gateway/#stage-4-testing-the-api","title":"Stage 4 - Testing the API","text":"<p>The Lambda and Mock resources can be tested in the browser. By default, any URL you enter into your browser performs a GET request (and remember, only our Lambda and Mock resources have the GET method set).</p>"},{"location":"aws-api-gateway/#mock","title":"Mock","text":"<p>If we visit our API URL and append \u201c/mock\u201d we should see the response we entered earlier</p> <p></p> <p>The reason we want a JSON output rather than a friendly human readable one, is because working with JSON in programming languages makes things much easier. Your code or application could read the \u201cstatusCode\u201d key and see a 200 value, and then it could read the \u201cmessage\u201d key and see it\u2019s value.</p>"},{"location":"aws-api-gateway/#lambda","title":"Lambda","text":"<p>Now if we visit our \u201c/lambda\u201d URL we should see our function response (your IP address)</p> <p></p>"},{"location":"aws-api-gateway/#sns","title":"SNS","text":"<p>For the last endpoint, we can\u2019t use a browser because browsers by default only perform \u201cGET\u201d requests, so we have a couple of options to test this endpoint.</p>"},{"location":"aws-api-gateway/#command-line","title":"Command Line","text":"<p>If you\u2019re comfortable using the command line (Linux, Mac, or WSL for Windows), follow these steps. If you would prefer to use a GUI, skip ahead.</p> <p>In your CLI, run the following command (replace \u201cREGION\u201d and \u201cACCOUNT-ID\u201d with your region and account ID), and don\u2019t forget to replace the URL with your API gateway URL</p> <pre><code>curl -X POST -G -d 'TopicArn=arn:aws:sns:REGION:ACCOUNT-ID:API-Messages' -d 'Message=Hello!'  https://abc123def.execute-api.ap-southeast-2.amazonaws.com/v1/sns\n</code></pre> <p>Note if you want to use spaces in your message, because query parameters are URL encoded, spaces need to be replaced with a +, so for example:</p> <pre><code>curl -X POST -G -d 'TopicArn=arn:aws:sns:REGION:ACCOUNT-ID:API-Messages' -d 'Message=I+like+sending+long+messages'  https://abc123def.execute-api.ap-southeast-2.amazonaws.com/v1/sns\n</code></pre>"},{"location":"aws-api-gateway/#gui-aws-console","title":"GUI (AWS Console)","text":"<p>The API Gateway console provides a handy way to test your API. </p> <p></p> <p>On the testing page, under \u201cQuery Strings\u201d, enter the following. Replace the TopicArn with your SNS Topic ARN, and the message with whatever you like.</p> <pre><code>TopicArn=arn:aws:sns:REGION:ACCOUNT-ID:API-Messages&amp;Message=APIs+are+fun\n</code></pre> <p>Note if you want to use spaces in your message, because query parameters are URL encoded, spaces need to be replaced with a +</p> <p>Scroll down and click Test</p> <p></p>"},{"location":"aws-api-gateway/#gui-postman","title":"GUI (Postman)","text":"<p>We can also use a popular API testing tool called Postman. It\u2019s available for Windows, Mac, and Linux</p> <p>It can be downloaded for free from their website: https://www.postman.com/downloads/</p> <p>I won\u2019t write instructions on installing the program, it\u2019s fairly self explanatory and isn\u2019t related to this demo.</p> <p>Once you\u2019ve opened Postman, click on the + up the top of the application to open a new tab. Set the method to POST, and enter your API Gateway URL.</p> <p>You can then enter the two Query Parameters we set earlier:</p> <p><code>TopicArn</code> which is the ARN of your SNS topic.</p> <p><code>Message</code> which is the message you want to send to the topic</p> <p></p> <p>Once you\u2019ve entered all that, hit Send</p> <p>No matter which method you chose, you should receive an email from SNS containing the message in your Query String</p> <p></p>"},{"location":"aws-api-gateway/#stage-5-cleaning-up","title":"Stage 5 - Cleaning up","text":"<p>Head to the API Gateway console: https://ap-southeast-2.console.aws.amazon.com/apigateway/main/apis?region=ap-southeast-2</p> <p>Select the API you created, and click Actions then Delete</p> <p></p> <p>Head to the SNS console: https://ap-southeast-2.console.aws.amazon.com/sns/v3/home?region=ap-southeast-2#/topics</p> <p>Go to Topics, select the Topic you created earlier, and click Delete</p> <p></p> <p>In the confirmation box, enter \u201cdelete me\u201d and click Delete</p> <p>Go to Subscriptions, select the Subscription you created for your email, and click Delete</p> <p></p> <p>Head to the IAM console: https://us-east-1.console.aws.amazon.com/iamv2/home?region=ap-southeast-2#/roles</p> <p>Under Roles, search for \"api-gw-sns-role\u201d</p> <p>Select the role we created earlier, and click Delete</p> <p></p> <p>Type \u201capi-gw-sns-role\u201d into the confirmation field, and click Delete</p> <p>Head to the Lambda console: https://ap-southeast-2.console.aws.amazon.com/lambda/home?region=ap-southeast-2#/functions</p> <p>Select the function we created earlier, and click Actions then Delete</p> <p></p> <p>Enter \u201cdelete\u201d in the confirmation window, and then click Delete</p> <p>Head to the Cloudwatch Logs console: https://ap-southeast-2.console.aws.amazon.com/cloudwatch/home?region=ap-southeast-2#logsV2:log-groups</p> <p>Search for the \"api-return-ip\u201d Log Group, select the log group , click Actions then Delete</p> <p></p> <p>In the confirmation popup, click Delete</p>"},{"location":"aws-cloudtrail-log-file-integrity/","title":"Aws Cloudtrail Log File Integrity","text":""},{"location":"aws-cloudtrail-log-file-integrity/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-cognito-web-identity-federation/","title":"AWS Cognito Web Identity Federation","text":""},{"location":"aws-cognito-web-identity-federation/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-control-tower/","title":"Control Tower","text":""},{"location":"aws-control-tower/#overview","title":"Overview","text":"<p>We\u2019re going to be setting up Control Tower in our AWS Organisation.</p> <p>You will be setting this up from your \u201cmanagement\u201d account. In a production environment, your management account would contain nothing but your Organization, Control Tower, Users (ideally SSO), and Billing. All resources, buckets, logs, etc should be in sub-accounts.</p> <p>For this demo you will need four to five different email accounts. If you use Gmail or Google Workspace, you can use \u201cplus addresses\u201d, which means if your email address is <code>jeffbezos@gmail.com</code>, you can use <code>jeffbezos+aws@gmail.com</code>, or <code>jeffbezos+sandbox@gmail.com</code>, or any other variation, and they will all route to your primary email. </p> <p>Alternatively you can use a service like 10 Minute Mail (https://10minutemail.com/) to get a free temporary email address. </p> <p>As you create each account, you should login to the root user for that account, reset the password, and note it down. You will need the root user password for the last step when you close the accounts.</p> <p>Another important point to note, at the end of this demo we are going to be closing AWS accounts. AWS documentation says that when an account is closed, the root user email address cannot be reused, so make sure you don\u2019t use an email address that you will want to use with AWS in the future.</p> <p>https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_close.html#:~:text=The root user's email address can't be reused if you close an account</p> <p>We will be creating this environment in the ap-southeast-2 region, so all links to the console will be there. Make sure you change region if you\u2019re deploying elsewhere.</p>"},{"location":"aws-control-tower/#instructions","title":"Instructions","text":""},{"location":"aws-control-tower/#stage-1-create-an-organization-and-sub-account","title":"Stage 1 - Create an Organization and sub-account","text":"<p>In your primary (management) account, head to the Organizations console: https://us-east-1.console.aws.amazon.com/organizations/v2/home?region=us-east-1#</p> <p>Click on Create an organization</p> <p>Now we\u2019ll add a sub-account to our organization, click on Add an AWS account</p> <p></p> <p>Leave \u201cCreate an AWS account\u201d selected.</p> <p>Enter your account name, we\u2019ll use \u201cSandbox\u201d.</p> <p>Enter your root user email address you would like to use for this new account. Note, this email must not be used with an AWS account already.</p> <p>Remember: If you use Gmail or Google Workspace, you can use \u201cplus addresses\u201d. </p> <p>Leave IAM role name as default.</p> <p>You should now have two accounts in your Organization</p> <p></p> <p>If you want to login to your new Sandbox account, you can head to the AWS Console login page and reset your password for your new root user. This is optional, the rest of these steps can be done without logging into your new account as root.</p> <p>You now have an AWS Organization with a management account and a sub-account. In the real world you might have dozens of accounts for different projects, environments (dev, testing, prod, etc), or for different teams. You can then organize these accounts into OUs (Organizational Units) based on team, project, or whatever you like.</p> <p>You can also set up SCPs (Service Control Policies) to enforce things such as mandatory backups, restricted regions, disabled services, etc. We will be doing this via Control Tower later in the guide.</p>"},{"location":"aws-control-tower/#stage-2-set-up-control-tower","title":"Stage 2 - Set up Control Tower","text":"<p>Head to the Control Tower console: https://ap-southeast-2.console.aws.amazon.com/controltower/home/landing?region=ap-southeast-2</p> <p>Click Set up landing zone</p> <p>Set your Home Region to your preferred region. This is where things like Cloudformation Stack Sets, logs, buckets, etc will be provisioned by Control Tower by default. This doesn\u2019t affect where you can provision resources. I\u2019m going to use \u201cAsia Pacific (Sydney)\u201d.</p> <p>Leave Region deny setting set to \u201cNot enabled\u201d, we don\u2019t want to restrict any regions right now.</p> <p>Click Next</p> <p>Control Tower sets up a new OU in your organization; \u201cSecurity\u201d used for log archives and security audits. You can change this OU name if you like, but we\u2019ll leave it as default.</p> <p>There is also a config box to set up a new OU called \u201cSandbox\u201d, which you would put your sandbox, dev, testing, etc accounts.</p> <p>Leave both of these as default and click Next</p> <p>On the next page, you will be prompted to create two new accounts, one for Log archiving, and one for Auditing (CloudTrail trail buckets, etc).</p> <p>You need to create these two accounts, so you will need two more email addresses. </p> <p>Note: You can use your Sandbox account you created earlier for one of these, but not both (they need to be separate accounts). You also can\u2019t use the management account for either of these. So at the bare minimum, you need one new account. AWS obviously recommends creating two new accounts and not sharing logs or audit with any resources.</p> <p>I\u2019m going to create two new accounts with plus addresses, so we will have four accounts total.</p> <p>On the next page, Control Tower will ask you if you wish to enable organization level CloudTrail, which enables CloudTrail for all of your accounts and aggregates the storage to your Audit account created on the previous page. Leave this set to Enabled.</p> <p>Leave the rest of the page as default and click Next</p> <p>Check the confirmation box, and then click Set up landing zone</p> <p></p> <p>Setting up Control Tower will take a while (in my experience, 20-30 minutes).</p>"},{"location":"aws-control-tower/#stage-3-creating-and-switching-to-an-iam-user","title":"Stage 3 - Creating and switching to an IAM user","text":"<p>For the next steps you cannot use the root account, so if you\u2019re logged in as root, we\u2019re going to create a new user in the IAM Identity Center (previously \u201cSSO\u201d), assign the necessary permissions, and add this user to our sub-account(s).</p> <p>Head to the IAM Identity Center console: https://ap-southeast-2.console.aws.amazon.com/singlesignon/home?region=ap-southeast-2#!/instances/users</p> <p>On the Users page, click on Add user</p> <p></p> <p>Set whatever you like for the Username, I\u2019ll use <code>jeffbezos</code></p> <p>Under Password, select \u201cGenerate a one-time password that you can share with this user\u201d</p> <p>Set the two Email addresses to (you guessed it) a new, unused email address.</p> <p>Fill out the First name and Last name with any values you like.</p> <p></p> <p>Click Next</p> <p>Add this user to all the groups listed. In production this should obviously be locked down to only the required groups, but for this demo, these permissions are fine.</p> <p></p> <p>Click Next</p> <p>Click Add user</p> <p>Note down the AWS access portal URL, your username, and your generated password.</p> <p>You will need this Access Portal URL multiple times throughout this guide.</p> <p></p> <p>Now we need to give your user access to your sub-account, so using Identity Center you can login to that account (which is required for the next step).</p> <p>Head to AWS Accounts, select your sub-account (not your management account), and click Assign users or groups</p> <p></p> <p>Switch to the Users tab, select your newly created user, and click Next</p> <p></p> <p>Select all Permission Sets, and click Next</p> <p></p> <p>Click Submit</p> <p>Now, head to that access portal URL, and login with your newly created account. You will be prompted to change your password.</p> <p>Once logged in, you will see a list of all your accounts that your new user has been added to. Click on your sub-account, then Management Console next to the role <code>AWSAdministratorAccess</code></p> <p></p> <p>Once you\u2019ve logged into your sub-account, head to the IAM console: https://us-east-1.console.aws.amazon.com/iamv2/home?region=ap-southeast-2#/roles</p> <p>Go to Roles and click Create Role</p> <p>Under Trusted entity type change this \u201cAWS account\u201d. Click on \u201cAnother AWS account\u201d, and paste the account ID of the management account. You can get this from the Identity Center page you were on previously.</p> <p></p> <p>Click Next</p> <p>Search for and attach the policy \u201cAdministratorAccess\u201d</p> <p></p> <p>Click Next</p> <p>Set the Role name to <code>AWSControlTowerExecution</code>. </p> <p>Click Create role</p> <p>This is the role that Control Tower will use when making changes in sub-accounts. When you create an account via Control Tower, it will automatically create this role using Account Factory, but adding an existing account requires this step to be done manually.</p> <p>So far this sub-account is in the AWS Organization, but is not managed by Control Tower. We\u2019ll do that next.</p>"},{"location":"aws-control-tower/#stage-4-enrolling-accounts-to-control-tower","title":"Stage 4 - Enrolling accounts to Control Tower","text":"<p>Log back into the management account using the Identity Center portal from earlier.</p> <p></p> <p>Head to the Control Tower console, and go to Organization: https://ap-southeast-2.console.aws.amazon.com/controltower/home/organization?region=ap-southeast-2</p> <p>You will see only your management, log, and audit accounts are enrolled in the Control Tower organization.</p> <p></p> <p>Click on your Sandbox account (or whichever account you created earlier), and click Actions \u2192 Enroll</p> <p></p> <p>Select any OU (there should only be 1), and click Enroll account</p> <p></p> <p>Click Enroll account in the confirmation box that pops up.</p> <p>The account status should now be \u201cEnrolling\u201d, this will take a few minutes.</p> <p></p> <p>Once that\u2019s done, you now have Control Tower set up, with a management account, an auditing account, a logging account, and a sub-account (where you would add your resources).</p>"},{"location":"aws-control-tower/#stage-5-enabling-a-configuration-control-for-your-organization","title":"Stage 5 - Enabling a configuration control for your organization","text":"<p>Head to the Control Tower console: https://ap-southeast-2.console.aws.amazon.com/controltower/home/controls?region=ap-southeast-2</p> <p>Click on All controls</p> <p>Click on the Find controls search bar and select \u201cGuidance\u201d</p> <p></p> <p>Then select \u201cEquals\u201d</p> <p></p> <p>Then select \u201cElective\u201d</p> <p></p> <p>Mandatory controls are already enabled, and we just want to browse what we can enable.</p> <p>There\u2019s three different types of control implementations; Config rule (used to detect), CloudFormation guard rules (used to prevent during CloudFormation creation), and Service Control Policy\u2019s (used to prevent at the API level).</p> <p>For this demo, we\u2019re going to set up an SCP, for S3, to prevent changes to the Encryption on buckets.</p> <p>Find the control called <code>[AWS-GR_AUDIT_BUCKET_ENCRYPTION_ENABLED] Disallow Changes to Encryption Configuration for Amazon S3 Buckets</code>, select it, and click Enable control</p> <p></p> <p>On the next page, select the OU you want this control to apply to. We\u2019ll apply this to the \u201cSandbox\u201d OU, as that\u2019s where our sub-account is.</p> <p></p> <p>Remember, in a production environment you might have dozens of nested OUs for different teams, departments, environments, etc, so this level of granularity can be useful.</p> <p>Click Enable control on OU</p> <p>Now let\u2019s test it out. Head back to the IAM Identity Center portal, and log into your sub-account (\u201dSandbox\u201d)</p> <p></p> <p>Head to the S3 console: https://s3.console.aws.amazon.com/s3/home?region=ap-southeast-2</p> <p>Go to Buckets and click Create bucket</p> <p></p> <p>Choose any Bucket name you like, and leave the rest of the options as default.</p> <p>Click Create bucket</p> <p>Note: You might get an error at the top of the S3 console saying you don\u2019t have permissions to <code>PutEncryption</code>, that is expected because of the control we have enabled.</p> <p>Once that\u2019s done, go into your newly created bucket, go to the Properties tab, and click Edit next to Default encryption</p> <p></p> <p>We\u2019ll try and change the encryption from the default S3 managed encryption key, to a KMS managed key.</p> <p></p> <p>Click Save changes</p> <p>And as expected, we got a permissions error.</p> <p></p> <p>This next part is optional, but we\u2019re now going to disable the S3 encryption control we enabled, and see if we can make that encryption change.</p> <p>Log back into the management account, using the Identity Center portal.</p> <p>Find the control that we enabled, it should be this one: https://ap-southeast-2.console.aws.amazon.com/controltower/home/controls/AWS-GR_AUDIT_BUCKET_ENCRYPTION_ENABLED?region=ap-southeast-2</p> <p>Under the OUs enabled tab, select the Sandbox OU that we chose earlier, and click Disable control</p> <p></p> <p>Click Disable in the pop up confirmation window.</p> <p>Now if we log back into the sub-account, go back to S3, back into the bucket, and try changing the encryption again, it should work.</p> <p></p> <p>Note: SCPs can take a couple of minutes to enable/disable, so if this doesn\u2019t work straight away, try again after a few minutes.</p>"},{"location":"aws-control-tower/#stage-6-creating-a-service-catalog-product","title":"Stage 6 - Creating a service catalog product","text":"<p>We\u2019re going to add a product to the Service Catalog in our management account. This will then be deployed in Stage 7, to a new account, using account factory. </p> <p>This is just a simple, AWS provided, Cloudformation based, product that creates an S3 bucket. In the real world you would most likely create your own CloudFormation stacks, or get products from AWS Marketplace to deploy more complex applications.</p> <p>First we need to create the required IAM role that Control Tower will use. Head to the IAM console: https://us-east-1.console.aws.amazon.com/iamv2/home?region=ap-southeast-2#/roles</p> <p>Go to Roles and click Create Role</p> <p>Under Trusted entity type change this \u201cAWS account\u201d. Leave \u201cThis account (\\&lt;account ID&gt;)\u201d selected.</p> <p>Click Next</p> <p>Search for and attach the policy \u201cAWSServiceCatalogAdminFullAccess\u201d</p> <p></p> <p>Click Next</p> <p>Set the Role name to <code>AWSControlTowerBlueprintAccess</code>. </p> <p>Click Create role</p> <p>Head to the Service Catalog console: https://ap-southeast-2.console.aws.amazon.com/servicecatalog/home?region=ap-southeast-2#home</p> <p>Go to the Getting started library</p> <p></p> <p>Search for <code>Amazon S3 Public Bucket with Read Only Access</code>, select it, and click Add to portfolio</p> <p></p> <p>On the next page, under Select portfolio, select the \u201cAWS Control Tower Account Factory Portfolio\u201d</p> <p></p> <p>Add to portfolio</p>"},{"location":"aws-control-tower/#stage-7-creating-an-account-with-account-factory","title":"Stage 7 - Creating an account with account factory","text":"<p>Head to the Control Tower console: https://ap-southeast-2.console.aws.amazon.com/controltower/home/dashboard?region=ap-southeast-2</p> <p>Go to Account factory and click Create account</p> <p></p> <p>On the next page, set the Account email to (you guessed it) a new email address. </p> <p>Set the Display name to anything you like.</p> <p>Under Access configuration, you can use the same email as the Account email above.</p> <p>Once that\u2019s filled out, expand the Account factory customization pane.</p> <p>Set the Account that contains your AWS Service Catalog products to the management account ID, and click Validate</p> <p></p> <p>It\u2019s possible to have a separate AWS account to store all of these product blueprints, but for this demo we\u2019re not going to do that, we\u2019re just going to use the management account.</p> <p>Under Select a product, choose the S3 product we created earlier.</p> <p></p> <p>Select Product version \u201cv1.0\u201d</p> <p>Under Deployment Regions select \u201cHome Region\u201d</p> <p>Click Create account</p> <p>If you go to Organization you will see your account being set up, and enrolling in your Organization and Control Tower</p> <p></p> <p>After a few minutes, this should change from \u201cEnrolling\u201d to \u201cEnrolled\u201d and move to the \u201cSandbox\u201d OU.</p> <p>Head back to the IAM Identity Center console: https://us-east-1.console.aws.amazon.com/iamv2/home?region=ap-southeast-2#/organization/accounts</p> <p>Go to the AWS Accounts page, select your newly created account, and click Assign users or groups</p> <p></p> <p>Go to the Users tab, select your IAM user, and click Next</p> <p></p> <p>Select all Permission sets, and click Next</p> <p></p> <p>Click Submit</p> <p>Now, head back to the Identity Center portal, select your new account, and login</p> <p></p> <p>Head to the CloudFormation console: https://ap-southeast-2.console.aws.amazon.com/cloudformation/home?region=ap-southeast-2#/stacks</p> <p>You will see a number of Stacks that have been set up by Control Tower, including our Service Catalog product</p> <p></p> <p>Now head to the S3 console: https://s3.console.aws.amazon.com/s3/buckets?region=ap-southeast-2</p> <p>You will see our bucket that was created from the Service Catalog product</p> <p></p> <p>Again, this is a very basic example of what is possible with Service Catalog, and usually it would be used for much more complex applications.</p>"},{"location":"aws-control-tower/#stage-8-clean-up","title":"Stage 8 - Clean up","text":"<p>This clean up is going to be a bit different, because we are going to be unmanaging (un-enrolling) accounts in Control Tower, deleting the Organization, and deleting AWS accounts. </p> <p>Obviously you should be very careful here to only delete the accounts you created for this demo.</p> <p>Log back in to your management account as the root user. We\u2019re going to be deleting our IAM users from Identity Centre, so we need to be using the root account.</p> <p>Head to the Service Catalog console: https://ap-southeast-2.console.aws.amazon.com/servicecatalog/home?region=ap-southeast-2#admin-products</p> <p>Go to the Product list page, and click on \u201cAmazon S3 Public Bucket with Read Only Access\u201d</p> <p>Under the Portfolios tab, select each portfolio, and click Disassociate product from portfolio</p> <p></p> <p>Go to the Product list page, select the \u201cAmazon S3 Public Bucket with Read Only Access\u201d product, click Actions \u2192 Remove from organization</p> <p></p> <p>Enter \u201cdelete\u201d in the confirmation box</p> <p>Head to the IAM Identity Center console: https://ap-southeast-2.console.aws.amazon.com/singlesignon/home?region=ap-southeast-2#!/instances/users</p> <p>On the Users page, select the IAM user you created earlier, and click Delete users</p> <p></p> <p>Click Delete user in the confirmation box</p> <p>Head to the Control Tower console: https://ap-southeast-2.console.aws.amazon.com/controltower/home/organization?region=ap-southeast-2</p> <p>Under Organization, select the accounts you wish to remove from Control Tower, and click Actions \u2192 Unmanage</p> <p></p> <p>Enter \u201cUNMANAGE\u201d in the confirmation box, and click Unmanage account</p> <p>Head to the Organizations console: https://us-east-1.console.aws.amazon.com/organizations/v2/home/accounts</p> <p>Select the accounts you wish to remove from the Organization, and click Actions \u2192 Remove from organization</p> <p></p> <p>Click Remove account in the confirmation box</p> <p>Once all accounts have been removed from the Organization, you can delete the Organization by going to Settings and then clicking Delete organization</p> <p></p> <p>Enter your oranization ID, and click Delete organization.</p> <p>Head to the IAM console: https://us-east-1.console.aws.amazon.com/iam/home?region=ap-southeast-2</p> <p>Go to the Roles page, search for and select <code>AWSControlTowerExecution</code> and click Delete</p> <p></p> <p>Enter the role name in the confirmation box, and click Delete</p> <p>Go to the Roles page, search for and select <code>AWSControlTowerBlueprintAccess</code> and click Delete</p> <p></p> <p>Enter the role name in the confirmation box, and click Delete</p> <p>Now, login to the root user account for each account you created in this guide. Click on the menu in the top right, then Account, then scroll down to the very bottom of the page</p> <p></p> <p>Click on Close Account</p> <p>Then in the confirmation box, click on Close Account again</p>"},{"location":"aws-dms-database-migration/","title":"AWS DMS Database Migration","text":""},{"location":"aws-dms-database-migration/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-dynamodb-lambda-trigger/","title":"AWS Dynamodb Lambda Trigger","text":""},{"location":"aws-dynamodb-lambda-trigger/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-efs/","title":"AWS EFS","text":""},{"location":"aws-efs/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-elastic-disaster-recovery/","title":"AWS Elastic Disaster Recovery","text":""},{"location":"aws-elastic-disaster-recovery/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-global-accelerator/","title":"AWS Global Accelerator","text":""},{"location":"aws-global-accelerator/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-iam-scp-permissions-boundary/","title":"AWS IAM Scp Permissions Boundary","text":""},{"location":"aws-iam-scp-permissions-boundary/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-lambda-s3-events/","title":"AWS Lambda S3 Events","text":""},{"location":"aws-lambda-s3-events/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-lambda-xray/","title":"AWS Lambda Xray","text":""},{"location":"aws-lambda-xray/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-lex-lambda-rds/","title":"AWS Lex Lambda RDS","text":""},{"location":"aws-lex-lambda-rds/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-macie/","title":"AWS Macie","text":""},{"location":"aws-macie/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-patch-manager/","title":"AWS Patch Manager","text":""},{"location":"aws-patch-manager/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-pet-rekognition-ecr/","title":"AWS Pet Rekognition ECR","text":""},{"location":"aws-pet-rekognition-ecr/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-systems-manager/","title":"AWS Systems Manager","text":""},{"location":"aws-systems-manager/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-video-on-demand/","title":"AWS Video On Demand","text":""},{"location":"aws-video-on-demand/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-vpc-flow-logs/","title":"AWS Vpc Flow Logs","text":""},{"location":"aws-vpc-flow-logs/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-waf/","title":"AWS Waf","text":""},{"location":"aws-waf/#overview","title":"Overview","text":"<p>We\u2019re going to be creating an EC2 instance running WordPress, we\u2019ll put an Application Load Balancer in front of it, and then associate an AWS WAF WebACL with that load balancer. </p> <p>We\u2019ll then explore the different rules and actions we can configure, as well as setting up WAF logging to S3 and viewing those logs.</p> <p>We will be creating this environment in the ap-southeast-4 (Melbourne) region, so all links to the console will be there. Make sure you change region if you\u2019re deploying elsewhere.</p>"},{"location":"aws-waf/#instructions","title":"Instructions","text":""},{"location":"aws-waf/#stage-1-creating-the-wordpress-instance","title":"Stage 1 - Creating the WordPress instance","text":"<p>First, we need to get the AMI of the latest version of WordPress. These AMIs are created by Bitnami and are made available for free. Head to this website to get the latest AMI for your region: https://bitnami.com/stack/wordpress/cloud/aws/amis</p> <p>Scroll down to the region you\u2019re using, and copy the AMI ID</p> <p></p> <p>Head to the EC2 console: https://ap-southeast-4.console.aws.amazon.com/ec2/home</p> <p>Go to Instances and click Launch instances</p> <p></p> <p>Set the Name to \u201cwordpress\u201d </p> <p>Under Application and OS Images (Amazon Machine Image) enter the AMI you copied earlier and press Enter</p> <p>In the window that pops up, go to the Community AMIs tab, and click Select next to the AMI result</p> <p></p> <p>Leave the Instance type as default (it should be t2.micro or t3.micro, which is free tier eligible)</p> <p>Set the Key pair (login) to \u201cProceed without a key pair (Not recommended)\u201d. We won\u2019t need to access this instance.</p> <p>Under Network settings, click Edit</p> <p></p> <p>We\u2019re going to use the default VPC, so you just need to make sure that is the VPC selected, and Auto-assign public IP is enabled</p> <p></p> <p>Under Firewall (security groups), leave \u201cCreate security group\u201d selected, and change the Security group name to \u201cwordpress-waf\u201d</p> <p>Under Security group rule 1, change the Type to HTTP</p> <p></p> <p>Leave everything else as default, and click Launch instance</p> <p>Once that\u2019s done, head back to the EC2 console: https://ap-southeast-4.console.aws.amazon.com/ec2/home</p> <p>Go to Instances, and copy the public IPv4 address of your new instance</p> <p></p> <p>Visit that IP in your browser, and you should see the default WordPress home page. You may need to wait a couple of minutes for your instance to finish booting. Also, make sure you\u2019re visiting http://, https:// won\u2019t work, and we won\u2019t be using HTTPS for this demo. <p></p>"},{"location":"aws-waf/#stage-2-creating-a-target-group","title":"Stage 2 - Creating a target group","text":"<p>Head to the EC2 console: https://ap-southeast-4.console.aws.amazon.com/ec2/home</p> <p>Go to Target Groups and click Create target group</p> <p></p> <p>Leave the Target Type as \u201cInstances\u201d</p> <p>Set the Target group name to \u201cwordpress\u201d</p> <p></p> <p>Leave everything else as default, and click Next</p> <p>Select the instance we created in the previous step, and click Include as pending below</p> <p></p> <p>Click Create target group</p>"},{"location":"aws-waf/#stage-3-creating-a-application-load-balancer","title":"Stage 3 - Creating a application load balancer","text":"<p>Head to the EC2 console: https://ap-southeast-4.console.aws.amazon.com/ec2/home</p> <p>Go to Load Balancers and click Create load balancer</p> <p></p> <p>Select \u201cApplication Load Balancer\u201d</p> <p></p> <p>Set the Load balancer name to \u201cwordpress-lb\u201d</p> <p>Select all the subnets under \u201cNetwork mapping\u201d, and make sure the VPC is set to default (<code>-</code>)</p> <p></p> <p>Under Security Groups, add the \u201cwordpress-waf\u201d security group we created in stage 1</p> <p></p> <p></p> <p>Under Listeners and routing, change the \u201cdefault action\u201d for HTTP to forward to the target group we created in the previous step</p> <p></p> <p>Click Create load balancer</p> <p>Once that\u2019s done, go back to Load Balancers and copy the DNS name for your newly created load balancer</p> <p></p> <p>Visit that URL in your browser (again, HTTP only). You should see your WordPress home page still.</p> <p>Note: ALB\u2019s can take a couple of minute to create and become ready</p> <p></p>"},{"location":"aws-waf/#stage-3-creating-an-s3-bucket-for-waf-logs","title":"Stage 3 - Creating an S3 bucket for WAF logs","text":"<p>Head to the S3 console: https://s3.console.aws.amazon.com/s3/buckets</p> <p>Go to Buckets and click Create bucket</p> <p></p> <p>S3 buckets for WAF logs must begin with <code>aws-waf-logs-</code>, so in my case I\u2019m going to use the bucket name <code>aws-waf-logs-demo-waf</code>. Remember S3 bucket names are unique so this may be taken, just pick another bucket name that begins with <code>aws-waf-logs-</code></p> <p>Make sure the region is set to the same region your EC2 instance was created in.</p> <p>Leave everything else as is, and click Create bucket</p>"},{"location":"aws-waf/#stage-4-setting-up-the-waf","title":"Stage 4 - Setting up the WAF","text":"<p>Head to the WAF console: https://us-east-1.console.aws.amazon.com/wafv2/homev2</p> <p>In each step in this stage, make sure your region remains set to the region your instance is deployed in. For me that is \u201cAsia Pacific (Melbourne)\u201d</p> <p></p> <p>First, we\u2019re going to create an IP Set, which is basically just a list of IP addresses grouped together, these are commonly used for allow-listed IPs (e.g. your home or office IPs, your developers, etc). IP Sets can contain up to 10,000 CIDR ranges, which makes allowing or blocking large numbers of networks very easy.</p> <p>In a new tab, open the following URL: https://checkip.amazonaws.com/</p> <p></p> <p>This will give you your IP address as seen by Amazon. This is useful if your work PC has a split-horizon VPN for example, where only AWS traffic is routed over the VPN.</p> <p>Copy this IP address down for the next step.</p> <p>Go to IP sets and click Create IP Set</p> <p></p> <p>Set the IP Set name to \u201chome-ip\u201d</p> <p>In the IP addresses box, add the IP you copied earlier followed by <code>/32</code>, so in my case <code>34.129.222.183/32</code></p> <p></p> <p>Click Create IP set</p> <p>Go to Regex pattern sets and click Create regex pattern set</p> <p></p> <p>Set the Regex pattern set name to \u201cno-wp-files\u201d</p> <p>In the Regular expressions box, enter:</p> <pre><code>(wp\\-login\\.php)$\n(.*wp\\-config.*)\n(xmlrpc\\.php)\n</code></pre> <p></p> <p>Click Create regex pattern set</p> <p>Go to Web ACLs and click Create web ACL</p> <p></p> <p>Set the Name to \u201cwordpress-acl\u201d</p> <p>Under Resource type select \u201cRegional resources\u201d</p> <p>Under Associated AWS resources click Add AWS resources</p> <p></p> <p>Select \u201cApplication Load Balancer\u201d (different regions will have different options), and select the ALB we created earlier</p> <p></p> <p>Click Add</p> <p>Click Next</p> <p>Under Rules, click Add rules and then Add managed rule groups</p> <p></p> <p>Expand AWS managed rule groups and you will see a list of WAF rule groups that are supplied and maintained by AWS.</p> <p>Under Free rule groups select the following</p> <p></p> <p>Each rule group has a description showing what kind of attacks the rule group helps protect against. </p> <p>Considering we\u2019re running a WordPress application, we definitely want the WordPress rules, and WordPress is built using PHP, so we want the PHP application rules as well. Our database for WordPress is MySQL, so the SQL database rule set will help protect against SQL Injection attacks (among other things)</p> <p>The \u201cCore rule set\u201d contains the most rules, and protects against common attack methods provided by the open source OWASP organisation (https://owasp.org/)</p> <p>Note each rule group has a \u201ccapacity\u201d value. Each Web ACL has a maximum capacity of 1500 \u201cWebACL Capacity Units\u201d (WCUs), and each rule we add uses up some of those units. \u201cCore rule set\u201d contains the most rules, so therefore is the most expensive at 700 WCUs. This limit is in place to prevent traffic inspection from taking too long (among presumably other reasons on AWS\u2019 side). </p> <p>You\u2019ll also see some other 3rd party rule groups provided by external security organisations. These are usually paid subscriptions that you subscribe to via the AWS Marketplace. We won\u2019t be using these in this demo.</p> <p>Click Add rules</p> <p></p> <p>On the next page, you can see it is telling us we\u2019ve used 1100/1500 WCUs. We\u2019re going to add more rules later, so this is fine.</p> <p>Under Default web ACL action for requests that don't match any rules leave this set to \u201cAllow\u201d. In our case, we only want traffic that matches one of our rules to be blocked, but if this was reversed and we had an application we only wanted to be accessed by specific IP addresses for example, we would change this to \u201cBlock\u201d.</p> <p>Click Next</p> <p>On the next page, we can change the priority of our rules. Similar to Network ACLs, rules within a Web ACL are executed in order of top to bottom. For now, we\u2019ll leave these as is.</p> <p></p> <p>Click Next</p> <p>On the next page, make sure Request sampling options is enabled, and click Next</p> <p></p> <p>On the final page, click Create web ACL</p> <p>On the next page, click on your newly created Web ACL</p> <p></p> <p>Go to the Logging and metrics tab and click Enable under Logging</p> <p></p> <p>Change the Logging destination to \u201cS3 bucket\u201d, and select your S3 bucket from the dropdown. There should only be one because the dropdown will only show buckets beginning with <code>aws-waf-logs-</code></p> <p></p> <p>In a production environment, you might want to redact certain fields such as an authorisation header, a query string, the HTTP method, etc. But for this demo we will leave these unselected.</p> <p>Click Save</p>"},{"location":"aws-waf/#stage-5-testing-our-waf","title":"Stage 5 - Testing our WAF","text":"<p>Open your ALB URL in a new tab (this will be something like http://wordpress-lb-1275728828.ap-southeast-4.elb.amazonaws.com/ that we visited earlier).</p> <p>Your WordPress homepage should still be viewable. You can visit the sample blog entry (<code>/sample-page/</code>), and the admin login (<code>/wp-admin</code>)</p> <p>Let\u2019s try and use a very basic SQL Injection attack on our website and see if the WAF stops us, go to the page: <code>/wp-login.php?user=1+ORDER+BY+10</code></p> <p>You should see a \u201c403 Forbidden\u201d message</p> <p></p> <p>This means our WAF rules are working as expected.</p> <p>If you head back to the WAF console: https://us-east-1.console.aws.amazon.com/wafv2/homev2/</p> <p>Go to Web ACLs and into the <code>wordpress-acl</code> we created (if your ACL doesn\u2019t show up, make sure the region is set correctly)</p> <p></p> <p>Then under Sampled requests, if you search for your IP address from earlier you should see a few of the requests you made, and their Action (allow or block) and the rule that caused any block actions</p> <p></p> <p>Sampled requests can take a few minutes to appear, and not all requests will appear (only certain requests are sampled and displayed).</p> <p>You will possibly also see various other requests from random other IP addresses that scan the internet, usually looking for vulnerable websites.</p> <p></p>"},{"location":"aws-waf/#stage-6-adding-custom-rules","title":"Stage 6 - Adding custom rules","text":"<p>We don\u2019t want our WordPress login page or XML-RPC page being accessed by anyone, so we\u2019re going to use our regex pattern we created earlier to block any requests to <code>/wp-login.php</code> or <code>/xmlrpc.php</code></p> <p>Head to the WAF console: https://us-east-1.console.aws.amazon.com/wafv2/homev2</p> <p>Click on your WordPress Web ACL</p> <p></p> <p>Go to the Rules tab then click Add rules then Add my own rules and rule groups</p> <p></p> <p>Leave Rule Type as \u201cRule builder\u201d</p> <p>Set the Rule Name to <code>no-wp-files</code> (you can call this anything you like, but for the demo we\u2019ll use this)</p> <p>Leave the Type as \u201cRegular rule\u201d. We could set this to a \u201cRate-based rule\u201d to prevent bots from trying hundreds or thousands of different passwords on our <code>wp-login.php</code> page, but we won\u2019t for this demo.</p> <p>Set Inspect to \u201cURI Path\u201d</p> <p></p> <p>Set Match Type to \u201cMatches pattern from regex pattern set\u201d</p> <p></p> <p>Set Regex pattern set to the pattern set we created earlier (it should be the only one there)</p> <p></p> <p>Set Text transformation to \u201cLowercase\u201d</p> <p></p> <p>What this does is changes everything in the \u201cInspect\u201d field (in our case, the URI Path) to lowercase. This is useful if you wanted to look for <code>/wp-login.php</code>, <code>/WP-LOGIN.PHP</code>, or even <code>/Wp-LoGiN.PhP</code>, WAF would see all of these as <code>/wp-login.php</code>. There are (as you can see) multiple other transformations such as compressing white space, decoding URL characters, Base64 decoding strings, etc, and you can have multiple of these.</p> <p>Under Action, select \u201cBlock\u201d</p> <p>Click Add rule</p> <p>On the next page, we need to move our new rule to the top of the priority list. Remember rules are executed in order of top to bottom. </p> <p>Select your new rule, and keep clicking Move up until it\u2019s at the top of the list.</p> <p></p> <p>Now if we head back to our website, and try accessing the <code>/wp-login.php</code> page, or <code>/wp-admin</code>, we will get a 403 Forbidden message</p> <p></p> <p>We can also try <code>/xmlrpc.php</code>, which will give us a 403 Forbidden as well.</p> <p>And if we head back to our WAF console, and go to the Overview tab, after a few minutes you should see your IP address, the URI\u2019s you tried to access, and the action \u201cBlock\u201d, note the Metric name also shows the name of the rule that was used.</p> <p></p> <p>Now, because we trust our home IP, we\u2019re going to allow everything from that IP address. </p> <p>Go to the Rules tab and click Add rules then Add my own rules and rule groups</p> <p></p> <p>Set the Rule type to \u201cIP Set\u201d</p> <p>Set the Rule name to \u201callow-home\u201d</p> <p>Change the IP set to the \u201chome-ip\u201d IP Set we created earlier</p> <p>Leave IP address to use as the originating address as \u201cSource IP address\u201d</p> <p>Change Action to \u201cAllow\u201d</p> <p></p> <p>Click Add rule</p> <p>On the next page, move this newly created rule to the top of the list</p> <p></p> <p>Click Save</p> <p>Now if we visit the <code>/wp-login.php</code> page again, you should be able to see it.</p> <p>Note: If you can\u2019t, double check your IP address hasn\u2019t changed since earlier (https://checkip.amazonaws.com/) and if it has, go back to Stage 4 and create a new IP Set (or add your IP to the <code>home-ip</code> IP Set)</p> <p></p> <p>Now as a final rule, we only want users in our country to be able to visit our website, so we\u2019re going to geo block all other countries.</p> <p>Note: Geo blocking can be inaccurate and it\u2019s definitely not uncommon for an IP to be configured as being in the wrong country. If an ISP purchases a block of IP addresses from an RIR (regional Internet registry) or from another ISP, it will usually take time for all of the many Geo IP databases to be updated, and then requires AWS to start using this updated database in AWS WAF, so keep this in mind if geo blocking doesn\u2019t work for you.</p> <p>We need to do this in two stages, we need a rule that allows our country (in my case, Australia), which will go at the bottom of the rule priority list (because we still want visitors from Australia to be checked by all of our other rules like SQL Injection, not accessing <code>wp-login.php</code>, etc). Then we need another rule that blocks all countries that aren\u2019t Australia.</p> <p>Obviously you can change this to be whatever country / countries you like.</p> <p>Go to the Rules tab and click Add rules then Add my own rules and rule groups</p> <p></p> <p>Set the rule Name to \u201callow-au\u201d (or allow-) <p>Change the Inspect option to \u201cOriginates from a country in\u201d</p> <p>Change Country codes to  (Australia in my case) <p>Change the Action to Allow</p> <p></p> <p>Click Add rule</p> <p>On the next page, leave this rule last and click Save</p> <p>Go to the Rules tab and click Add rules then Add my own rules and rule groups</p> <p></p> <p>Set the rule Name to \u201cblock-earth\u201d</p> <p>Change If a request to \u201cdoesn\u2019t match the statement (NOT)\u201d</p> <p>Change the Inspect option to \u201cOriginates from a country in\u201d</p> <p>Change Country codes to  (Australia in my case) <p>Change the Action to Block</p> <p></p> <p>Click Add rule</p> <p>On the next page, move this rule up to the second position. We still want our home IP to have access at all times. This is useful if you\u2019re allow listing your companies IP address(es) for example, you don\u2019t want your staff to lose access if there was a geo blocking error.</p> <p></p> <p>Click Save</p> <p>Now we should be able visit our WordPress website still</p> <p></p> <p>But let\u2019s test it from another country. To do this, we will use a website speed test service called Pingdom: https://tools.pingdom.com/</p> <p>Enter your ALB URL, and change Test from to a country that isn\u2019t your country (e.g. I\u2019m in Australia, so I will choose to test from Germany)</p> <p></p> <p>Click Start test</p> <p>After a few seconds the test will complete. Scroll down to Response codes and you should only see <code>403</code></p> <p></p> <p>So the website loaded quickly, we got an \u201cA\u201d in Performance, but that is because AWS WAF returned essentially an empty page with the words \u201c403 Forbidden\u201d.</p> <p>If Pingdom offers tests from your country, try running the test again from there, in my case, Australia.</p> <p></p> <p>You can see there were no <code>403</code> responses, only 200\u2019s (and 302, which is fine).</p>"},{"location":"aws-waf/#stage-7-viewing-our-waf-logs","title":"Stage 7 - Viewing our WAF Logs","text":"<p>Head to the S3 console: https://s3.console.aws.amazon.com/s3/buckets</p> <p>Go to Buckets and click on your AWS Waf Logs bucket</p> <p></p> <p>The directory structure will be:</p> <p><code>/AWSLogs/[ACCOUNT ID]/WAFLogs/[REGION]/[WEB ACL]/[YEAR]/[MONTH]/[DAY]/[HOUR]/[5 MINUTE BLOCK]/[LOGS]</code></p> <p>Try to go into the directory that was created around the time you were going through the previous steps (using Pingdom), select one of (or the only) log file, and click Download</p> <p></p> <p>This will download a log file that is gzipped. MacOS and Linux systems can extract these natively, however Windows PCs will need a program such as 7-Zip (download here).</p> <p>On MacOS you can simply double click the <code>.gz</code> file, on Windows you will need to right click, go to <code>7-Zip</code> then <code>Extract here</code></p> <p></p> <p>This will give you a <code>.log</code> text file. Open it up in whichever text editor you\u2019re comfortable with, I\u2019ll use VSCode.</p> <p>Each log line contains information about that specific request. It can be a bit difficult to read, but you can use a website like https://jsonformatter.org/json-pretty-print to make the JSON log line more readable (note, paste one line at a time on this website). Otherwise, if you have a plugin for VSCode (or your text editor) that can \u201cpretty print\u201d JSON, you can use that.</p> <p>Here\u2019s an example of one of my log entries, the <code>action</code> is \u201cALLOW\u201d, and the <code>terminatingRuleId</code> is \u201callow-home\u201d. This is my home IP address that is allowed as the first rule in the Web ACL.</p> <pre><code>{\n    \"timestamp\": 1680696799239,\n    \"formatVersion\": 1,\n    \"webaclId\": \"arn:aws:wafv2:ap-southeast-4:123456789012:regional/webacl/wordpress-acl/1be732ad-bf31-44d9-8745-4da02eeaac18\",\n    \"terminatingRuleId\": \"allow-home\",\n    \"terminatingRuleType\": \"REGULAR\",\n    \"action\": \"ALLOW\",\n    \"terminatingRuleMatchDetails\": [],\n    \"httpSourceName\": \"ALB\",\n    \"httpSourceId\": \"123456789012-app/wordpress-lb/19541f9649c59058\",\n    \"ruleGroupList\": [],\n    \"rateBasedRuleList\": [],\n    \"nonTerminatingMatchingRules\": [],\n    \"requestHeadersInserted\": null,\n    \"responseCodeSent\": null,\n    \"httpRequest\": {\n        \"clientIp\": \"34.129.222.183\",\n        \"country\": \"AU\",\n        \"headers\": [\n            {\n                \"name\": \"Host\",\n                \"value\": \"wordpress-lb-1275728828.ap-southeast-4.elb.amazonaws.com\"\n            },\n            {\n                \"name\": \"Connection\",\n                \"value\": \"keep-alive\"\n            },\n            {\n                \"name\": \"Cache-Control\",\n                \"value\": \"max-age=0\"\n            },\n            {\n                \"name\": \"Upgrade-Insecure-Requests\",\n                \"value\": \"1\"\n            },\n            {\n                \"name\": \"User-Agent\",\n                \"value\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36\"\n            },\n            {\n                \"name\": \"Accept\",\n                \"value\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\"\n            },\n            {\n                \"name\": \"Accept-Encoding\",\n                \"value\": \"gzip, deflate\"\n            },\n            {\n                \"name\": \"Accept-Language\",\n                \"value\": \"en-AU,en-GB;q=0.9,en-US;q=0.8,en;q=0.7\"\n            },\n            {\n                \"name\": \"Cookie\",\n                \"value\": \"wordpress_test_cookie=WP%20Cookie%20check\"\n            }\n        ],\n        \"uri\": \"/wp-login.php\",\n        \"args\": \"\",\n        \"httpVersion\": \"HTTP/1.1\",\n        \"httpMethod\": \"GET\",\n        \"requestId\": \"1-642d65df-2bcb4716046498c504b1432a\"\n    }\n}\n</code></pre> <p>Below that, we can see the Pingdom request from German that was blocked by our \u201cblock-earth\u201d rule, note the country code under \u201chttpRequest\u201d is \u201cDE\u201d (Germany)</p> <pre><code>{\n    \"timestamp\": 1680696828189,\n    \"formatVersion\": 1,\n    \"webaclId\": \"arn:aws:wafv2:ap-southeast-4:123456789012:regional/webacl/wordpress-acl/1be732ad-bf31-44d9-8745-4da02eeaac18\",\n    \"terminatingRuleId\": \"block-earth\",\n    \"terminatingRuleType\": \"REGULAR\",\n    \"action\": \"BLOCK\",\n    \"terminatingRuleMatchDetails\": [],\n    \"httpSourceName\": \"ALB\",\n    \"httpSourceId\": \"123456789012-app/wordpress-lb/19541f9649c59058\",\n    \"ruleGroupList\": [],\n    \"rateBasedRuleList\": [],\n    \"nonTerminatingMatchingRules\": [],\n    \"requestHeadersInserted\": null,\n    \"responseCodeSent\": null,\n    \"httpRequest\": {\n        \"clientIp\": \"35.156.182.26\",\n        \"country\": \"DE\",\n        \"headers\": [\n            {\n                \"name\": \"Host\",\n                \"value\": \"wordpress-lb-1275728828.ap-southeast-4.elb.amazonaws.com\"\n            },\n            {\n                \"name\": \"Connection\",\n                \"value\": \"keep-alive\"\n            },\n            {\n                \"name\": \"Pragma\",\n                \"value\": \"no-cache\"\n            },\n            {\n                \"name\": \"Cache-Control\",\n                \"value\": \"no-cache\"\n            },\n            {\n                \"name\": \"User-Agent\",\n                \"value\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/61.0.3163.100 Chrome/61.0.3163.100 Safari/537.36 PingdomPageSpeed/1.0 (pingbot/2.0; +http://www.pingdom.com/)\"\n            },\n            {\n                \"name\": \"Accept\",\n                \"value\": \"image/webp,image/apng,image/*,*/*;q=0.8\"\n            },\n            {\n                \"name\": \"Referer\",\n                \"value\": \"http://wordpress-lb-1275728828.ap-southeast-4.elb.amazonaws.com/\"\n            },\n            {\n                \"name\": \"Accept-Encoding\",\n                \"value\": \"gzip, deflate\"\n            },\n            {\n                \"name\": \"Accept-Language\",\n                \"value\": \"en-US,en;q=0.8\"\n            }\n        ],\n        \"uri\": \"/favicon.ico\",\n        \"args\": \"\",\n        \"httpVersion\": \"HTTP/1.1\",\n        \"httpMethod\": \"GET\",\n        \"requestId\": \"1-642d65fc-1fbafd367ee9c2b671146b3f\"\n    },\n    \"labels\": [\n        {\n            \"name\": \"awswaf:clientip:geo:country:DE\"\n        },\n        {\n            \"name\": \"awswaf:clientip:geo:region:DE-HE\"\n        }\n    ]\n}\n</code></pre> <p>You can also see all of our request headers are there, as well as the URI, query strings, etc. Remember these can be redacted from logs files if needed.</p>"},{"location":"aws-waf/#stage-8-optional-custom-response-bodies","title":"Stage 8 - Optional: Custom response bodies","text":"<p>This can be useful if you would prefer a branded or nicer looking response from AWS WAF, especially if a request is being blocked (403).</p> <p>Head to the WAF console: https://us-east-1.console.aws.amazon.com/wafv2/homev2</p> <p>Click on your WordPress Web ACL</p> <p></p> <p>Go to Custom response bodies and click Create custom response body&lt;/kbd <p></p> <p>Set the Response body object name to \u201cfriendly-block\u201d</p> <p>Set the Content type to HTML</p> <p>Set the Response body to:</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;title&gt;Forbidden&lt;/title&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;div style=\"text-align:center;\"&gt;\n      &lt;h1 style=\"color: pink;\"&gt;Forbidden&lt;/h1&gt;\n      &lt;img src=\"https://img.freepik.com/free-photo/red-white-cat-i-white-studio_155003-13189.jpg\" alt=\"A red and white cat\"&gt;\n    &lt;/div&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>Note: This image may move or be deleted, feel free to replace the image file in <code>&lt;img src=</code> with another image of your choosing.</p> <p>Click Save</p> <p>Go to Rules, select the <code>allow-home</code> rule, and click Edit</p> <p></p> <p>For demo purposes, we\u2019re going to change this rule to block, so that we can see our custom response.</p> <p>Change the Action to \u201cblock\u201d, and then expand Custom response</p> <p>Select Enable</p> <p>Set Response code to 403</p> <p>Set Choose how you would like to specify the response body to \u201cfriendly-block\u201d</p> <p></p> <p>Click Save rule</p> <p>Now if we refresh our WordPress website, we\u2019ll see a much nicer forbidden message</p> <p></p> <p>Obviously in a production environment this would likely be company branded, and you would apply it to the other rules that you expect to block users. But for this demo, it was much easier to apply it to our home IP rule just so we could see it.</p>"},{"location":"aws-waf/#stage-9-clean-up","title":"Stage 9 - Clean up","text":"<p>Head to the WAF console: https://us-east-1.console.aws.amazon.com/wafv2/homev2</p> <p>Click on your WordPress Web ACL</p> <p></p> <p>Go to the Associated AWS resources tab, select your <code>wordpress-lb</code> ALB, and click Disassociate</p> <p></p> <p>Type \u201cremove\u201d in the confirmation box and click Disassociate</p> <p>Go back to Web ACLs, select your Web ACL and click Delete</p> <p></p> <p>Enter \u201cdelete\u201d in the confirmation box and click Delete</p> <p>Go to IP Sets, select your <code>home-ip</code> IP Set and click Delete</p> <p></p> <p>Enter \u201cdelete\u201d in the confirmation box and click Delete</p> <p>Go to Regex pattern sets, select your <code>no-wp-files</code> pattern set, and click Delete</p> <p></p> <p>Enter \u201cdelete\u201d in the confirmation box and click Delete</p> <p>Head to the S3 console: https://s3.console.aws.amazon.com/s3/buckets</p> <p>Go to Buckets and select your <code>aws-waf-logs-</code> bucket, and click Empty</p> <p></p> <p>In the confirmation window, enter \u201cpermanently delete\u201d and click Empty</p> <p>Go to Buckets and select your <code>aws-waf-logs-</code> bucket, and click Delete</p> <p></p> <p>In the confirmation window, enter your bucket name and click Delete</p> <p>Head to the EC2 console: https://ap-southeast-4.console.aws.amazon.com/ec2</p> <p>Go to Load Balancers, select your <code>wordpress-lb</code> ALB, click Actions then Delete load balancer</p> <p></p> <p>Enter \u201cconfirm\u201d in the confirmation box, and click Delete</p> <p>Go to Target Groups, select your <code>wordpress</code> target group, click Actions then Delete</p> <p></p> <p>Click Yes, delete in the confirmation box.</p> <p>Go to Instances, select your <code>wordpress</code> instance, click Instance state then Terminate instance</p> <p>Be careful and make sure you\u2019re deleting the instance we created at the beginning of the demo.</p> <p></p> <p>Click Terminate in the confirmation box.</p> <p>Go to Security Groups, select the <code>wordpress-waf</code> security group, click Actions then Delete security groups</p> <p></p> <p>Click Delete in the confirmation box</p> <p>Note: If you get an error similar to the following, wait a few minutes for your ALB and EC2 instance to finish terminating, then try again</p> <p></p>"},{"location":"dockerbeginning/","title":"Flask Calculator Web Application","text":"<p>Introduction</p> <p>This documentation provides an overview of the Flask calculator web application and guides you on how to set up and run the project. This simple web application allows users to perform basic arithmetic calculations such as addition, subtraction, multiplication, and division.</p> <p>Tools Requirements</p> <p>Python | installation guide Click Here </p> <p>Docker | installation guide Click Here</p>"},{"location":"dockerbeginning/#getting-started","title":"Getting Started","text":"<p>Open terminal and run this command to clone the repository of Flask Calculator Web Application.</p> <p>Command:</p> <pre><code>git clone https://github.com/cloudspaceacademy/Flask-Calculator-app.git\n</code></pre> <p>Now navigate to the directory Flask-Calculator-app. You will see a cloudspace directory now open that directory using the terminal. </p> <p>You can simply use this command:</p> <p><code>cd Flask-Calculator-app &amp;&amp; cd cloudspace</code></p> <p>Now run  <code>ls</code>  command and Now what are the scenarios there you will see app.py,Dockerfile,requirements.txt file and a directory named templates.</p> <p>Let's elaborate why we use that Dockerfile for this application and how we write that Dockerfile.</p>"},{"location":"dockerbeginning/#why-do-we-use-docker","title":"Why do we use docker?","text":"<p>Docker provides a way to package, distribute, and run applications consistently across various environments. It simplifies the process of creating, deploying, and maintaining applications while improving portability, scalability, and resource efficiency. The Dockerfile is at the heart of this process, defining how the container image is built and what software is included, making it a valuable tool for modern software development and deployment.</p> <p>Now let's elaborate how we write that Dockerfile to package our application.</p>"},{"location":"dockerbeginning/#dockerfile","title":"Dockerfile","text":"<pre><code>FROM python:3.9-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\n\nRUN pip install -r requirements.txt\n\nCOPY . .\n\nEXPOSE 5000\n\nCMD [\"python\", \"app.py\"] \n</code></pre> <p>Now just break down all the code and elaborate why they use for </p> <p>FROM python:3.9-slim</p> <p>This line specifies the base image for our Docker container. In this case, it's using the official Python 3.9 slim image. The \"slim\" variant provides a smaller image size by excluding some non-essential components, making it suitable for many Python applications.</p> <p>WORKDIR /app</p> <p>This line sets the working directory inside the container to /app. This is where the application code and files will be placed, and it's the directory where commands will be executed by default. It's a good practice to organize your application within a dedicated directory.</p> <p>COPY requirements.txt .</p> <p>This line copies the requirements.txt file from the host system into the container. The . at the end specifies the destination path, which is the current directory within the container (i.e., /app).</p> <p>RUN pip install -r requirements.txt</p> <p>This line uses the RUN instruction to execute a command within the container. In this case, it runs pip install -r requirements.txt to install all the Python packages listed in the requirements.txt file. This step ensures that the necessary dependencies are installed in the container.</p> <p>COPY . .</p> <p>This line copies all the files and directories from the current directory (the directory where the Dockerfile is located) on our host system to the /app directory in the container. This includes our application code (app.py) and any other files or directories in the project folder.</p> <p>EXPOSE 5000</p> <p>This line informs Docker that the container will listen on port 5000. While this line is not necessary for port mapping, it serves as a form of documentation to indicate which ports the container expects to use. It's good practice for clarity.</p> <p>CMD [\"python\", \"app.py\"]</p> <p>This line defines the default command to execute when the container is started. It runs the Flask application by executing python app.py. The Flask development server will start, and our application will be served on port 5000 as specified earlier.</p> <p>Now lets learn how to run the application by using just two docker commands from your local machine. </p>"},{"location":"dockerbeginning/#running-the-application-with-docker","title":"Running the Application with Docker","text":""},{"location":"dockerbeginning/#building-a-docker-image","title":"Building a Docker Image","text":"<p>To build a Docker image, you use the docker build command. </p> <p><code>docker build -t flask-calculator .</code></p> <p>docker build -t flask-calculator .  Run this command where the Dockerfile is located. Our Dockerfile is located in the root directory cloudspace so that we use ( . )  </p> <p>Now break down the code </p> <p>docker build: This is the Docker command for building an image.</p> <p>-t flask-calculator: This part specifies the image name and tag. In this example, the image is named \"flask-calculator,\" and it has no specific tag. The tag is used to version your images; if you don't specify one, it defaults to \"latest.\"</p> <p>.: The dot (.) at the end of the command tells Docker to use the current directory as the build context. It means that Docker will look in the current directory for a Dockerfile to use as the basis for building the image.</p>"},{"location":"dockerbeginning/#docker-image-used-for","title":"Docker image used for :","text":"<p>A Docker image is a lightweight, standalone, executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, system libraries, and settings. Images are used as the basis for creating Docker containers.</p>"},{"location":"dockerbeginning/#running-a-docker-container","title":"Running a Docker Container","text":"<p>Once we have built our Docker image, we can create and run containers from it using the docker run command. </p> <p><code>docker run -p 5000:80 -d flask-calculator</code></p> <p>Now break down the code </p> <p>docker run: This is the Docker command for creating and running a container.</p> <p>-p 5000:80: This part maps port 5000 from our host machine to port 80 inside the container. It allows us to access our Flask application running in the container on port 5000.</p> <p>-d: this defines the container will run in detached mode, which means it runs in the background.</p> <p>flask-calculator: This is the name of the Docker image you want to use to create the container. The image name corresponds to the one you specified when building the image.</p>"},{"location":"dockerbeginning/#docker-container-used-for","title":"Docker Container used for :","text":"<p>A Docker container is a runnable instance of a Docker image. It is an isolated environment where our application runs. Containers share the same OS kernel as the host system but are isolated from each other. They are ephemeral and can be started, stopped, and removed as needed.</p>"},{"location":"dockerbeginning/#access-the-application","title":"Access the application","text":"<p>Now you will see the app is running from your local machine on port 5000. Open your web browser and go to localhost:5000 to access the Flask calculator web application within the Docker container.</p>"},{"location":"dockercomposebeginning/","title":"Flask Calculator Web Application","text":"<p>Introduction</p> <p>This documentation provides an overview of the Flask calculator web application and guides you on how to set up and run the project. The Flask calculator is a straightforward web application designed for basic arithmetic calculations, including addition, subtraction, multiplication, and division.</p> <p>Tools Requirements</p> <p>Python | installation guide Click Here </p> <p>Docker | installation guide Click Here</p>"},{"location":"dockercomposebeginning/#getting-started","title":"Getting Started","text":"<p>Open your terminal and clone the Flask Calculator Web Application repository using the following command:</p> <pre><code>git clone https://github.com/cloudspaceacademy/Flask-Calculator-app.git\n</code></pre> <p>Navigate to the \"Flask-Calculator-app\" directory By running this command :</p> <p><code>cd Flask-Calculator-app</code></p> <p>Inside the \"Flask-Calculator-app\" directory, you will find a \"cloudspace\" directory. Enter this directory using the following command:</p> <p><code>cd cloudspace</code></p> <p>In Previous session \"Docker Beginner\" you have learned about how to run this \"Flask Calculator Web Application\" using Dockerfile.Now we are gonna learn how to run this application using docker-compose.yml file.</p> <p>In Previous session \"Docker Beginner\" you have learned about Dockerfile.How To write a Dockerfile,why its used for,how to run the application using Dockerfile etc.Now let's jump up to docker-compose.yml file.</p>"},{"location":"dockercomposebeginning/#docker-composeyml-file-used-for","title":"docker-compose.yml file used for","text":"<p>A docker-compose.yml file is used to define a multi-container Docker application. It allows you to define and configure multiple services, networks, and volumes in a single file. Docker Compose simplifies the process of orchestrating complex applications by providing a way to manage multiple containers as a single application stack.</p> <p>Simply Dockerfile is used for single service or to run the application using one container and docker-compose.yml file is used for multiple services or to packaging and run the application using multi containers.</p>"},{"location":"dockercomposebeginning/#docker-composeyml","title":"docker-compose.yml","text":"<pre><code>version: '3.8'\n\nservices:\n  flask-calculator:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - \"8080:80\"\n\n</code></pre> <p>Let's Break down the code </p> <p>version: '3.8':Specifies the version of the Docker Compose file syntax. In this case, it's using version 3.8.</p> <p>services:Defines the services that make up the Docker application.</p> <p>flask-calculator:The name of the service. In this case, it's named flask-calculator.</p> <p>build:Specifies how to build the Docker image for the service.</p> <p>context: . :Specifies the build context, which is the path to the directory containing the Dockerfile and any files needed for the build. In this case, it's set to the current directory (.)</p> <p>dockerfile: Dockerfile:Specifies the name of the Dockerfile to use for building the image. In this case, it's named Dockerfile.</p> <p>ports:Specifies the ports to expose from the container. - \"8080:80\":</p> <p>Maps port 80 from the container to port 8080 on the host machine. The format is host_port:container_port.</p>"},{"location":"dockercomposebeginning/#running-the-application-with-docker-compose","title":"Running the Application with Docker Compose","text":"<p>To run the application run this command where the docker-compose.yml file is located.</p> <p>Command :</p> <p><code>docker-compose up -d</code> </p> <p>Break down Codes:</p> <p>docker-compose: The Docker Compose command-line tool.</p> <p>up: This command is used to create and start containers based on the configurations specified in the docker-compose.yml file.</p> <p>-d: Stands for \"detached mode.\" When you run Docker containers in detached mode, the containers run in the background, and you get your terminal prompt back. This allows you to continue using the same terminal session for other tasks.</p> <p>Now Run <code>docker ps</code> command to see the running container id that you have just created.</p>"},{"location":"dockercomposebeginning/#access-the-application","title":"Access the application","text":"<p>Now you will see the app is running from your local machine on port 8080. Open your web browser and go to localhost:8080 to access the Flask calculator web application within the Docker container.</p>"}]}