{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Cloud Space","text":"<p>Website : www.cloudspaceacademy.com   info@cloudspaceacademy.com   Phone: +1-855-200-7653   WhatsApp: +1-571-454-4691  Facebook </p> <p>At CloudSpace, our students' success is our top priority. We understand it can be difficult to study alone once the BootCamp is over. CloudSpace is committed to providing you with the necessary support until you land that dream job. The FREE follow-up program is a dedicated place to keep learning with an industry leader DevOps instructor until you land a job. All this free of charge as long as you have completed the AWS Cloud &amp; DevOps BootCamp with us. So after completing the 6 months AWS &amp; DevOps Bootcamp, you can attend the Free Follow-up Program free of charge on a weekly basis to keep working on interview preparation and DevOps projects with an experienced instructor. So you will be allowed to attend this post-bootcamp until you land a job.</p> <p>Follow-Up Program Schedule: Thursdays (8PM-9PM) Interview Prep - Sundays (1PM-3PM) Projects Hands On</p>"},{"location":"AWS%20E-Commerce%20Application/","title":"AWS E-Commerce Application DevOps","text":"<p>Description</p> <p>Develop and deploy a full-fledged e-commerce application in an AWS environment. This includes a database, S3 bucket for storage, ECS for deployment, and infrastructure as code with CloudFormation.</p> <p>Tools</p> <p>\u25cf Infrastructure as Code: AWS CloudFormation to define the infrastructure.</p> <p>\u25cf Database: AWS RDS or DynamoDB for database management.</p> <p>\u25cf Storage: AWS S3 for file storage.</p> <p>\u25cf Deployment: AWS ECS for container-based deployment.</p> <p>\u25cf CI/CD: Automate deployment using GitHub Actions.</p>"},{"location":"AWS%20Photo%20Gallery%20Application/","title":"AWS Photo Gallery Application DevOps","text":"<p>Description</p> <p>This application is a photo gallery platform similar to Pexels . Users can freely upload and download images. It leverages AWS services for scalability and reliability.</p> <p>Tools</p> <p>\u25cf Version Control: AWS CodeCommit for code storage.</p> <p>\u25cf CI/CD: AWS CodePipeline for automating deployment.</p> <p>\u25cf Infrastructure as Code: Terraform to define infrastructure components such as S3 buckets, ECS clusters, EC2 instances for MongoDB databases.</p> <p>\u25cf Monitoring and Alerting: Amazon CloudWatch for monitoring and alerting.</p>"},{"location":"Ansible-Playbook-Library/","title":"Ansible Playbook Library","text":""},{"location":"Ansible-Playbook-Library/#coming-soon","title":"Coming Soon !","text":""},{"location":"Banking%20Web%20Application/","title":"Banking Web Application DevOps","text":"<p>Create a web-based banking application that includes load balancing using Nginx and microservice tracking using Application Gateway with distributed tracing.</p> <p>Tools</p> <p>\u25cf Web Server: Nginx for load balancing.</p> <p>\u25cf Microservice Tracking: AWS Elastic Load Balancing for managing web traffic and enabling distributed tracing.</p>"},{"location":"CI-CD-Beginner/","title":"CI/CD Beginner","text":"<p>In the ever-evolving landscape of software development, the seamless integration of efficient and reliable Continuous Integration/Continuous Deployment (CI/CD) pipelines is imperative for delivering high-quality applications with speed and consistency. This project embarks on the journey of demystifying the CI/CD process using GitHub Actions and orchestrating the deployment of a Dockerized application to both development and production environments within the Amazon Web Services (AWS) ecosystem.</p> <p>The core objective of this project is to provide a clear understanding of the fundamental concepts surrounding CI/CD, containerization with Docker, and deployment in AWS environments. By leveraging GitHub Actions, a powerful and flexible automation tool integrated into the GitHub platform, developers gain the ability to automate workflows, conduct automated testing, and seamlessly deploy applications, all within the same version control environment.</p> <p>The chosen technology stack revolves around Docker, offering containerization to encapsulate the application and its dependencies, ensuring consistency across various environments. AWS serves as the deployment platform, with a focus on providing scalable and resilient infrastructure to host Dockerized applications.</p> <p>Throughout this project, we will explore the step-by-step process of building a CI/CD pipeline, containerizing an application, and deploying it to both a development and a production environment. By the end, developers will not only have a functional CI/CD pipeline but will also grasp the principles and practices essential for streamlining software development workflows in a modern, cloud-native context. Let's embark on this journey to unlock the potential of CI/CD, Docker, and AWS for a more efficient and robust software development lifecycle.</p>"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/","title":"Creating a Kubernetes Cluster with Minikube","text":"<p>Introduction</p> <p>Minikube is a tool that allows you to run Kubernetes clusters on your local machine. It's great for development, testing, and learning Kubernetes concepts. This guide will walk you through the process of setting up a Kubernetes cluster using Minikube.</p> <p>Prerequisites</p> <p>Hypervisor: Minikube requires a hypervisor to run the virtual machines. Common choices include VirtualBox, Hyper-V, KVM, or Docker. Ensure your preferred hypervisor is installed and properly configured.We use Docker for that </p> <p>Docker | installation guide Click Here</p> <p>kubectl: The kubectl command-line tool is used to interact with your Kubernetes cluster. Make sure it's installed on your machine.</p> <p>Kubectl | installation guide Click Here</p>"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#install-minikube","title":"Install Minikube","text":"<p>Visit the Minikube Installation Guide and follow the instructions for your operating system to install Minikube.</p> <p>Minikube | installation guide Click Here</p>"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#start-minikube","title":"Start Minikube","text":"<p>Open a terminal and run the following command to start Minikube:</p> <p><code>minikube start</code></p> <p>This command will download the Minikube ISO, start a virtual machine, and set up the Kubernetes cluster.</p>"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#verify-cluster-status","title":"Verify Cluster Status","text":"<p>After Minikube has started, you can check the cluster status using:</p> <p><code>kubectl cluster-info</code></p> <p>This command will display information about the cluster, including the Kubernetes master and services.</p>"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#check-nodes","title":"Check Nodes","text":"<p>Verify that Minikube has created a node for your cluster:</p> <p><code>kubectl get nodes</code></p> <p>This command should show the Minikube node with a status of Ready.</p>"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#kubernetes-dashboard-optional","title":"Kubernetes Dashboard (Optional)","text":"<p>If you want to use the Kubernetes Dashboard, you can start it with:</p> <p><code>minikube dashboard</code></p> <p>This will open a web browser with the Kubernetes Dashboard, providing a visual interface to manage your cluster.</p>"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#interact-with-kubernetes","title":"Interact with Kubernetes","text":"<p>Now that your Minikube cluster is running, you can start deploying applications and managing your Kubernetes cluster using kubectl.</p>"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#example-deployment","title":"Example Deployment","text":"<p>As a simple test, let's deploy a sample Nginx application. Save the following YAML as nginx-deployment.yaml:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n        ports:\n        - containerPort: 80\n</code></pre> <p>Apply the configuration:</p> <p><code>kubectl apply -f nginx-deployment.yaml</code></p> <p>This will deploy two replicas of the Nginx web server.</p>"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#conclusion","title":"Conclusion","text":"<p>Congratulations! You've successfully created a Kubernetes cluster using Minikube and deployed a sample application. This is just the beginning\u2014explore more Kubernetes features and concepts as you continue your journey with container orchestration.</p>"},{"location":"Logging-with-ELK-Stack/","title":"Logging with ELK Stack","text":""},{"location":"Logging-with-ELK-Stack/#coming-soon","title":"Coming Soon !","text":""},{"location":"Monitoring-with-Prometheus/","title":"Monitoring with Prometheus","text":"<p>In the intricate tapestry of modern software architecture, the ability to monitor and analyze the performance and health metrics of applications is pivotal. This project embarks on a journey to demystify the realm of monitoring by introducing Prometheus, a powerful open-source monitoring and alerting toolkit, and Grafana, a versatile platform for visualizing time-series data.</p> <p>Monitoring applications in real-time is indispensable for identifying bottlenecks, detecting anomalies, and ensuring optimal performance. Prometheus, with its robust data model and flexible querying language, has emerged as a cornerstone in the domain of monitoring, providing developers and operators with the tools needed to gain insights into the intricate workings of their systems.</p> <p>This project is designed to elucidate the fundamental concepts of Prometheus and Grafana by guiding participants through the process of setting up a monitoring infrastructure for their applications. By integrating Prometheus to collect and store time-series data and Grafana to create intuitive dashboards, developers can gain actionable insights into the health and performance of their systems.</p> <p>Throughout this project, we will delve into the intricacies of Prometheus, exploring its capabilities for metric collection, querying, and alerting. Grafana will then complement this monitoring setup by providing a visually appealing and customizable interface to represent and analyze the collected metrics.</p> <p>By the culmination of this project, participants will have a solid understanding of how to implement Prometheus for monitoring and Grafana for visualization, empowering them to make informed decisions based on real-time data. Let's embark on this exploration of monitoring and visualization to unlock the potential of Prometheus and Grafana in enhancing the observability of your applications.</p>"},{"location":"Open%20Source%20Photo%20Gallery%20Application/","title":"Open Source Photo Gallery Application DevOps","text":"<p>Description</p> <p>An open-source version of the photo gallery application.</p> <p>Tools</p> <p>\u25cf Version Control: GitHub for code storage.</p> <p>\u25cf CI/CD: GitHub Actions for automating deployment.</p> <p>\u25cf Infrastructure as Code: Terraform for defining infrastructure elements.</p> <p>\u25cf Database and Log Monitoring: ELK Stack (Elasticsearch Logstash, Kibana) for log monitoring.</p> <p>\u25cf Alerting: Prometheus and Grafana for alerting.</p> <p>\u25cf Automatic Deployment: Deploy the application automatically.</p>"},{"location":"Real-Time%20Chat%20Application/","title":"Real-Time Chat Application DevOps","text":"<p>Description</p> <p>Create a real-time chat application using message brokers and set up Application Performance Metrics (APM) in a Kubernetes cluster.</p> <p>Tools</p> <p>\u25cf Container Orchestration: Kubernetes for managing containers and applications.</p> <p>\u25cf Message Brokers: Utilize message brokers like Apache Kafka or RabbitMQ for real-time messaging.</p>"},{"location":"Scan-Docker-Container/","title":"Scan Docker Container","text":"<p>This project aims to elucidate the crucial practices of security and vulnerability checking within Docker containers through the lens of CI/CD pipelines. By integrating automated testing and security scanning into the development workflow, developers can proactively identify and address security vulnerabilities, thereby bolstering the resilience of their applications.</p> <p>The pipeline will be designed to execute automated tests, validating the functional aspects of the application, and concurrently performing security scans to scrutinize the containerized environment for potential vulnerabilities. This dual-pronged approach ensures that the development process not only meets functional requirements but also adheres to fundamental security principles.</p> <p>By the culmination of this project, participants will have a practical understanding of how to implement an end-to-end CI/CD pipeline that includes automated testing for functional correctness and security scans to fortify Docker containers against potential vulnerabilities. Through this exploration, developers will be equipped with the knowledge and tools to embed security into their development workflows, fostering a proactive and secure approach to containerized application deployment. Let's embark on this journey to integrate security seamlessly into the CI/CD pipeline, fortifying our applications in the ever-evolving landscape of software development.</p>"},{"location":"Solution-Architecht/","title":"Solution Architecht Projects","text":"<p>Description</p> <p>Create a real-time chat application using message brokers and set up Application Performance Metrics (APM) in a Kubernetes cluster.</p> <p>Tools</p> <p>\u25cf Container Orchestration: Kubernetes for managing containers and applications.</p> <p>\u25cf Message Brokers: Utilize message brokers like Apache Kafka or RabbitMQ for real-time messaging.</p>"},{"location":"Terraform-Starter/","title":"Terraform Starter","text":""},{"location":"Terraform-Starter/#coming-soon","title":"Coming Soon !","text":""},{"location":"about/","title":"About","text":""},{"location":"about/#who-we-are","title":"Who We Are","text":"<p>Our career training turns ambitions into job-ready skills and business goals into tangible results. We follow the teaching method that helps students to understand the concepts and implement it by themselves.</p> <p>You can choose technology career track that includes Cloud Computing, Linux Administration, Network Engineering and Cybersecurity.</p> <p>The CloudSpace Academy training aims to make you an expert in your chosen career track and make you capable of implementing your skills in a job. Our training program has been recognized for empowering and teaching underprivileged communities.</p>"},{"location":"aws-api-gateway/","title":"API Gateway integration with Lambda, Mock, and AWS Service integrations","text":""},{"location":"aws-api-gateway/#overview","title":"Overview","text":"<p>We\u2019re going to set up an API Gateway REST API, with a few different endpoints, including Mock, Lambda, and AWS service (SNS) integrations.</p> <p>You can read about the differences between REST and HTTP APIs here: https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-vs-rest.html</p> <p>For this demo we will use REST, because it allows the use of Mock integrations.</p> <p>I will be creating this in the ap-southeast-2 region, so all links to the console will be there. Make sure you change region if you\u2019re deploying elsewhere.</p>"},{"location":"aws-api-gateway/#instructions","title":"Instructions","text":""},{"location":"aws-api-gateway/#stage-1-setting-up-sns","title":"Stage 1 - Setting up SNS","text":"<p>Head to the SNS console: https://ap-southeast-2.console.aws.amazon.com/sns/v3/home?region=ap-southeast-2#/topics</p> <p>Click on Create topic</p> <p>Set the Type to \u201cStandard\u201d</p> <p>Set the Name to be \u201cAPI-Messages\u201d</p> <p>Under Access policy, leave the Method as \u201cBasic\u201d</p> <p>Change Define who can publish messages to the topic to \u201cOnly the specified AWS accounts\u201d and enter your account ID (found in the top right of the console)</p> <p>Change Define who can subscribe to this topic to \u201cOnly the specified AWS accounts\u201d and enter your account ID again</p> <p>In the real world, this should be locked down further to only the resources you want publishing to the topic, but in this temporary example set up, locking down to just the account is fine and safe enough</p> <p>Leave all other options as default</p> <p>Click on Create topic</p> <p>On the next page, click on Create subscription</p> <p>Change the Protocol to \u201cEmail\u201d</p> <p>In the Endpoint field, enter your personal email</p> <p>Click Create subscription</p> <p>You will receive a confirmation email shortly after, with a link you need to click on. This tells SNS that you\u2019re happy to receive emails from the topic, and prevents spam from being sent via SNS.</p> <p>Side note: While writing this, my confirmation went to Spam in Gmail, so don\u2019t forget to check there.</p> <p>Your subscription should now be in the Confirmed state:</p> <p></p>"},{"location":"aws-api-gateway/#stage-2-create-the-lambda","title":"Stage 2 - Create the Lambda","text":"<p>Head to the Lambda console: https://ap-southeast-2.console.aws.amazon.com/lambda/home?region=ap-southeast-2#/functions</p> <p>Click Create function</p> <p>Leave Author from scratch selected</p> <p>Set the Function name to <code>api-return-ip</code></p> <p>Set the Runtime to \u201cPython 3.9\u201d</p> <p>Leave the Architecture as \u201cx86_64\u201d</p> <p>Click Create function</p> <p>In the Code tab, enter the following code:</p> <pre><code>def lambda_handler(event, context):   \n    return {\n        'statusCode': 200,\n        'headers': {},\n        'body': event['requestContext']['identity']['sourceIp'],\n        'isBase64Encoded': False\n        }\n</code></pre> <p>This is an extremely basic function that just returns the source IP of the requester (you).</p> <p>Don\u2019t forget to click Deploy to save the function.</p> <p></p>"},{"location":"aws-api-gateway/#stage-3-create-the-api","title":"Stage 3 - Create the API","text":"<p>Head to the API Gateway console: https://ap-southeast-2.console.aws.amazon.com/apigateway/main/apis?region=ap-southeast-2</p> <p>Click Create API</p> <p>Select REST API \u2192 Build</p> <p>Make sure you don\u2019t select \u201cREST API Private\u201d.</p> <p></p> <p>Leave the Protocol and \u201cCreate new API\u201d options as is, and set your API name to whatever you like</p> <p></p> <p>Click Create API</p> <p>Once that\u2019s done you will see all of the \u201cResources\u201d (endpoints / API paths), right now we have none, so click on Actions and then \u201cCreate Resource\u201d</p> <p></p> <p>This first resource we create will be for the Mock integration, so we\u2019ll just name it \u201cMock\u201d. The Resource Path is the URL path you will use to call it, so in this case it would be something like</p> <p><code>https://abcdef1234.execute-api.ap-southeast-2.amazonaws.com/mock</code></p> <p></p> <p>Next we have to attach a Method. A Method is the HTTP method that the resource (path) will accept, such as \u201cGET\u201d, \u201cPOST\u201d, \u201cDELETE\u201d, etc.</p> <p>For the Mock integration, we will just use \u201cGET\u201d. </p> <p>Make sure you are in the /mock resource</p> <p></p> <p>Then click Actions then \u201cCreate Method\u201d</p> <p></p> <p>Select \u201cGET\u201d</p> <p></p> <p>Then click the tick to accept.</p> <p>Once that\u2019s done, API Gateway will present a list of possible integrations. For this one, select \u201cMock\u201d then click Save</p> <p></p> <p>Once that\u2019s done, click on \u201cIntegration Response\u201d</p> <p></p> <p>This is where we tell API Gateway what the Mock integration should respond with. </p> <p>Expand the 200 status line, then the Mapping Templates section, and set the Content-Type to <code>application/json</code></p> <p></p> <p>Click on the tick, then in the template section, enter the following (you can replace the message with whatever you like)</p> <pre><code>{\n    \"statusCode\": 200,\n    \"message\": \"This response is mocking you\"\n}\n</code></pre> <p>Then click Save (it won\u2019t give any feedback that it\u2019s saved, but it has)</p> <p></p> <p>Then click Save on the method response</p> <p></p> <p>That\u2019s all done. Now we\u2019ll set up the Lambda integration.</p> <p>Go back to the root (<code>/</code>)resource</p> <p></p> <p>Then click Actions then Create Resource</p> <p>For this one, set the resource name to \u201cLambda\u201d, and leave the Resource Path as \u201c/lambda\u201d</p> <p>Click Create Resource</p> <p>Click on Actions then Create Method. This will also be a \u201cGET\u201d</p> <p></p> <p>On the next page, set the \u201cIntegration type\u201d to \u201cLambda function\u201d</p> <p>Enable \u201cUse Lambda Proxy integration\u201d</p> <p>Once you click the \u201cLambda Function\u201d text field, it should drop down with a list of Lambda\u2019s in that region, select the one you created earlier</p> <p>Leave all other options as is, and click Save</p> <p></p> <p>You should see a popup telling you that you\u2019re about to give API Gateway permission to invoke your Lambda, click OK</p> <p>Lastly we\u2019ll set up another resource for SNS.</p> <p>For this one, we will need to set up an IAM role that API Gateway will use to publish messages to SNS. </p> <p>Head to the IAM Console: https://us-east-1.console.aws.amazon.com/iamv2/home?region=ap-southeast-2#/roles</p> <p>Go to the Roles page, and click Create Role</p> <p></p> <p>Under \u201cTrusted entity\u201d, select \u201cAWS Service\u201d, and in the drop-down, select API Gateway. Make sure you select the radio button for \u201cAPI Gateway\u201d as well.</p> <p></p> <p>Click Next</p> <p>On the Permissions page, click Next</p> <p>Lastly, set the role name to \u201capi-gw-sns-role\u201d then click Create role</p> <p>Now go into the role you just created</p> <p></p> <p>Click on Add permissions then Create inline policy</p> <p></p> <p>Go to the JSON tab and enter the following</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"sns:Publish\",\n            \"Resource\": \"*\"\n        }\n    ]\n}\n</code></pre> <p>Click Review policy</p> <p>Under \u201cReview policy\u201d set the name to \u201cSnsPublish\u201d and click Create policy</p> <p>On the summary page, copy the ARN of the role you just created, you will need it for the next step</p> <p></p> <p>Now head back to the API Gateway console: https://ap-southeast-2.console.aws.amazon.com/apigateway/main/apis?region=ap-southeast-2</p> <p>Go back into your REST API</p> <p>Go back to the root resource</p> <p></p> <p>Then click Actions then Create Resource</p> <p>For this one, set the resource name to \u201cSNS\u201d, and leave the Resource Path as \u201c/sns\u201d</p> <p>Click Create Resource</p> <p>Click on Actions then Create Method. </p> <p>This one will be a \u201cPOST\u201d.</p> <p>On the next page, set the \u201cIntegration type\u201d to \u201cAWS Service\u201d</p> <p>Set the AWS Region as the same region as your API / SNS (for me this is ap-southeast-2)</p> <p>Set the AWS Service as \u201cSimple Notification Service (SNS)\u201d</p> <p>Leave the AWS Subdomain blank</p> <p>Set the HTTP method to POST</p> <p>Leave the Action Type as \u201cUse action name\u201d</p> <p>Set the \u201cAction\u201d to \u201cPublish\u201d</p> <p>Under Execution Role you need to put the ARN of the IAM role you just created for SNS.  </p> <p>Leave the rest of the form as is, and click Save</p> <p></p> <p>If you\u2019re wondering where we got the \u201cPublish\u201d action, and where to find other Action types, you can view them all in the API Reference for the service: https://docs.aws.amazon.com/sns/latest/api/API_Operations.html</p> <p>There\u2019s a few ways to pass your message through API Gateway to the AWS service, but to make things easy for this example, we\u2019re going to use Query Strings. Query Strings are found in the URL of websites after a question mark, e.g. <code>https://google.com/search?q=hello</code>, where \u201cq=hello\u201d is the query string.</p> <p>Go to the \u201c/sns\u201d resource, and the \u201cPOST\u201d method, and click \u201cMethod Request\u201d </p> <p></p> <p>Under \u201cURL Query String Parameters\u201d click \u201cAdd query string\u201d and enter \u201cTopicArn\u201d and click the tick.</p> <p>Then click \u201cAdd query string\u201d and enter \u201cMessage\u201d and click the tick.</p> <p>Your query string should look like this</p> <p></p> <p>Go back to the \u201c/sns\u201d resource, and the \u201cPOST\u201d method, and click \u201cIntegration Request\u201d</p> <p></p> <p>Under \u201cURL Query String Parameters\u201d click \u201cAdd query string\u201d </p> <p>Set the Name to \u201cMessage\u201d and the Mapped from to <code>method.request.querystring.Message</code></p> <p>Click the tick to save</p> <p>Click \u201cAdd query string\u201d again</p> <p>Set the Name to \u201cTopicArn\u201d and the Mapped from to <code>method.request.querystring.TopicArn</code></p> <p>Click the tick to save</p> <p>Now we need to Deploy the API. </p> <p>Click on Actions, then Deploy API</p> <p></p> <p>In the pop up window, set the \u201cDeployment stage\u201d to \u201c[New Stage]\u201d, and the \u201cStage name\u201d to \u201cv1\u201d.</p> <p></p> <p>The \u201cStage name\u201d can really be anything, and is used to direct API requests to different \u201cversions\u201d of the API. So you could have a \u201cdev\u201d stage, and \u201cprod\u201d stage, or just use the standard \u201cv1\u201d, \u201cv2\u201d, etc.</p> <p>Click Deploy</p> <p>Once that\u2019s done, you will be sent to the Stage Editor page, where you can set things like Rate Limiting, WAF associations, caching, logs, etc. We don\u2019t need to change any of these for this demo.</p> <p>At the top of the screen you will see your API URL</p> <p></p> <p>Copy that URL for the next step.</p>"},{"location":"aws-api-gateway/#stage-4-testing-the-api","title":"Stage 4 - Testing the API","text":"<p>The Lambda and Mock resources can be tested in the browser. By default, any URL you enter into your browser performs a GET request (and remember, only our Lambda and Mock resources have the GET method set).</p>"},{"location":"aws-api-gateway/#mock","title":"Mock","text":"<p>If we visit our API URL and append \u201c/mock\u201d we should see the response we entered earlier</p> <p></p> <p>The reason we want a JSON output rather than a friendly human readable one, is because working with JSON in programming languages makes things much easier. Your code or application could read the \u201cstatusCode\u201d key and see a 200 value, and then it could read the \u201cmessage\u201d key and see it\u2019s value.</p>"},{"location":"aws-api-gateway/#lambda","title":"Lambda","text":"<p>Now if we visit our \u201c/lambda\u201d URL we should see our function response (your IP address)</p> <p></p>"},{"location":"aws-api-gateway/#sns","title":"SNS","text":"<p>For the last endpoint, we can\u2019t use a browser because browsers by default only perform \u201cGET\u201d requests, so we have a couple of options to test this endpoint.</p>"},{"location":"aws-api-gateway/#command-line","title":"Command Line","text":"<p>If you\u2019re comfortable using the command line (Linux, Mac, or WSL for Windows), follow these steps. If you would prefer to use a GUI, skip ahead.</p> <p>In your CLI, run the following command (replace \u201cREGION\u201d and \u201cACCOUNT-ID\u201d with your region and account ID), and don\u2019t forget to replace the URL with your API gateway URL</p> <pre><code>curl -X POST -G -d 'TopicArn=arn:aws:sns:REGION:ACCOUNT-ID:API-Messages' -d 'Message=Hello!'  https://abc123def.execute-api.ap-southeast-2.amazonaws.com/v1/sns\n</code></pre> <p>Note if you want to use spaces in your message, because query parameters are URL encoded, spaces need to be replaced with a +, so for example:</p> <pre><code>curl -X POST -G -d 'TopicArn=arn:aws:sns:REGION:ACCOUNT-ID:API-Messages' -d 'Message=I+like+sending+long+messages'  https://abc123def.execute-api.ap-southeast-2.amazonaws.com/v1/sns\n</code></pre>"},{"location":"aws-api-gateway/#gui-aws-console","title":"GUI (AWS Console)","text":"<p>The API Gateway console provides a handy way to test your API. </p> <p></p> <p>On the testing page, under \u201cQuery Strings\u201d, enter the following. Replace the TopicArn with your SNS Topic ARN, and the message with whatever you like.</p> <pre><code>TopicArn=arn:aws:sns:REGION:ACCOUNT-ID:API-Messages&amp;Message=APIs+are+fun\n</code></pre> <p>Note if you want to use spaces in your message, because query parameters are URL encoded, spaces need to be replaced with a +</p> <p>Scroll down and click Test</p> <p></p>"},{"location":"aws-api-gateway/#gui-postman","title":"GUI (Postman)","text":"<p>We can also use a popular API testing tool called Postman. It\u2019s available for Windows, Mac, and Linux</p> <p>It can be downloaded for free from their website: https://www.postman.com/downloads/</p> <p>I won\u2019t write instructions on installing the program, it\u2019s fairly self explanatory and isn\u2019t related to this demo.</p> <p>Once you\u2019ve opened Postman, click on the + up the top of the application to open a new tab. Set the method to POST, and enter your API Gateway URL.</p> <p>You can then enter the two Query Parameters we set earlier:</p> <p><code>TopicArn</code> which is the ARN of your SNS topic.</p> <p><code>Message</code> which is the message you want to send to the topic</p> <p></p> <p>Once you\u2019ve entered all that, hit Send</p> <p>No matter which method you chose, you should receive an email from SNS containing the message in your Query String</p> <p></p>"},{"location":"aws-api-gateway/#stage-5-cleaning-up","title":"Stage 5 - Cleaning up","text":"<p>Head to the API Gateway console: https://ap-southeast-2.console.aws.amazon.com/apigateway/main/apis?region=ap-southeast-2</p> <p>Select the API you created, and click Actions then Delete</p> <p></p> <p>Head to the SNS console: https://ap-southeast-2.console.aws.amazon.com/sns/v3/home?region=ap-southeast-2#/topics</p> <p>Go to Topics, select the Topic you created earlier, and click Delete</p> <p></p> <p>In the confirmation box, enter \u201cdelete me\u201d and click Delete</p> <p>Go to Subscriptions, select the Subscription you created for your email, and click Delete</p> <p></p> <p>Head to the IAM console: https://us-east-1.console.aws.amazon.com/iamv2/home?region=ap-southeast-2#/roles</p> <p>Under Roles, search for \"api-gw-sns-role\u201d</p> <p>Select the role we created earlier, and click Delete</p> <p></p> <p>Type \u201capi-gw-sns-role\u201d into the confirmation field, and click Delete</p> <p>Head to the Lambda console: https://ap-southeast-2.console.aws.amazon.com/lambda/home?region=ap-southeast-2#/functions</p> <p>Select the function we created earlier, and click Actions then Delete</p> <p></p> <p>Enter \u201cdelete\u201d in the confirmation window, and then click Delete</p> <p>Head to the Cloudwatch Logs console: https://ap-southeast-2.console.aws.amazon.com/cloudwatch/home?region=ap-southeast-2#logsV2:log-groups</p> <p>Search for the \"api-return-ip\u201d Log Group, select the log group , click Actions then Delete</p> <p></p> <p>In the confirmation popup, click Delete</p>"},{"location":"aws-cloudtrail-log-file-integrity/","title":"Aws Cloudtrail Log File Integrity","text":""},{"location":"aws-cloudtrail-log-file-integrity/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-cognito-web-identity-federation/","title":"AWS Cognito Web Identity Federation","text":""},{"location":"aws-cognito-web-identity-federation/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-control-tower/","title":"AWS Control Tower","text":""},{"location":"aws-control-tower/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-dms-database-migration/","title":"AWS DMS Database Migration","text":""},{"location":"aws-dms-database-migration/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-dynamodb-lambda-trigger/","title":"AWS Dynamodb Lambda Trigger","text":""},{"location":"aws-dynamodb-lambda-trigger/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-efs/","title":"AWS EFS","text":""},{"location":"aws-efs/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-elastic-disaster-recovery/","title":"AWS Elastic Disaster Recovery","text":""},{"location":"aws-elastic-disaster-recovery/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-global-accelerator/","title":"AWS Global Accelerator","text":""},{"location":"aws-global-accelerator/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-iam-scp-permissions-boundary/","title":"AWS IAM Scp Permissions Boundary","text":""},{"location":"aws-iam-scp-permissions-boundary/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-lambda-s3-events/","title":"AWS Lambda S3 Events","text":""},{"location":"aws-lambda-s3-events/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-lambda-xray/","title":"AWS Lambda Xray","text":""},{"location":"aws-lambda-xray/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-lex-lambda-rds/","title":"AWS Lex Lambda RDS","text":""},{"location":"aws-lex-lambda-rds/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-macie/","title":"AWS Macie","text":""},{"location":"aws-macie/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-patch-manager/","title":"AWS Patch Manager","text":""},{"location":"aws-patch-manager/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-pet-rekognition-ecr/","title":"AWS Pet Rekognition ECR","text":""},{"location":"aws-pet-rekognition-ecr/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-systems-manager/","title":"AWS Systems Manager","text":""},{"location":"aws-systems-manager/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-video-on-demand/","title":"AWS Video On Demand","text":""},{"location":"aws-video-on-demand/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-vpc-flow-logs/","title":"AWS Vpc Flow Logs","text":""},{"location":"aws-vpc-flow-logs/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-waf/","title":"AWS Waf","text":""},{"location":"aws-waf/#coming-soon","title":"Coming Soon !","text":""},{"location":"dockerbeginning/","title":"Flask Calculator Web Application","text":"<p>Introduction</p> <p>This documentation provides an overview of the Flask calculator web application and guides you on how to set up and run the project. This simple web application allows users to perform basic arithmetic calculations such as addition, subtraction, multiplication, and division.</p> <p>Tools Requirements</p> <p>Python | installation guide Click Here </p> <p>Docker | installation guide Click Here</p>"},{"location":"dockerbeginning/#getting-started","title":"Getting Started","text":"<p>Open terminal and run this command to clone the repository of Flask Calculator Web Application.</p> <p>Command:</p> <pre><code>git clone https://github.com/cloudspaceacademy/Flask-Calculator-app.git\n</code></pre> <p>Now navigate to the directory Flask-Calculator-app. You will see a cloudspace directory now open that directory using the terminal. </p> <p>You can simply use this command:</p> <p><code>cd Flask-Calculator-app &amp;&amp; cd cloudspace</code></p> <p>Now run  <code>ls</code>  command and Now what are the scenarios there you will see app.py,Dockerfile,requirements.txt file and a directory named templates.</p> <p>Let's elaborate why we use that Dockerfile for this application and how we write that Dockerfile.</p>"},{"location":"dockerbeginning/#why-do-we-use-docker","title":"Why do we use docker?","text":"<p>Docker provides a way to package, distribute, and run applications consistently across various environments. It simplifies the process of creating, deploying, and maintaining applications while improving portability, scalability, and resource efficiency. The Dockerfile is at the heart of this process, defining how the container image is built and what software is included, making it a valuable tool for modern software development and deployment.</p> <p>Now let's elaborate how we write that Dockerfile to package our application.</p>"},{"location":"dockerbeginning/#dockerfile","title":"Dockerfile","text":"<pre><code>FROM python:3.9-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\n\nRUN pip install -r requirements.txt\n\nCOPY . .\n\nEXPOSE 5000\n\nCMD [\"python\", \"app.py\"] \n</code></pre> <p>Now just break down all the code and elaborate why they use for </p> <p>FROM python:3.9-slim</p> <p>This line specifies the base image for our Docker container. In this case, it's using the official Python 3.9 slim image. The \"slim\" variant provides a smaller image size by excluding some non-essential components, making it suitable for many Python applications.</p> <p>WORKDIR /app</p> <p>This line sets the working directory inside the container to /app. This is where the application code and files will be placed, and it's the directory where commands will be executed by default. It's a good practice to organize your application within a dedicated directory.</p> <p>COPY requirements.txt .</p> <p>This line copies the requirements.txt file from the host system into the container. The . at the end specifies the destination path, which is the current directory within the container (i.e., /app).</p> <p>RUN pip install -r requirements.txt</p> <p>This line uses the RUN instruction to execute a command within the container. In this case, it runs pip install -r requirements.txt to install all the Python packages listed in the requirements.txt file. This step ensures that the necessary dependencies are installed in the container.</p> <p>COPY . .</p> <p>This line copies all the files and directories from the current directory (the directory where the Dockerfile is located) on our host system to the /app directory in the container. This includes our application code (app.py) and any other files or directories in the project folder.</p> <p>EXPOSE 5000</p> <p>This line informs Docker that the container will listen on port 5000. While this line is not necessary for port mapping, it serves as a form of documentation to indicate which ports the container expects to use. It's good practice for clarity.</p> <p>CMD [\"python\", \"app.py\"]</p> <p>This line defines the default command to execute when the container is started. It runs the Flask application by executing python app.py. The Flask development server will start, and our application will be served on port 5000 as specified earlier.</p> <p>Now lets learn how to run the application by using just two docker commands from your local machine. </p>"},{"location":"dockerbeginning/#running-the-application-with-docker","title":"Running the Application with Docker","text":""},{"location":"dockerbeginning/#building-a-docker-image","title":"Building a Docker Image","text":"<p>To build a Docker image, you use the docker build command. </p> <p><code>docker build -t flask-calculator .</code></p> <p>docker build -t flask-calculator .  Run this command where the Dockerfile is located. Our Dockerfile is located in the root directory cloudspace so that we use ( . )  </p> <p>Now break down the code </p> <p>docker build: This is the Docker command for building an image.</p> <p>-t flask-calculator: This part specifies the image name and tag. In this example, the image is named \"flask-calculator,\" and it has no specific tag. The tag is used to version your images; if you don't specify one, it defaults to \"latest.\"</p> <p>.: The dot (.) at the end of the command tells Docker to use the current directory as the build context. It means that Docker will look in the current directory for a Dockerfile to use as the basis for building the image.</p>"},{"location":"dockerbeginning/#docker-image-used-for","title":"Docker image used for :","text":"<p>A Docker image is a lightweight, standalone, executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, system libraries, and settings. Images are used as the basis for creating Docker containers.</p>"},{"location":"dockerbeginning/#running-a-docker-container","title":"Running a Docker Container","text":"<p>Once we have built our Docker image, we can create and run containers from it using the docker run command. </p> <p><code>docker run -p 5000:80 -d flask-calculator</code></p> <p>Now break down the code </p> <p>docker run: This is the Docker command for creating and running a container.</p> <p>-p 5000:80: This part maps port 5000 from our host machine to port 80 inside the container. It allows us to access our Flask application running in the container on port 5000.</p> <p>-d: this defines the container will run in detached mode, which means it runs in the background.</p> <p>flask-calculator: This is the name of the Docker image you want to use to create the container. The image name corresponds to the one you specified when building the image.</p>"},{"location":"dockerbeginning/#docker-container-used-for","title":"Docker Container used for :","text":"<p>A Docker container is a runnable instance of a Docker image. It is an isolated environment where our application runs. Containers share the same OS kernel as the host system but are isolated from each other. They are ephemeral and can be started, stopped, and removed as needed.</p>"},{"location":"dockerbeginning/#access-the-application","title":"Access the application","text":"<p>Now you will see the app is running from your local machine on port 5000. Open your web browser and go to localhost:5000 to access the Flask calculator web application within the Docker container.</p>"},{"location":"dockercomposebeginning/","title":"Flask Calculator Web Application","text":"<p>Introduction</p> <p>This documentation provides an overview of the Flask calculator web application and guides you on how to set up and run the project. The Flask calculator is a straightforward web application designed for basic arithmetic calculations, including addition, subtraction, multiplication, and division.</p> <p>Tools Requirements</p> <p>Python | installation guide Click Here </p> <p>Docker | installation guide Click Here</p>"},{"location":"dockercomposebeginning/#getting-started","title":"Getting Started","text":"<p>Open your terminal and clone the Flask Calculator Web Application repository using the following command:</p> <pre><code>git clone https://github.com/cloudspaceacademy/Flask-Calculator-app.git\n</code></pre> <p>Navigate to the \"Flask-Calculator-app\" directory By running this command :</p> <p><code>cd Flask-Calculator-app</code></p> <p>Inside the \"Flask-Calculator-app\" directory, you will find a \"cloudspace\" directory. Enter this directory using the following command:</p> <p><code>cd cloudspace</code></p> <p>In Previous session \"Docker Beginner\" you have learned about how to run this \"Flask Calculator Web Application\" using Dockerfile.Now we are gonna learn how to run this application using docker-compose.yml file.</p> <p>In Previous session \"Docker Beginner\" you have learned about Dockerfile.How To write a Dockerfile,why its used for,how to run the application using Dockerfile etc.Now let's jump up to docker-compose.yml file.</p>"},{"location":"dockercomposebeginning/#docker-composeyml-file-used-for","title":"docker-compose.yml file used for","text":"<p>A docker-compose.yml file is used to define a multi-container Docker application. It allows you to define and configure multiple services, networks, and volumes in a single file. Docker Compose simplifies the process of orchestrating complex applications by providing a way to manage multiple containers as a single application stack.</p> <p>Simply Dockerfile is used for single service or to run the application using one container and docker-compose.yml file is used for multiple services or to packaging and run the application using multi containers.</p>"},{"location":"dockercomposebeginning/#docker-composeyml","title":"docker-compose.yml","text":"<pre><code>version: '3.8'\n\nservices:\n  flask-calculator:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - \"8080:80\"\n\n</code></pre> <p>Let's Break down the code </p> <p>version: '3.8':Specifies the version of the Docker Compose file syntax. In this case, it's using version 3.8.</p> <p>services:Defines the services that make up the Docker application.</p> <p>flask-calculator:The name of the service. In this case, it's named flask-calculator.</p> <p>build:Specifies how to build the Docker image for the service.</p> <p>context: . :Specifies the build context, which is the path to the directory containing the Dockerfile and any files needed for the build. In this case, it's set to the current directory (.)</p> <p>dockerfile: Dockerfile:Specifies the name of the Dockerfile to use for building the image. In this case, it's named Dockerfile.</p> <p>ports:Specifies the ports to expose from the container. - \"8080:80\":</p> <p>Maps port 80 from the container to port 8080 on the host machine. The format is host_port:container_port.</p>"},{"location":"dockercomposebeginning/#running-the-application-with-docker-compose","title":"Running the Application with Docker Compose","text":"<p>To run the application run this command where the docker-compose.yml file is located.</p> <p>Command :</p> <p><code>docker-compose up -d</code> </p> <p>Break down Codes:</p> <p>docker-compose: The Docker Compose command-line tool.</p> <p>up: This command is used to create and start containers based on the configurations specified in the docker-compose.yml file.</p> <p>-d: Stands for \"detached mode.\" When you run Docker containers in detached mode, the containers run in the background, and you get your terminal prompt back. This allows you to continue using the same terminal session for other tasks.</p> <p>Now Run <code>docker ps</code> command to see the running container id that you have just created.</p>"},{"location":"dockercomposebeginning/#access-the-application","title":"Access the application","text":"<p>Now you will see the app is running from your local machine on port 8080. Open your web browser and go to localhost:8080 to access the Flask calculator web application within the Docker container.</p>"}]}