{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Cloud Space","text":"<p>Website : www.cloudspaceacademy.com   info@cloudspaceacademy.com   Phone: +1-855-200-7653   WhatsApp: +1-571-454-4691  Facebook </p> <p>At CloudSpace, our students' success is our top priority. We understand it can be difficult to study alone once the BootCamp is over. CloudSpace is committed to providing you with the necessary support until you land that dream job. The FREE follow-up program is a dedicated place to keep learning with an industry leader DevOps instructor until you land a job. All this free of charge as long as you have completed the AWS Cloud &amp; DevOps BootCamp with us. So after completing the 6 months AWS &amp; DevOps Bootcamp, you can attend the Free Follow-up Program free of charge on a weekly basis to keep working on interview preparation and DevOps projects with an experienced instructor. So you will be allowed to attend this post-bootcamp until you land a job.</p> <p>Follow-Up Program Schedule: Thursdays (8PM-9PM) Interview Prep - Sundays (1PM-3PM) Projects Hands On</p>"},{"location":"AWS%20E-Commerce%20Application/","title":"AWS E-Commerce Application DevOps","text":"<p>Description</p> <p>Develop and deploy a full-fledged e-commerce application in an AWS environment. This includes a database, S3 bucket for storage, ECS for deployment, and infrastructure as code with CloudFormation.</p> <p>Tools</p> <p>\u25cf Infrastructure as Code: AWS CloudFormation to define the infrastructure.</p> <p>\u25cf Database: AWS RDS or DynamoDB for database management.</p> <p>\u25cf Storage: AWS S3 for file storage.</p> <p>\u25cf Deployment: AWS ECS for container-based deployment.</p> <p>\u25cf CI/CD: Automate deployment using GitHub Actions.</p>"},{"location":"AWS%20Photo%20Gallery%20Application/","title":"AWS Photo Gallery Application DevOps","text":"<p>Description</p> <p>This application is a photo gallery platform similar to Pexels . Users can freely upload and download images. It leverages AWS services for scalability and reliability.</p> <p>Tools</p> <p>\u25cf Version Control: AWS CodeCommit for code storage.</p> <p>\u25cf CI/CD: AWS CodePipeline for automating deployment.</p> <p>\u25cf Infrastructure as Code: Terraform to define infrastructure components such as S3 buckets, ECS clusters, EC2 instances for MongoDB databases.</p> <p>\u25cf Monitoring and Alerting: Amazon CloudWatch for monitoring and alerting.</p>"},{"location":"Ansible-Playbook-Library/","title":"Ansible Playbook Library","text":""},{"location":"Ansible-Playbook-Library/#coming-soon","title":"Coming Soon !","text":""},{"location":"Banking%20Web%20Application/","title":"Banking Web Application DevOps","text":"<p>Create a web-based banking application that includes load balancing using Nginx and microservice tracking using Application Gateway with distributed tracing.</p> <p>Tools</p> <p>\u25cf Web Server: Nginx for load balancing.</p> <p>\u25cf Microservice Tracking: AWS Elastic Load Balancing for managing web traffic and enabling distributed tracing.</p>"},{"location":"CI-CD-Beginner/","title":"CI/CD Beginner","text":"<p>In the ever-evolving landscape of software development, the seamless integration of efficient and reliable Continuous Integration/Continuous Deployment (CI/CD) pipelines is imperative for delivering high-quality applications with speed and consistency. This project embarks on the journey of demystifying the CI/CD process using GitHub Actions and orchestrating the deployment of a Dockerized application to both development and production environments within the Amazon Web Services (AWS) ecosystem.</p> <p>The core objective of this project is to provide a clear understanding of the fundamental concepts surrounding CI/CD, containerization with Docker, and deployment in AWS environments. By leveraging GitHub Actions, a powerful and flexible automation tool integrated into the GitHub platform, developers gain the ability to automate workflows, conduct automated testing, and seamlessly deploy applications, all within the same version control environment.</p> <p>The chosen technology stack revolves around Docker, offering containerization to encapsulate the application and its dependencies, ensuring consistency across various environments. AWS serves as the deployment platform, with a focus on providing scalable and resilient infrastructure to host Dockerized applications.</p> <p>Throughout this project, we will explore the step-by-step process of building a CI/CD pipeline, containerizing an application, and deploying it to both a development and a production environment. By the end, developers will not only have a functional CI/CD pipeline but will also grasp the principles and practices essential for streamlining software development workflows in a modern, cloud-native context. Let's embark on this journey to unlock the potential of CI/CD, Docker, and AWS for a more efficient and robust software development lifecycle.</p>"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/","title":"Creating a Kubernetes Cluster with Minikube","text":"<p>Introduction</p> <p>Minikube is a tool that allows you to run Kubernetes clusters on your local machine. It's great for development, testing, and learning Kubernetes concepts. This guide will walk you through the process of setting up a Kubernetes cluster using Minikube.</p> <p>Prerequisites</p> <p>Hypervisor: Minikube requires a hypervisor to run the virtual machines. Common choices include VirtualBox, Hyper-V, KVM, or Docker. Ensure your preferred hypervisor is installed and properly configured.We use Docker for that </p> <p>Docker | installation guide Click Here</p> <p>kubectl: The kubectl command-line tool is used to interact with your Kubernetes cluster. Make sure it's installed on your machine.</p> <p>Kubectl | installation guide Click Here</p>"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#install-minikube","title":"Install Minikube","text":"<p>Visit the Minikube Installation Guide and follow the instructions for your operating system to install Minikube.</p> <p>Minikube | installation guide Click Here</p>"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#start-minikube","title":"Start Minikube","text":"<p>Open a terminal and run the following command to start Minikube:</p> <p><code>minikube start</code></p> <p>This command will download the Minikube ISO, start a virtual machine, and set up the Kubernetes cluster.</p>"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#verify-cluster-status","title":"Verify Cluster Status","text":"<p>After Minikube has started, you can check the cluster status using:</p> <p><code>kubectl cluster-info</code></p> <p>This command will display information about the cluster, including the Kubernetes master and services.</p>"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#check-nodes","title":"Check Nodes","text":"<p>Verify that Minikube has created a node for your cluster:</p> <p><code>kubectl get nodes</code></p> <p>This command should show the Minikube node with a status of Ready.</p>"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#kubernetes-dashboard-optional","title":"Kubernetes Dashboard (Optional)","text":"<p>If you want to use the Kubernetes Dashboard, you can start it with:</p> <p><code>minikube dashboard</code></p> <p>This will open a web browser with the Kubernetes Dashboard, providing a visual interface to manage your cluster.</p>"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#interact-with-kubernetes","title":"Interact with Kubernetes","text":"<p>Now that your Minikube cluster is running, you can start deploying applications and managing your Kubernetes cluster using kubectl.</p>"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#example-deployment","title":"Example Deployment","text":"<p>As a simple test, let's deploy a sample Nginx application. Save the following YAML as nginx-deployment.yaml:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n        ports:\n        - containerPort: 80\n</code></pre> <p>Apply the configuration:</p> <p><code>kubectl apply -f nginx-deployment.yaml</code></p> <p>This will deploy two replicas of the Nginx web server.</p>"},{"location":"Kubernetes-Minikube%20Cluster%20Beginner/#conclusion","title":"Conclusion","text":"<p>Congratulations! You've successfully created a Kubernetes cluster using Minikube and deployed a sample application. This is just the beginning\u2014explore more Kubernetes features and concepts as you continue your journey with container orchestration.</p>"},{"location":"Logging-with-ELK-Stack/","title":"Logging with ELK Stack","text":""},{"location":"Logging-with-ELK-Stack/#coming-soon","title":"Coming Soon !","text":""},{"location":"Monitoring-with-Prometheus/","title":"Monitoring with Prometheus","text":"<p>In the intricate tapestry of modern software architecture, the ability to monitor and analyze the performance and health metrics of applications is pivotal. This project embarks on a journey to demystify the realm of monitoring by introducing Prometheus, a powerful open-source monitoring and alerting toolkit, and Grafana, a versatile platform for visualizing time-series data.</p> <p>Monitoring applications in real-time is indispensable for identifying bottlenecks, detecting anomalies, and ensuring optimal performance. Prometheus, with its robust data model and flexible querying language, has emerged as a cornerstone in the domain of monitoring, providing developers and operators with the tools needed to gain insights into the intricate workings of their systems.</p> <p>This project is designed to elucidate the fundamental concepts of Prometheus and Grafana by guiding participants through the process of setting up a monitoring infrastructure for their applications. By integrating Prometheus to collect and store time-series data and Grafana to create intuitive dashboards, developers can gain actionable insights into the health and performance of their systems.</p> <p>Throughout this project, we will delve into the intricacies of Prometheus, exploring its capabilities for metric collection, querying, and alerting. Grafana will then complement this monitoring setup by providing a visually appealing and customizable interface to represent and analyze the collected metrics.</p> <p>By the culmination of this project, participants will have a solid understanding of how to implement Prometheus for monitoring and Grafana for visualization, empowering them to make informed decisions based on real-time data. Let's embark on this exploration of monitoring and visualization to unlock the potential of Prometheus and Grafana in enhancing the observability of your applications.</p>"},{"location":"Open%20Source%20Photo%20Gallery%20Application/","title":"Open Source Photo Gallery Application DevOps","text":"<p>Description</p> <p>An open-source version of the photo gallery application.</p> <p>Tools</p> <p>\u25cf Version Control: GitHub for code storage.</p> <p>\u25cf CI/CD: GitHub Actions for automating deployment.</p> <p>\u25cf Infrastructure as Code: Terraform for defining infrastructure elements.</p> <p>\u25cf Database and Log Monitoring: ELK Stack (Elasticsearch Logstash, Kibana) for log monitoring.</p> <p>\u25cf Alerting: Prometheus and Grafana for alerting.</p> <p>\u25cf Automatic Deployment: Deploy the application automatically.</p>"},{"location":"Real-Time%20Chat%20Application/","title":"Real-Time Chat Application DevOps","text":"<p>Description</p> <p>Create a real-time chat application using message brokers and set up Application Performance Metrics (APM) in a Kubernetes cluster.</p> <p>Tools</p> <p>\u25cf Container Orchestration: Kubernetes for managing containers and applications.</p> <p>\u25cf Message Brokers: Utilize message brokers like Apache Kafka or RabbitMQ for real-time messaging.</p>"},{"location":"Scan-Docker-Container/","title":"Scan Docker Container","text":"<p>This project aims to elucidate the crucial practices of security and vulnerability checking within Docker containers through the lens of CI/CD pipelines. By integrating automated testing and security scanning into the development workflow, developers can proactively identify and address security vulnerabilities, thereby bolstering the resilience of their applications.</p> <p>The pipeline will be designed to execute automated tests, validating the functional aspects of the application, and concurrently performing security scans to scrutinize the containerized environment for potential vulnerabilities. This dual-pronged approach ensures that the development process not only meets functional requirements but also adheres to fundamental security principles.</p> <p>By the culmination of this project, participants will have a practical understanding of how to implement an end-to-end CI/CD pipeline that includes automated testing for functional correctness and security scans to fortify Docker containers against potential vulnerabilities. Through this exploration, developers will be equipped with the knowledge and tools to embed security into their development workflows, fostering a proactive and secure approach to containerized application deployment. Let's embark on this journey to integrate security seamlessly into the CI/CD pipeline, fortifying our applications in the ever-evolving landscape of software development.</p>"},{"location":"Solution-Architecht/","title":"Solution Architecht Projects","text":"<p>Description</p> <p>Create a real-time chat application using message brokers and set up Application Performance Metrics (APM) in a Kubernetes cluster.</p> <p>Tools</p> <p>\u25cf Container Orchestration: Kubernetes for managing containers and applications.</p> <p>\u25cf Message Brokers: Utilize message brokers like Apache Kafka or RabbitMQ for real-time messaging.</p>"},{"location":"Terraform-Starter/","title":"Terraform Starter","text":""},{"location":"Terraform-Starter/#coming-soon","title":"Coming Soon !","text":""},{"location":"about/","title":"About","text":""},{"location":"about/#who-we-are","title":"Who We Are","text":"<p>Our career training turns ambitions into job-ready skills and business goals into tangible results. We follow the teaching method that helps students to understand the concepts and implement it by themselves.</p> <p>You can choose technology career track that includes Cloud Computing, Linux Administration, Network Engineering and Cybersecurity.</p> <p>The CloudSpace Academy training aims to make you an expert in your chosen career track and make you capable of implementing your skills in a job. Our training program has been recognized for empowering and teaching underprivileged communities.</p>"},{"location":"aws-api-gateway/","title":"API Gateway integration with Lambda, Mock, and AWS Service integrations","text":""},{"location":"aws-api-gateway/#overview","title":"Overview","text":"<p>We\u2019re going to set up an API Gateway REST API, with a few different endpoints, including Mock, Lambda, and AWS service (SNS) integrations.</p> <p>You can read about the differences between REST and HTTP APIs here: https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-vs-rest.html</p> <p>For this demo we will use REST, because it allows the use of Mock integrations.</p> <p>I will be creating this in the ap-southeast-2 region, so all links to the console will be there. Make sure you change region if you\u2019re deploying elsewhere.</p>"},{"location":"aws-api-gateway/#instructions","title":"Instructions","text":""},{"location":"aws-api-gateway/#stage-1-setting-up-sns","title":"Stage 1 - Setting up SNS","text":"<p>Head to the SNS console: https://ap-southeast-2.console.aws.amazon.com/sns/v3/home?region=ap-southeast-2#/topics</p> <p>Click on Create topic</p> <p>Set the Type to \u201cStandard\u201d</p> <p>Set the Name to be \u201cAPI-Messages\u201d</p> <p>Under Access policy, leave the Method as \u201cBasic\u201d</p> <p>Change Define who can publish messages to the topic to \u201cOnly the specified AWS accounts\u201d and enter your account ID (found in the top right of the console)</p> <p>Change Define who can subscribe to this topic to \u201cOnly the specified AWS accounts\u201d and enter your account ID again</p> <p>In the real world, this should be locked down further to only the resources you want publishing to the topic, but in this temporary example set up, locking down to just the account is fine and safe enough</p> <p>Leave all other options as default</p> <p>Click on Create topic</p> <p>On the next page, click on Create subscription</p> <p>Change the Protocol to \u201cEmail\u201d</p> <p>In the Endpoint field, enter your personal email</p> <p>Click Create subscription</p> <p>You will receive a confirmation email shortly after, with a link you need to click on. This tells SNS that you\u2019re happy to receive emails from the topic, and prevents spam from being sent via SNS.</p> <p>Side note: While writing this, my confirmation went to Spam in Gmail, so don\u2019t forget to check there.</p> <p>Your subscription should now be in the Confirmed state:</p> <p></p>"},{"location":"aws-api-gateway/#stage-2-create-the-lambda","title":"Stage 2 - Create the Lambda","text":"<p>Head to the Lambda console: https://ap-southeast-2.console.aws.amazon.com/lambda/home?region=ap-southeast-2#/functions</p> <p>Click Create function</p> <p>Leave Author from scratch selected</p> <p>Set the Function name to <code>api-return-ip</code></p> <p>Set the Runtime to \u201cPython 3.9\u201d</p> <p>Leave the Architecture as \u201cx86_64\u201d</p> <p>Click Create function</p> <p>In the Code tab, enter the following code:</p> <pre><code>def lambda_handler(event, context):   \n    return {\n        'statusCode': 200,\n        'headers': {},\n        'body': event['requestContext']['identity']['sourceIp'],\n        'isBase64Encoded': False\n        }\n</code></pre> <p>This is an extremely basic function that just returns the source IP of the requester (you).</p> <p>Don\u2019t forget to click Deploy to save the function.</p> <p></p>"},{"location":"aws-api-gateway/#stage-3-create-the-api","title":"Stage 3 - Create the API","text":"<p>Head to the API Gateway console: https://ap-southeast-2.console.aws.amazon.com/apigateway/main/apis?region=ap-southeast-2</p> <p>Click Create API</p> <p>Select REST API \u2192 Build</p> <p>Make sure you don\u2019t select \u201cREST API Private\u201d.</p> <p></p> <p>Leave the Protocol and \u201cCreate new API\u201d options as is, and set your API name to whatever you like</p> <p></p> <p>Click Create API</p> <p>Once that\u2019s done you will see all of the \u201cResources\u201d (endpoints / API paths), right now we have none, so click on Actions and then \u201cCreate Resource\u201d</p> <p></p> <p>This first resource we create will be for the Mock integration, so we\u2019ll just name it \u201cMock\u201d. The Resource Path is the URL path you will use to call it, so in this case it would be something like</p> <p><code>https://abcdef1234.execute-api.ap-southeast-2.amazonaws.com/mock</code></p> <p></p> <p>Next we have to attach a Method. A Method is the HTTP method that the resource (path) will accept, such as \u201cGET\u201d, \u201cPOST\u201d, \u201cDELETE\u201d, etc.</p> <p>For the Mock integration, we will just use \u201cGET\u201d. </p> <p>Make sure you are in the /mock resource</p> <p></p> <p>Then click Actions then \u201cCreate Method\u201d</p> <p></p> <p>Select \u201cGET\u201d</p> <p></p> <p>Then click the tick to accept.</p> <p>Once that\u2019s done, API Gateway will present a list of possible integrations. For this one, select \u201cMock\u201d then click Save</p> <p></p> <p>Once that\u2019s done, click on \u201cIntegration Response\u201d</p> <p></p> <p>This is where we tell API Gateway what the Mock integration should respond with. </p> <p>Expand the 200 status line, then the Mapping Templates section, and set the Content-Type to <code>application/json</code></p> <p></p> <p>Click on the tick, then in the template section, enter the following (you can replace the message with whatever you like)</p> <pre><code>{\n    \"statusCode\": 200,\n    \"message\": \"This response is mocking you\"\n}\n</code></pre> <p>Then click Save (it won\u2019t give any feedback that it\u2019s saved, but it has)</p> <p></p> <p>Then click Save on the method response</p> <p></p> <p>That\u2019s all done. Now we\u2019ll set up the Lambda integration.</p> <p>Go back to the root (<code>/</code>)resource</p> <p></p> <p>Then click Actions then Create Resource</p> <p>For this one, set the resource name to \u201cLambda\u201d, and leave the Resource Path as \u201c/lambda\u201d</p> <p>Click Create Resource</p> <p>Click on Actions then Create Method. This will also be a \u201cGET\u201d</p> <p></p> <p>On the next page, set the \u201cIntegration type\u201d to \u201cLambda function\u201d</p> <p>Enable \u201cUse Lambda Proxy integration\u201d</p> <p>Once you click the \u201cLambda Function\u201d text field, it should drop down with a list of Lambda\u2019s in that region, select the one you created earlier</p> <p>Leave all other options as is, and click Save</p> <p></p> <p>You should see a popup telling you that you\u2019re about to give API Gateway permission to invoke your Lambda, click OK</p> <p>Lastly we\u2019ll set up another resource for SNS.</p> <p>For this one, we will need to set up an IAM role that API Gateway will use to publish messages to SNS. </p> <p>Head to the IAM Console: https://us-east-1.console.aws.amazon.com/iamv2/home?region=ap-southeast-2#/roles</p> <p>Go to the Roles page, and click Create Role</p> <p></p> <p>Under \u201cTrusted entity\u201d, select \u201cAWS Service\u201d, and in the drop-down, select API Gateway. Make sure you select the radio button for \u201cAPI Gateway\u201d as well.</p> <p></p> <p>Click Next</p> <p>On the Permissions page, click Next</p> <p>Lastly, set the role name to \u201capi-gw-sns-role\u201d then click Create role</p> <p>Now go into the role you just created</p> <p></p> <p>Click on Add permissions then Create inline policy</p> <p></p> <p>Go to the JSON tab and enter the following</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"sns:Publish\",\n            \"Resource\": \"*\"\n        }\n    ]\n}\n</code></pre> <p>Click Review policy</p> <p>Under \u201cReview policy\u201d set the name to \u201cSnsPublish\u201d and click Create policy</p> <p>On the summary page, copy the ARN of the role you just created, you will need it for the next step</p> <p></p> <p>Now head back to the API Gateway console: https://ap-southeast-2.console.aws.amazon.com/apigateway/main/apis?region=ap-southeast-2</p> <p>Go back into your REST API</p> <p>Go back to the root resource</p> <p></p> <p>Then click Actions then Create Resource</p> <p>For this one, set the resource name to \u201cSNS\u201d, and leave the Resource Path as \u201c/sns\u201d</p> <p>Click Create Resource</p> <p>Click on Actions then Create Method. </p> <p>This one will be a \u201cPOST\u201d.</p> <p>On the next page, set the \u201cIntegration type\u201d to \u201cAWS Service\u201d</p> <p>Set the AWS Region as the same region as your API / SNS (for me this is ap-southeast-2)</p> <p>Set the AWS Service as \u201cSimple Notification Service (SNS)\u201d</p> <p>Leave the AWS Subdomain blank</p> <p>Set the HTTP method to POST</p> <p>Leave the Action Type as \u201cUse action name\u201d</p> <p>Set the \u201cAction\u201d to \u201cPublish\u201d</p> <p>Under Execution Role you need to put the ARN of the IAM role you just created for SNS.  </p> <p>Leave the rest of the form as is, and click Save</p> <p></p> <p>If you\u2019re wondering where we got the \u201cPublish\u201d action, and where to find other Action types, you can view them all in the API Reference for the service: https://docs.aws.amazon.com/sns/latest/api/API_Operations.html</p> <p>There\u2019s a few ways to pass your message through API Gateway to the AWS service, but to make things easy for this example, we\u2019re going to use Query Strings. Query Strings are found in the URL of websites after a question mark, e.g. <code>https://google.com/search?q=hello</code>, where \u201cq=hello\u201d is the query string.</p> <p>Go to the \u201c/sns\u201d resource, and the \u201cPOST\u201d method, and click \u201cMethod Request\u201d </p> <p></p> <p>Under \u201cURL Query String Parameters\u201d click \u201cAdd query string\u201d and enter \u201cTopicArn\u201d and click the tick.</p> <p>Then click \u201cAdd query string\u201d and enter \u201cMessage\u201d and click the tick.</p> <p>Your query string should look like this</p> <p></p> <p>Go back to the \u201c/sns\u201d resource, and the \u201cPOST\u201d method, and click \u201cIntegration Request\u201d</p> <p></p> <p>Under \u201cURL Query String Parameters\u201d click \u201cAdd query string\u201d </p> <p>Set the Name to \u201cMessage\u201d and the Mapped from to <code>method.request.querystring.Message</code></p> <p>Click the tick to save</p> <p>Click \u201cAdd query string\u201d again</p> <p>Set the Name to \u201cTopicArn\u201d and the Mapped from to <code>method.request.querystring.TopicArn</code></p> <p>Click the tick to save</p> <p>Now we need to Deploy the API. </p> <p>Click on Actions, then Deploy API</p> <p></p> <p>In the pop up window, set the \u201cDeployment stage\u201d to \u201c[New Stage]\u201d, and the \u201cStage name\u201d to \u201cv1\u201d.</p> <p></p> <p>The \u201cStage name\u201d can really be anything, and is used to direct API requests to different \u201cversions\u201d of the API. So you could have a \u201cdev\u201d stage, and \u201cprod\u201d stage, or just use the standard \u201cv1\u201d, \u201cv2\u201d, etc.</p> <p>Click Deploy</p> <p>Once that\u2019s done, you will be sent to the Stage Editor page, where you can set things like Rate Limiting, WAF associations, caching, logs, etc. We don\u2019t need to change any of these for this demo.</p> <p>At the top of the screen you will see your API URL</p> <p></p> <p>Copy that URL for the next step.</p>"},{"location":"aws-api-gateway/#stage-4-testing-the-api","title":"Stage 4 - Testing the API","text":"<p>The Lambda and Mock resources can be tested in the browser. By default, any URL you enter into your browser performs a GET request (and remember, only our Lambda and Mock resources have the GET method set).</p>"},{"location":"aws-api-gateway/#mock","title":"Mock","text":"<p>If we visit our API URL and append \u201c/mock\u201d we should see the response we entered earlier</p> <p></p> <p>The reason we want a JSON output rather than a friendly human readable one, is because working with JSON in programming languages makes things much easier. Your code or application could read the \u201cstatusCode\u201d key and see a 200 value, and then it could read the \u201cmessage\u201d key and see it\u2019s value.</p>"},{"location":"aws-api-gateway/#lambda","title":"Lambda","text":"<p>Now if we visit our \u201c/lambda\u201d URL we should see our function response (your IP address)</p> <p></p>"},{"location":"aws-api-gateway/#sns","title":"SNS","text":"<p>For the last endpoint, we can\u2019t use a browser because browsers by default only perform \u201cGET\u201d requests, so we have a couple of options to test this endpoint.</p>"},{"location":"aws-api-gateway/#command-line","title":"Command Line","text":"<p>If you\u2019re comfortable using the command line (Linux, Mac, or WSL for Windows), follow these steps. If you would prefer to use a GUI, skip ahead.</p> <p>In your CLI, run the following command (replace \u201cREGION\u201d and \u201cACCOUNT-ID\u201d with your region and account ID), and don\u2019t forget to replace the URL with your API gateway URL</p> <pre><code>curl -X POST -G -d 'TopicArn=arn:aws:sns:REGION:ACCOUNT-ID:API-Messages' -d 'Message=Hello!'  https://abc123def.execute-api.ap-southeast-2.amazonaws.com/v1/sns\n</code></pre> <p>Note if you want to use spaces in your message, because query parameters are URL encoded, spaces need to be replaced with a +, so for example:</p> <pre><code>curl -X POST -G -d 'TopicArn=arn:aws:sns:REGION:ACCOUNT-ID:API-Messages' -d 'Message=I+like+sending+long+messages'  https://abc123def.execute-api.ap-southeast-2.amazonaws.com/v1/sns\n</code></pre>"},{"location":"aws-api-gateway/#gui-aws-console","title":"GUI (AWS Console)","text":"<p>The API Gateway console provides a handy way to test your API. </p> <p></p> <p>On the testing page, under \u201cQuery Strings\u201d, enter the following. Replace the TopicArn with your SNS Topic ARN, and the message with whatever you like.</p> <pre><code>TopicArn=arn:aws:sns:REGION:ACCOUNT-ID:API-Messages&amp;Message=APIs+are+fun\n</code></pre> <p>Note if you want to use spaces in your message, because query parameters are URL encoded, spaces need to be replaced with a +</p> <p>Scroll down and click Test</p> <p></p>"},{"location":"aws-api-gateway/#gui-postman","title":"GUI (Postman)","text":"<p>We can also use a popular API testing tool called Postman. It\u2019s available for Windows, Mac, and Linux</p> <p>It can be downloaded for free from their website: https://www.postman.com/downloads/</p> <p>I won\u2019t write instructions on installing the program, it\u2019s fairly self explanatory and isn\u2019t related to this demo.</p> <p>Once you\u2019ve opened Postman, click on the + up the top of the application to open a new tab. Set the method to POST, and enter your API Gateway URL.</p> <p>You can then enter the two Query Parameters we set earlier:</p> <p><code>TopicArn</code> which is the ARN of your SNS topic.</p> <p><code>Message</code> which is the message you want to send to the topic</p> <p></p> <p>Once you\u2019ve entered all that, hit Send</p> <p>No matter which method you chose, you should receive an email from SNS containing the message in your Query String</p> <p></p>"},{"location":"aws-api-gateway/#stage-5-cleaning-up","title":"Stage 5 - Cleaning up","text":"<p>Head to the API Gateway console: https://ap-southeast-2.console.aws.amazon.com/apigateway/main/apis?region=ap-southeast-2</p> <p>Select the API you created, and click Actions then Delete</p> <p></p> <p>Head to the SNS console: https://ap-southeast-2.console.aws.amazon.com/sns/v3/home?region=ap-southeast-2#/topics</p> <p>Go to Topics, select the Topic you created earlier, and click Delete</p> <p></p> <p>In the confirmation box, enter \u201cdelete me\u201d and click Delete</p> <p>Go to Subscriptions, select the Subscription you created for your email, and click Delete</p> <p></p> <p>Head to the IAM console: https://us-east-1.console.aws.amazon.com/iamv2/home?region=ap-southeast-2#/roles</p> <p>Under Roles, search for \"api-gw-sns-role\u201d</p> <p>Select the role we created earlier, and click Delete</p> <p></p> <p>Type \u201capi-gw-sns-role\u201d into the confirmation field, and click Delete</p> <p>Head to the Lambda console: https://ap-southeast-2.console.aws.amazon.com/lambda/home?region=ap-southeast-2#/functions</p> <p>Select the function we created earlier, and click Actions then Delete</p> <p></p> <p>Enter \u201cdelete\u201d in the confirmation window, and then click Delete</p> <p>Head to the Cloudwatch Logs console: https://ap-southeast-2.console.aws.amazon.com/cloudwatch/home?region=ap-southeast-2#logsV2:log-groups</p> <p>Search for the \"api-return-ip\u201d Log Group, select the log group , click Actions then Delete</p> <p></p> <p>In the confirmation popup, click Delete</p>"},{"location":"aws-cloudtrail-log-file-integrity/","title":"CloudTrail with Log File Integrity","text":""},{"location":"aws-cloudtrail-log-file-integrity/#overview","title":"Overview","text":"<p>We\u2019re going to be creating a CloudTrail Trail, logging to an S3 bucket, with Log file validation enabled.</p> <p>We\u2019re then going to see how our AWS API calls appear in the log files, we\u2019re going to check the checksum / hash of the log files, look at the log file hashes in the digest files, and then we will try modifying a log file and a digest file, to see how it breaks validity.</p> <p>We will be creating this environment in the ap-southeast-4 (Melbourne) region, so all links to the console will be there. Make sure you change region if you\u2019re deploying elsewhere.</p>"},{"location":"aws-cloudtrail-log-file-integrity/#instructions","title":"Instructions","text":""},{"location":"aws-cloudtrail-log-file-integrity/#stage-1-create-an-s3-bucket","title":"Stage 1 - Create an S3 bucket","text":"<p>Head to the S3 console: https://s3.console.aws.amazon.com/s3/buckets</p> <p>Go to Buckets and click on Create bucket</p> <p>Under \u201cBucket Name\u201d enter anything you like, for this demo I will use <code>demo-cloudtrail-logs</code></p> <p>Set the AWS Region to the region you\u2019re setting up CloudTrail in, in my case, <code>ap-southeast-4</code></p> <p>Leave everything else as default and click Create bucket</p>"},{"location":"aws-cloudtrail-log-file-integrity/#stage-2-create-a-cloudtrail-trail","title":"Stage 2 - Create a CloudTrail Trail","text":"<p>Head to the CloudTrail console: https://ap-southeast-4.console.aws.amazon.com/cloudtrail</p> <p>Go to Trails and click Create trail</p> <p></p> <p>Note: You may already have a trail created if you have set up Control Tower in the past, you can ignore that, it\u2019s fine to have multiple trails configured.</p> <p>On the next page, set the trail name to <code>demo</code></p> <p>Select \u201cUse existing S3 bucket\u201d, and enter (or browse to) the name of your newly created bucket</p> <p>Under \u201cCustomer managed AWS KMS key\u201d, leave \u201cNew\u201d selected, and change the \u201cAWS KMS alias\u201d to <code>demo-key</code></p> <p>Ensure \u201cLog file validation\u201d is selected</p> <p>Leave all other settings as default and click Next</p> <p></p> <p>On the next page, leave all the settings as default. For this demo we only care about management events (AWS API calls), not data events (S3 reads and writes for example)</p> <p>Click Next</p> <p>Click Create trail</p>"},{"location":"aws-cloudtrail-log-file-integrity/#stage-3-generating-events","title":"Stage 3 - Generating events","text":"<p>Head to the EC2 console: https://ap-southeast-4.console.aws.amazon.com/ec2/home</p> <p>Go to AMIs</p> <p></p> <p>Head to the Lambda console: https://ap-southeast-4.console.aws.amazon.com/lambda/home</p> <p>Go to Functions</p> <p></p> <p>Head to the RDS console: https://ap-southeast-4.console.aws.amazon.com/rds/home\u2019</p> <p>Go to Databases</p> <p></p> <p>All these actions don\u2019t look like they\u2019re doing anything, but each one is generating multiple AWS API calls, such as <code>DescribeImages</code>, <code>ListFunctions</code>, <code>DescribeDBInstances</code>, plus dozens of others just by loading the page.</p> <p>Obviously feel free to perform any other actions you like, such as creating a Lambda function, launching an EC2 instance, etc, however for this demo we won\u2019t be doing any other actions, so you will need to clean these resources up yourself when you\u2019re done.</p> <p>Once you\u2019re done, wait one hour and then proceed to the next step. CloudTrail log digests are created each hour, so we\u2019ll wait for that to complete before proceeding.</p> <p>Note down roughly what time you performed these actions, so we know which log file to view in the next step.</p>"},{"location":"aws-cloudtrail-log-file-integrity/#stage-4-viewing-the-logs","title":"Stage 4 - Viewing the logs","text":"<p>Head to the S3 console: https://s3.console.aws.amazon.com/s3/buckets</p> <p>Click on Buckets and go into your CloudTrail bucket you created earlier</p> <p></p> <p>Go into the <code>AWSLogs/</code> directory, then into your account number, then <code>CloudTrail/</code> then the region you are in, then the year, month, day:</p> <p></p> <p>You will see CloudTrail creates a log file every ~5 minutes. I\u2019m going to view the log file from 14:11 as that\u2019s when I was performing random actions in the previous stage.</p> <p>Select the log file, and click Download</p>"},{"location":"aws-cloudtrail-log-file-integrity/#viewing-json-files","title":"Viewing JSON files","text":"<p>JSON files are relatively difficult to read as a human, so while it is possible, I\u2019d recommend using a tool to format the JSON nicely.</p> <p>Command Line</p> <p>Using the <code>jq</code> tool, you can <code>cat</code> the downloaded log file and pipe it into <code>jq</code> like this:</p> <pre><code>cat 1232456789012_CloudTrail_ap-southeast-4_20230315T0315Z_LOi9LL5VQOn9UZ6k.json | jq\n</code></pre> <p>Which will output:</p> <pre><code>{\n  \"Records\": [\n    {\n      \"eventVersion\": \"1.08\",\n      \"userIdentity\": {\n        \"type\": \"IAMUser\",\n        \"principalId\": \"AAAAAIIIIIIDDDDDZZZZ\",\n        \"arn\": \"arn:aws:iam::123456789012:user/username\",\n        \"accountId\": \"123456789012\",\n        \"accessKeyId\": \"AAAAAIIIIIIDDDDDZZZZ\",\n        \"userName\": \"username\",\n        \"sessionContext\": {\n          \"sessionIssuer\": {},\n          \"webIdFederationData\": {},\n          \"attributes\": {\n            \"creationDate\": \"2023-03-15T00:10:12Z\",\n            \"mfaAuthenticated\": \"true\"\n          }\n        }\n      },\n      \"eventTime\": \"2023-03-15T03:06:00Z\",\n      \"eventSource\": \"rds.amazonaws.com\",\n      \"eventName\": \"DescribeOrderableDBInstanceOptions\",\n&lt;truncated&gt;\n</code></pre> <p>Web Browser</p> <p>The following website can be used to format JSON. </p> <p>Remember, always be careful with what sensitive data / output you\u2019re entering into a third party website.</p> <p>https://jsonformatter.curiousconcept.com/#</p> <p>VSCode</p> <p>If you use VSCode, you can install the Prettier extension, which will format and colour the JSON file.</p> <p></p> <p>Browser Extensions</p> <p>I haven\u2019t tested these personally, but there\u2019s numerous browser extensions for JSON formatting</p> <p>https://chrome.google.com/webstore/detail/json-formatter/bcjindcccaagfpapjjmafapmmgkkhgoa?hl=en</p> <p>https://addons.mozilla.org/en-US/firefox/addon/basic-json-formatter/</p> <p>https://microsoftedge.microsoft.com/addons/detail/json-formatter-for-edge/njpoigijhgbionbfdbaopheedbpdoddi</p> <p>Once you\u2019ve found a suitable method for reading JSON files, if you take a look through the CloudTrail log, each event is an API call made by a user, so for example:</p> <pre><code>{\n            \"eventVersion\": \"1.08\",\n            \"userIdentity\": {\n                \"type\": \"IAMUser\",\n                \"principalId\": \"AAAAAIIIIIIDDDDDZZZZ\",\n                \"arn\": \"arn:aws:iam::123456789012:user/jeff.bezos\",\n                \"accountId\": \"123456789012\",\n                \"accessKeyId\": \"AAAAAIIIIIIDDDDDZZZZ\",\n                \"userName\": \"jeff.bezos\",\n                \"sessionContext\": {\n                    \"sessionIssuer\": {},\n                    \"webIdFederationData\": {},\n                    \"attributes\": {\n                        \"creationDate\": \"2023-03-15T00:10:12Z\",\n                        \"mfaAuthenticated\": \"true\"\n                    }\n                }\n            },\n            \"eventTime\": \"2023-03-15T03:08:29Z\",\n            \"eventSource\": \"lambda.amazonaws.com\",\n            \"eventName\": \"ListFunctions20150331\",\n            \"awsRegion\": \"ap-southeast-4\",\n            \"sourceIPAddress\": \"123.123.123.123\",\n            \"userAgent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36\",\n            \"requestParameters\": null,\n            \"responseElements\": null,\n            \"requestID\": \"1bde6a85-035b-49a1-9d71-9c5abcdc7c8e\",\n            \"eventID\": \"277e8de0-6410-4202-af17-c3eecbacbf0d\",\n            \"readOnly\": true,\n            \"eventType\": \"AwsApiCall\",\n            \"managementEvent\": true,\n            \"recipientAccountId\": \"123456789012\",\n            \"eventCategory\": \"Management\",\n            \"tlsDetails\": {\n                \"tlsVersion\": \"TLSv1.2\",\n                \"cipherSuite\": \"ECDHE-RSA-AES128-GCM-SHA256\",\n                \"clientProvidedHostHeader\": \"lambda.ap-southeast-4.amazonaws.com\"\n            },\n            \"sessionCredentialFromConsole\": \"true\"\n        },\n</code></pre> <p>So in this event you can see the user <code>jeff.bezos</code> called the <code>ListFunctions20150331</code> API call, from IP address <code>123.123.123.123</code>. </p> <p>You can also see this is a \u201cReadOnly\u201d event, meaning no changes were made to any resources. This field is useful to filter for only CloudTrail actions that made a change to something in the account.</p>"},{"location":"aws-cloudtrail-log-file-integrity/#stage-4-viewing-the-digest-file","title":"Stage 4 - Viewing the digest file","text":"<p>Head to the S3 console: https://s3.console.aws.amazon.com/s3/buckets</p> <p>Click on Buckets and go into your CloudTrail bucket you created earlier</p> <p></p> <p>Go into the <code>AWSLogs/</code> directory, then into your account number, then <code>CloudTrail-Digest/</code> then the region you are in, then the year, month, day.</p> <p>Select the digest file that was created at the end of the hour, so for example the log file I was viewing was last modified at 14:16 so the digest file I\u2019m going to view is the one that was created at 15:09</p> <p></p> <p>Just like you did for the CloudTrail log file, download the file and open it in whatever tool you\u2019re using to view JSON files</p> <p>The CloudTrail log file I\u2019m viewing is</p> <p><code>1234567789012_CloudTrail_ap-southeast-4_20230315T0320Z_1iwuEXInvYIpo1Zq.json.gz</code></p> <p>So if I search for that in the digest file I can see the time stamp of the first and last event, and the hash value</p> <pre><code>{\n            \"s3Bucket\": \"demo-cloudtrail-logs\",\n            \"s3Object\": \"AWSLogs/1234567789012/CloudTrail/ap-southeast-4/2023/03/15/1234567789012_CloudTrail_ap-southeast-4_20230315T0320Z_1iwuEXInvYIpo1Zq.json.gz\",\n            \"hashValue\": \"3007368a401b56dab0576c7cc9c3acb4ff41d423c7d403242d15d3f4f49f21d2\",\n            \"hashAlgorithm\": \"SHA-256\",\n            \"newestEventTime\": \"2023-03-15T03:15:14Z\",\n            \"oldestEventTime\": \"2023-03-15T03:11:08Z\"\n        },\n</code></pre> <p>At the beginning of the digest file we can also see a lot of meta data about the digest file itself:</p> <pre><code>{\n    \"awsAccountId\": \"1234567789012\",\n    \"digestStartTime\": \"2023-03-15T03:00:48Z\",\n    \"digestEndTime\": \"2023-03-15T04:00:48Z\",\n    \"digestS3Bucket\": \"demo-cloudtrail-logs\",\n    \"digestS3Object\": \"AWSLogs/1234567789012/CloudTrail-Digest/ap-southeast-4/2023/03/15/1234567789012_CloudTrail-Digest_ap-southeast-4_demo_ap-southeast-4_20230315T040048Z.json.gz\",\n    \"digestPublicKeyFingerprint\": \"ad89cdd3a01ca82b01f9ba802987d3cf\",\n    \"digestSignatureAlgorithm\": \"SHA256withRSA\",\n    \"newestEventTime\": \"2023-03-15T03:55:44Z\",\n    \"oldestEventTime\": \"2023-03-15T03:00:48Z\",\n    \"previousDigestS3Bucket\": \"demo-cloudtrail-logs\",\n    \"previousDigestS3Object\": \"AWSLogs/1234567789012/CloudTrail-Digest/ap-southeast-4/2023/03/15/1234567789012_CloudTrail-Digest_ap-southeast-4_demo_ap-southeast-4_20230315T030048Z.json.gz\",\n    \"previousDigestHashValue\": \"23d7d7a2e7721ca4e69dcbf1317ccda319ee164072b1fd2aba80e172b5f1b8f9\",\n    \"previousDigestHashAlgorithm\": \"SHA-256\",\n    \"previousDigestSignature\": \"809df1e29ddf8b1389e2164352fa7a95e76d8af8d85e5e5209811c70ed174f623e059fed01daf20ee6b02b6bac43f8b1c54a370bb1f20d84f53c1c1b4f4b5cd6004587a6bf2827aa762393c2f9be9374c1b6886b3e301eb34d35fc9ba813bb2f0579245db89c8feab7184c1955cb9bb88b50b14ac0f095131b1640c735d3023246e3973dbd2d57cf096d1d189527eecdb46eac7b88b874c29237a63082d9044f7d0c9eb89e4685b63906319ee9f68145ab67a3796652a7042f98d2ab1243b6bab811b28c02a2b2b36d9e12145a963973452fdbd96508c67d7b269e441a48aaf9634e117d76eed3e69eac6677c9555008efebcd41619c95ac1e93b9ee93083c38\",\n    \"logFiles\": [\n        {\n&lt;truncated&gt;\n</code></pre> <p>First we\u2019ll look at the hash value of the CloudTrail log we were viewing earlier, in the digest file it says that log file <code>AWSLogs/1234567789012/CloudTrail/ap-southeast-4/2023/03/15/1234567789012_CloudTrail_ap-southeast-4_20230315T0320Z_1iwuEXInvYIpo1Zq.json.gz</code> has the hash <code>3007368a401b56dab0576c7cc9c3acb4ff41d423c7d403242d15d3f4f49f21d2</code></p> <p>Now to calculate the hash of that file, there\u2019s a few ways to do it depending on your OS.</p> <p>Linux</p> <p>Most Linux distributions will have <code>sha256sum</code> installed, which is a command line tool to calculate the SHA256 checksum of a file</p> <pre><code>sha256sum 1234567789012_CloudTrail_ap-southeast-4_20230315T0320Z_1iwuEXInvYIpo1Zq.json\n3007368a401b56dab0576c7cc9c3acb4ff41d423c7d403242d15d3f4f49f21d2  1234567789012_CloudTrail_ap-southeast-4_20230315T0320Z_1iwuEXInvYIpo1Zq.json\n</code></pre> <p>MacOS</p> <p>MacOS should have the <code>shasum</code> tool, which accepts the argument <code>-a 256</code> telling it to calculate the SHA256 checksum</p> <pre><code>\u276f shasum -a 256 1234567789012_CloudTrail_ap-southeast-4_20230315T0320Z_1iwuEXInvYIpo1Zq.json\n3007368a401b56dab0576c7cc9c3acb4ff41d423c7d403242d15d3f4f49f21d2  1234567789012_CloudTrail_ap-southeast-4_20230315T0320Z_1iwuEXInvYIpo1Zq.json\n</code></pre> <p>Windows</p> <p>Windows has a built in tool that can be called from Command Prompt or PowerShell called <code>certutil</code></p> <pre><code>PS C:\\Users\\hello\\Downloads&gt; certutil.exe -hashfile .\\1234567789012_CloudTrail_ap-southeast-4_20230315T0320Z_1iwuEXInvYIpo1Zq.json SHA256\nSHA256 hash of .\\1234567789012_CloudTrail_ap-southeast-4_20230315T0320Z_1iwuEXInvYIpo1Zq.json:\n3007368a401b56dab0576c7cc9c3acb4ff41d423c7d403242d15d3f4f49f21d2\n</code></pre> <p>So in all three cases the checksum was the same, <code>3007368a401b56dab0576c7cc9c3acb4ff41d423c7d403242d15d3f4f49f21d2</code> which matches what is in the digest file, so we can be certain this log file hasn\u2019t been modified.</p> <p>Let\u2019s see what happens if we modified anything in the CloudTrail log, let\u2019s throw Bill Gates under the bus and say he called the <code>ListFunctions20150331</code> API</p> <pre><code>{\n            \"eventVersion\": \"1.08\",\n            \"userIdentity\": {\n                \"type\": \"IAMUser\",\n                \"principalId\": \"AAAAAIIIIIIDDDDDZZZZ\",\n                \"arn\": \"arn:aws:iam::123456789012:user/bill.gates\",\n                \"accountId\": \"123456789012\",\n                \"accessKeyId\": \"AAAAAIIIIIIDDDDDZZZZ\",\n                \"userName\": \"bill.gates\",\n                \"sessionContext\": {\n                    \"sessionIssuer\": {},\n                    \"webIdFederationData\": {},\n                    \"attributes\": {\n                        \"creationDate\": \"2023-03-15T00:10:12Z\",\n                        \"mfaAuthenticated\": \"true\"\n                    }\n                }\n            },\n            \"eventTime\": \"2023-03-15T03:08:29Z\",\n            \"eventSource\": \"lambda.amazonaws.com\",\n            \"eventName\": \"ListFunctions20150331\",\n&lt;truncated&gt;\n</code></pre> <p>Now if we check the SHA256 sum:</p> <pre><code>\u276f sha256sum 123456789012_CloudTrail_ap-southeast-4_20230315T0320Z_1iwuEXInvYIpo1Zq.json\n33f36af460f6e7cd28b28b50bb15f730b2339d920474b46ccf4e74f8fa6cb5f4  123456789012_CloudTrail_ap-southeast-4_20230315T0320Z_1iwuEXInvYIpo1Zq.json\n</code></pre> <p>The checksum is completely different.</p>"},{"location":"aws-cloudtrail-log-file-integrity/#stage-5-verifying-the-log-and-digest-files","title":"Stage 5 - Verifying the log and digest files","text":"<p>For this stage you will need the AWS CLI, you can either download it locally here, or use the AWS CloudShell. Don\u2019t forget to specify the region that your CloudTrail trail exists in.</p> <p>The AWS CLI has the functionality to iterate through your log files between specified times, validate their checksum, as well as validate the checksums of the digest files. If we were to do this manually, it would be quite a bit harder: https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-custom-validation.html#cloudtrail-log-file-custom-validation-sample-code</p> <p>The AWS CLI command you need to run is the following. Don\u2019t forget to update the region, trail ARN and start / end times in UTC time):</p> <pre><code>aws --region ap-southeast-4 cloudtrail validate-logs --trail-arn arn:aws:cloudtrail:ap-southeast-4:123456789012:trail/demo --start-time '2023-03-15T05:00Z' --end-time '2023-03-15T06:00Z' --verbose\n</code></pre> <p>The output for my log files, for the above command is:</p> <pre><code>\u276f aws --region ap-southeast-4 cloudtrail validate-logs --trail-arn arn:aws:cloudtrail:ap-southeast-4:123456789012:trail/demo --start-time '2023-03-15T05:00Z' --end-time '2023-03-15T06:00Z' --verbose\nValidating log files for trail arn:aws:cloudtrail:ap-southeast-4:123456789012:trail/demo between 2023-03-15T05:00:00Z and 2023-03-15T06:00:00Z\n\nDigest file     s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail-Digest/ap-southeast-4/2023/03/15/123456789012_CloudTrail-Digest_ap-southeast-4_demo_ap-southeast-4_20230315T060048Z.json.gz   valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0510Z_V0Ys6ocepSSYl8OL.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0520Z_lsOn2GUFH6ldeVTX.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0555Z_tOJccVTgE22nohRt.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0540Z_ucmpBygJirk3zxwF.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0550Z_5KSaiSOWXTYD4Hhu.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0535Z_pgdRLTJq3DOGhcL4.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0515Z_tKDGrvgNkbNEPuLE.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0600Z_5ZJPMA0cMbI6VN9T.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0525Z_HTPauvC2uXY9mEQQ.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0530Z_VI2VONm9Sk73V6pJ.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0510Z_vBNgDIuTquQXeGxV.json.gz      valid\nDigest file     s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail-Digest/ap-southeast-4/2023/03/15/123456789012_CloudTrail-Digest_ap-southeast-4_demo_ap-southeast-4_20230315T050048Z.json.gz   valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0435Z_QyTPaZWQlrb7ToQA.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0420Z_V9P9WffzoudHMW11.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0430Z_bwQdRtWImSHgYq5q.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0445Z_pMuQDAL39jTMoWh7.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0500Z_yM3zq8sMxeebSgXY.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0450Z_9WAm73sQcXRxqBs7.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0440Z_bWCTGu8Eb0hgpPGk.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0405Z_uWSLxq8RzwJDZc2p.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0455Z_y4rozjRSNIJ5baZw.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0425Z_Sv7lYiFgRZ0dXt1y.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0415Z_UqPzkRCNevxWTMxC.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0410Z_Xyd38WrYN0F0DpJo.json.gz      valid\n\nResults requested for 2023-03-15T05:00:00Z to 2023-03-15T06:00:00Z\nResults found for 2023-03-15T05:00:48Z to 2023-03-15T06:00:00Z:\n\n2/2 digest files valid\n23/23 log files valid\n</code></pre> <p>So this is telling us that each log file and digest file created between 5AM and 6AM UTC was valid and unmodified. </p> <p>Let\u2019s modify a log file and see what happens when we check the validity. </p> <p>Head to the S3 console: https://s3.console.aws.amazon.com/s3/buckets</p> <p>Click on Buckets and go into your CloudTrail bucket you created earlier</p> <p></p> <p>Go into the <code>AWSLogs/</code> directory, then into your account number directory, then <code>CloudTrail/</code> then the region you are in, then the year, month, day.</p> <p>Choose a log file that was created between the range you\u2019re validating, so for me that\u2019s 5AM to 6AM UTC, or 3PM AEDT to 4PM AEDT</p> <p></p> <p>I\u2019m going to modify one of the events in the log file and change the username from my user, to <code>jeff.bezos</code>, but you can change anything in this log file for this step.</p> <p>Save the file, and before we upload it, we need to Gzip it. Gzip is installed on Linux and MacOS by default, but for Windows you will need to use a tool like 7Zip.</p> <p>On Linux or MacOS, you can run:</p> <pre><code>gzip &lt;filename&gt;\n</code></pre> <p>On Windows, using 7-Zip, right click on the file and go to 7-Zip, then \u201cAdd to archive\u2026\u201d</p> <p></p> <p>Change the \u201cArchive format\u201d to \u201cgzip\u201d and click OK</p> <p></p> <p>Now go back to the S3 console and click Upload</p> <p></p> <p>Upload the CloudTrail log you just modified, and click Upload</p> <p>Make sure you select the file ending in .gz</p> <p></p> <p>Now if we run the <code>validate-logs</code> command again, we get an error saying the log file is invalid</p> <pre><code>\u276f aws --region ap-southeast-4 cloudtrail validate-logs --trail-arn arn:aws:cloudtrail:ap-southeast-4:123456789012:trail/demo --start-time '2023-03-15T05:00Z' --end-time '2023-03-15T06:00Z' --verbose\nValidating log files for trail arn:aws:cloudtrail:ap-southeast-4:123456789012:trail/demo between 2023-03-15T05:00:00Z and 2023-03-15T06:00:00Z\n\nDigest file     s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail-Digest/ap-southeast-4/2023/03/15/123456789012_CloudTrail-Digest_ap-southeast-4_demo_ap-southeast-4_20230315T060048Z.json.gz   valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0510Z_V0Ys6ocepSSYl8OL.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0520Z_lsOn2GUFH6ldeVTX.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0555Z_tOJccVTgE22nohRt.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0540Z_ucmpBygJirk3zxwF.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0550Z_5KSaiSOWXTYD4Hhu.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0535Z_pgdRLTJq3DOGhcL4.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0515Z_tKDGrvgNkbNEPuLE.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0600Z_5ZJPMA0cMbI6VN9T.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0525Z_HTPauvC2uXY9mEQQ.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0530Z_VI2VONm9Sk73V6pJ.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0510Z_vBNgDIuTquQXeGxV.json.gz      valid\nDigest file     s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail-Digest/ap-southeast-4/2023/03/15/123456789012_CloudTrail-Digest_ap-southeast-4_demo_ap-southeast-4_20230315T050048Z.json.gz   valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0435Z_QyTPaZWQlrb7ToQA.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0420Z_V9P9WffzoudHMW11.json.gz      valid\n\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0430Z_bwQdRtWImSHgYq5q.json.gz      INVALID: hash value doesn't match\n\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0445Z_pMuQDAL39jTMoWh7.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0500Z_yM3zq8sMxeebSgXY.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0450Z_9WAm73sQcXRxqBs7.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0440Z_bWCTGu8Eb0hgpPGk.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0405Z_uWSLxq8RzwJDZc2p.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0455Z_y4rozjRSNIJ5baZw.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0425Z_Sv7lYiFgRZ0dXt1y.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0415Z_UqPzkRCNevxWTMxC.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0410Z_Xyd38WrYN0F0DpJo.json.gz      valid\n\nResults requested for 2023-03-15T05:00:00Z to 2023-03-15T06:00:00Z\nResults found for 2023-03-15T05:00:48Z to 2023-03-15T06:00:00Z:\n\n2/2 digest files valid\n22/23 log files valid, 1/23 log files INVALID\n</code></pre> <p>The same can be done for the digest files. The log file we modified is contained in the 16:09 digest file (05:00 UTC), so let\u2019s download that digest file, modify it, gzip it, and re-upload it. I won\u2019t go over the steps again, they\u2019re mostly identical to what we just did.</p> <p>However, in the digest file, I\u2019m going to change the checksum of the log file I modified, to the new sneaky checksum:</p> <pre><code>\u276f sha256sum 123456789012_CloudTrail_ap-southeast-4_20230315T0430Z_bwQdRtWImSHgYq5q.json.gz\nb3d30b38373363ae0f16601db7f0f598a09e96100f13ead994d887233452b611  123456789012_CloudTrail_ap-southeast-4_20230315T0430Z_bwQdRtWImSHgYq5q.json.gz\n</code></pre> <p>This is where I\u2019m updating the hash</p> <p></p> <p>Now I\u2019ve gzip\u2019d it, re-uploaded it, and re-ran the <code>validate-logs</code> command:</p> <pre><code>\u276f aws --region ap-southeast-4 cloudtrail validate-logs --trail-arn arn:aws:cloudtrail:ap-southeast-4:123456789012:trail/demo --start-time '2023-03-15T05:00Z' --end-time '2023-03-15T06:00Z' --verbose\nValidating log files for trail arn:aws:cloudtrail:ap-southeast-4:123456789012:trail/demo between 2023-03-15T05:00:00Z and 2023-03-15T06:00:00Z\n\nDigest file     s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail-Digest/ap-southeast-4/2023/03/15/123456789012_CloudTrail-Digest_ap-southeast-4_demo_ap-southeast-4_20230315T060048Z.json.gz   valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0510Z_V0Ys6ocepSSYl8OL.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0520Z_lsOn2GUFH6ldeVTX.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0555Z_tOJccVTgE22nohRt.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0540Z_ucmpBygJirk3zxwF.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0550Z_5KSaiSOWXTYD4Hhu.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0535Z_pgdRLTJq3DOGhcL4.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0515Z_tKDGrvgNkbNEPuLE.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0600Z_5ZJPMA0cMbI6VN9T.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0525Z_HTPauvC2uXY9mEQQ.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0530Z_VI2VONm9Sk73V6pJ.json.gz      valid\nLog file        s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail/ap-southeast-4/2023/03/15/123456789012_CloudTrail_ap-southeast-4_20230315T0510Z_vBNgDIuTquQXeGxV.json.gz      valid\n\nDigest file     s3://demo-cloudtrail-logs/AWSLogs/123456789012/CloudTrail-Digest/ap-southeast-4/2023/03/15/123456789012_CloudTrail-Digest_ap-southeast-4_demo_ap-southeast-4_20230315T050048Z.json.gz   INVALID: signature verification failed\n\nResults requested for 2023-03-15T05:00:00Z to 2023-03-15T06:00:00Z\nResults found for 2023-03-15T05:00:48Z to 2023-03-15T06:00:00Z:\n\n1/2 digest files valid, 1/2 digest files INVALID\n11/11 log files valid\n</code></pre> <p>Now I\u2019ve broken the digest validity, and the chain of trust that each digest file uses.</p>"},{"location":"aws-cloudtrail-log-file-integrity/#stage-6-clean-up","title":"Stage 6 - Clean up","text":"<p>Head to the CloudTrail console: https://ap-southeast-4.console.aws.amazon.com/cloudtrail</p> <p>Go to Trails and select the <code>demo</code> trail you created, and click Delete</p> <p></p> <p>In the confirmation box, click Delete</p> <p>Head to the S3 console: https://s3.console.aws.amazon.com/s3/buckets</p> <p>Note: CloudTrail may add a few files to your bucket for a few minutes after your Trail has been deleted, so if you empty your bucket, and then go to delete it and it says there\u2019s still objects in there, this will be why. Just wait a few minutes and try again.</p> <p>Select your CloudTrail trail bucket, and click Empty</p> <p></p> <p>Enter \u201cpermanently delete\u201d in the confirmation window, and click Empty</p> <p>Then, select your CloudTrail trail bucket, and click Delete</p> <p></p> <p>Enter the bucket name in the confirmation window, and click Delete</p>"},{"location":"aws-cognito-web-identity-federation/","title":"Advanced Demo - Web Identity Federation","text":"<p>In this advanced demo series you will be implementing a simple serverless application which uses Web Identity Federation. The application runs using the following technologies</p> <ul> <li>S3 for front-end application hosting</li> <li>Google API Project as an ID Provider</li> <li>Cognito and IAM Roles to swap Google Token for AWS credentials</li> </ul> <p>The application runs from a browser, gets the user to login using a Google ID and then loads all images from a private S3 bucket into a browser using presignedURLs.</p> <p>This advanced demo consists of 6 stages :-</p> <ul> <li>STAGE 1 : Provision the environment and review tasks </li> <li>STAGE 2 : Create Google API Project &amp; Client ID</li> <li>STAGE 3 : Create Cognito Identity Pool</li> <li>STAGE 4 : Update App Bucket &amp; Test Application</li> <li>STAGE 5 : Cleanup the account</li> </ul> <p></p>"},{"location":"aws-cognito-web-identity-federation/#instructions","title":"Instructions","text":"<ul> <li>Stage1</li> <li>Stage2</li> <li>Stage3</li> <li>Stage4</li> <li>Stage5</li> </ul>"},{"location":"aws-cognito-web-identity-federation/#1-click-installs","title":"1-Click Installs","text":"<p>Make sure you are logged into AWS and in <code>us-east-1</code> </p> <ul> <li>WEBIDF</li> </ul>"},{"location":"aws-cognito-web-identity-federation/#video-guides","title":"Video Guides","text":"<ul> <li>STAGE1 - SETUP</li> <li>STAGE2 - Google API project</li> <li>STAGE3 - Cognito Identity Pool</li> <li>STAGE4 - Configure and Test Application</li> <li>STAGE5 - Cleanup</li> </ul>"},{"location":"aws-cognito-web-identity-federation/#architecture-diagrams","title":"Architecture Diagrams","text":"<ul> <li>Overall - PNG</li> <li> <p>Overall - PDF</p> </li> <li> <p>Stage1 - PNG</p> </li> <li>Stage1 - PDF</li> <li>Stage2 - PNG</li> <li>Stage2 - PDF</li> <li>Stage3 - PNG</li> <li>Stage3 - PDF</li> <li>Stage4 - PNG</li> <li>Stage4 - PDF</li> </ul>"},{"location":"aws-continuous-delivery/","title":"Continuous Delivery on AWS","text":"<p>Continous 'Delivery' on AWS.</p>"},{"location":"aws-continuous-delivery/#project-overview","title":"\ud83d\ude80 Project Overview","text":"<p>The project involves a continuous integration and deployment (CI/CD) pipeline for software development on AWS. Here's an overview of the process:</p> <ul> <li> <p>Code Commit: Developers regularly commit code changes, which trigger the CI/CD pipeline.</p> </li> <li> <p>CodeBuild Trigger: Commits trigger AWS CodeBuild, which initiates the build process.</p> </li> <li> <p>Code Analysis: CodeBuild performs code analysis using Sonarcloud. SonarCloud (same as SonarQube) is a cloud-based code quality and security service provided by SonarSource. It integrates with Git repositories to analyze code for bugs, vulnerabilities, code smells, and other issues.</p> </li> <li> <p>Dependency Management: Dependencies required for the project are downloaded from AWS CodeArtifact, ensuring consistency and reliability.</p> </li> <li> <p>Artifact Building: CodeBuild utilizes Maven to build the artifact from the source code.</p> </li> <li> <p>Artifact Storage: The built artifact is stored in an S3 bucket for future deployment.</p> </li> <li> <p>Deployment: AWS Deploy deploys the artifact to an Elastic Beanstalk environment, ensuring scalability and ease of management.</p> </li> <li> <p>Database Connection: The Beanstalk environment is connected to an RDS (Relational Database Service) instance for data storage and retrieval.</p> </li> <li> <p>Software Testing: After deployment, software testing is executed using AWS CodeBuild services to verify the functionality and stability of the deployed application.</p> </li> </ul> <p>This CI/CD pipeline automates the software development process, from code commits to deployment, ensuring efficient and reliable delivery of software updates.</p>"},{"location":"aws-continuous-delivery/#problem-statement","title":"\ud83d\udd27 Problem Statement","text":"<p>In the context of software development on AWS, there exists a need to establish a robust and efficient continuous integration and deployment (CI/CD) pipeline. The current manual processes for code commits, code analysis, artifact building, and deployment are prone to errors, inconsistencies, and inefficiencies. To address these challenges, the objective is to design and implement an automated CI/CD pipeline that seamlessly integrates with AWS services. This pipeline should enable developers to commit code changes confidently, knowing that they will undergo thorough code analysis, dependency management, artifact building, and deployment processes. Additionally, the pipeline should facilitate seamless database connection, software testing, and scalability of deployed applications. The ultimate goal is to streamline the software development lifecycle, ensuring consistent, reliable, and timely delivery of software updates while minimizing manual intervention and maximizing resource utilization.</p>"},{"location":"aws-continuous-delivery/#techonology-stack","title":"\ud83d\udcbd Techonology Stack","text":"<p>\u25cf Application Integration: Simple Notification Service (SNS)</p> <p>\u25cf Management &amp; Governance: CloudWatch.</p> <p>\u25cf Security, Identity &amp; Compliance: Secret Manager, SonarCloud(SonarQube)</p> <p>\u25cf CI/CD: Automate deployment using AWS Code Pipeline, AWS CodeBuild, AWS CodeCommit, AWS CodeArtifact</p>"},{"location":"aws-continuous-delivery/#architecture-diagram","title":"\ud83d\udccc Architecture Diagram","text":""},{"location":"aws-continuous-delivery/#getting-started","title":"\ud83d\udea6 Getting Started","text":""},{"location":"aws-continuous-delivery/#prerequisites","title":"Prerequisites","text":"<p>Before you get started, make sure you have the following prerequisites in place:</p> <ul> <li>AWS account.</li> <li>AWS CLI.</li> <li>SonarCloud account.</li> <li>Git for cloning the repository.</li> </ul>"},{"location":"aws-continuous-delivery/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ul> <li>Step-1: Setup AWS CodeCommit</li> <li>Step-2: Setup AWS CodeArtifact</li> <li>Step-3: Setup SonarCloud</li> <li>Step-4: Store SonarCloud variables in System Manager Parameter Store</li> <li>Step-5: AWS CodeBuild for SonarQube Code Analysis</li> <li>Step-6: AWS CodeBuild for Build Artifact</li> <li>Step-7: AWS CodePipeline and Notification with SNS</li> <li>Step-8: Validate CodePipeline</li> <li>Step-9: Create Elastic Beanstalk environment</li> <li>Step-10: Create RDS MySQL Database</li> <li>Step-11: Update RDS Security Group</li> <li>Step-12: Use Beanstalk instance to connect RDS to deploy schemas</li> <li>Step-13: Update Code with pom &amp; setting.xml</li> <li>Step-14: Build Job Setup</li> <li>Step-15: Create Pipeline</li> <li>Step-16: SNS Notification</li> <li>Step-17: Validate &amp; Test</li> </ul>"},{"location":"aws-continuous-delivery/#step-1-setup-aws-codecommit","title":"\u2728 Step-1-Setup-AWS-CodeCommit","text":"<ul> <li>Go to AWS Console, and pick us-east-1 region then go to CodeCommit service. Create repository.</li> </ul> <p><code>bash    Name: vprofile-code-repo</code></p> <ul> <li>Next, create an IAM user with CodeCommit access from IAM console. We will create a policy for CodeCommit and allow full access only for vprofile-code-repo.</li> </ul> <p><code>bash    Name: vprofile-code-admin-repo-fullaccess</code></p> <p></p> <ul> <li>To be able connect our repo, follow steps given in CodeCommit.</li> </ul> <p></p> <ul> <li>Create SSH key in local server and add public key to IAM role Security credentials.</li> </ul> <p></p> <ul> <li>Update configuration under .ssh/config and add our Host information. And change permissions with chmod 600 config</li> </ul> <p><code>bash    Host git-codecommit.us-east-1.amazonaws.com        User &lt;SSH_Key_ID_from IAM_user&gt;        IdentityFile ~/.ssh/vpro-codecommit_rsa</code></p> <ul> <li>We can test our ssh connection to CodeCommit.</li> </ul> <p><code>bash    ssh git-codecommit.us-east-1.amazonaws.com</code></p> <p></p> <ul> <li> <p>Next, clone the repository to a location of your choice in your local server.</p> </li> <li> <p>Convert the Github repository for vprofile-project in your local server, to your CodeCommit repository. In Github repo directory.</p> </li> <li> <p>Run the command below.</p> </li> </ul> <p><code>bash     git checkout master     git branch -a | grep -v HEAD | cur -d'/' -f3 | grep -v master &gt; /tmp/branches     for i in `cat  /tmp/branches`; do git checkout $i; done     git fetch --tags     git remote rm origin     git remote add origin ssh://git-codecommit.us-east-1.amazonaws.com/v1/repos/vprofile-code-repo     cat .git/config     git push origin --all     git push --tags</code></p> <ul> <li>Our repo is ready on CodeCommit with all branches.</li> </ul> <p></p>"},{"location":"aws-continuous-delivery/#step-2-setup-aws-codeartifact","title":"\ud83c\udf1f Step-2-Setup-AWS-CodeArtifact","text":"<ul> <li>Create CodeArtifact repository for Maven:</li> </ul> <p><code>bash     Name: vprofile-maven-repo     Public upstraem Repo: maven-central-store     This AWS account     Domain name: visualpath</code> </p> <ul> <li>Follow connection instructions given in CodeArtifact for maven-central-repo.</li> </ul> <p></p> <ul> <li>Create an IAM user for CodeArtifact and configure aws CLI with its credentials. We will give Programmatic access to this user to enable use of aws cli and download credentials file.</li> </ul> <p><code>bash     aws configure # provide iam user credentials</code> </p> <p></p> <ul> <li>Run command get token as in the instructions.</li> </ul> <p><code>bash     export CODEARTIFACT_AUTH_TOKEN=`aws codeartifact get-authorization-token --domain visualpath --domain-owner 392530415763 --region us-east-1 --query authorizationToken --output text`</code> - Update pom.xml and setting.xml file with correct urls as suggested in instruction then push files to codeCommit.</p> <p><code>bash     git add .     git commit -m \"message\"     git push origin ci-aws</code> </p>"},{"location":"aws-continuous-delivery/#step-3-setup-sonarcloud","title":"\ud83d\ude80 Step-3-Setup-SonarCloud","text":"<ul> <li> <p>Create an Account with SonarCloud. https://sonarcloud.io/ </p> </li> <li> <p>From account avatar -&gt; My Account -&gt; Security. Generate token name as vprofile-sonartoken. Note the token.</p> </li> </ul> <p></p> <ul> <li>Next we create a project, + -&gt; Analyze Project -&gt; create project manually. Below details will be used in our Build.</li> </ul> <p><code>bash     Organization: kubeirving-projects     Project key: vprofile-repo8</code> </p> <ul> <li>Sonar Cloud is ready!</li> </ul> <p></p>"},{"location":"aws-continuous-delivery/#step-4-store-sonar-in-ssm-parameter-store","title":"\ud83d\udcbd Step-4-Store-Sonar-in-SSM-Parameter-Store","text":"<ul> <li>Create parameters with the variables below.</li> </ul> <p><code>bash     CODEARTIFACT_TOKEN     SecureString      HOST                   https://sonarcloud.io     ORGANIZATION           kubeirving-projects     PROJECT                vprofile-repo8     SONARTOKEN             SecureString</code> </p>"},{"location":"aws-continuous-delivery/#step-5-codebuild-for-sonarqube","title":"\ud83d\udd27 Step-5-CodeBuild-for-SonarQube","text":"<ul> <li>From AWS Console, go to CodeBuild -&gt; Create Build Project. This step is similar to Jenkins Job.</li> </ul> <p><code>bash     ProjectName: Vprofile-Build     Source: CodeCommit     Branch: ci-aws     Environment: Ubuntu     runtime: standard:5.0     New service role     Insert build commands from foler aws-files/sonar_buildspec.yml     Logs-&gt; GroupName: vprofile-buildlogs     StreamName: sonarbuildjob</code> </p> <ul> <li>Update sonar_buildspec.yml file parameter store sections with the exact names we have given in SSM Parameter store.</li> </ul> <p></p> <ul> <li>Add a policy to the service role created for this Build project -&gt; find name of role from Environment, go to IAM add policy as below:</li> </ul> <p></p> <ul> <li>Build your project.</li> </ul> <p> </p> <ul> <li>Check from SonarCloud too.</li> </ul> <p></p>"},{"location":"aws-continuous-delivery/#step-6-codebuild-for-build-artifact","title":"\ud83d\ude80 Step-6-CodeBuild-for-Build-Artifact","text":"<ul> <li>From AWS Console, go to CodeBuild -&gt; Create Build Project. This step is similar to Jenkins Job.</li> </ul> <p><code>bash     ProjectName: Vprofile-Build-Artifact     Source: CodeCommit     Branch: ci-aws     Environment: Ubuntu     runtime: standard:5.0     Use existing role from previous build     Insert build commands from foler aws-files/build_buildspec.yml     Logs-&gt; GroupName: vprofile-buildlogs     StreamName: artifactbuildjob</code></p> <ul> <li>It\u2019s time to build project.</li> </ul> <p></p>"},{"location":"aws-continuous-delivery/#step-7-codepipeline-and-notification-with-sns","title":"\ud83d\udcbc Step-7-CodePipeline-and-Notification-with-SNS","text":"<ul> <li>First we will create an SNS topic from SNS service and subscribe to topic with email.</li> </ul> <ul> <li>Confirm your subscription from your email.</li> </ul> <ul> <li>Next, create an S3 bucket to store our deploy artifacts.</li> </ul> <ul> <li>Create CodePipeline.</li> </ul> <p><code>bash     Name: vprofile-CI-Pipeline     SourceProvider: Codecommit     branch: ci-aws     Change detection options: CloudWatch events     Build Provider: CodeBuild     ProjectName: vprofile-Build-Aetifact     BuildType: single build     Deploy provider: Amazon S3     Bucket name: vprofile98-build-artifact     object name: pipeline-artifact</code></p> <ul> <li>Add Test and Deploy stages to your pipeline.</li> <li>Last step before running the pipeline is to setup Notifications.</li> <li>Go to Settings in CodePipeline -&gt; Notifications.</li> <li>Time to run our CodePipeline.</li> </ul> <p></p>"},{"location":"aws-continuous-delivery/#step-8-validate-codepipeline","title":"\u2705 Step-8: Validate CodePipeline","text":"<ul> <li>Make some changes in README file in your source code, once this change is pushed, CloudWatch will detect the changes and a notification event will trigger Pipeline.</li> </ul>"},{"location":"aws-continuous-delivery/#step-9-create-elastic-beanstalk-environment","title":"\ud83c\udf31 Step-9-Create-Elastic-Beanstalk-environment","text":"<p>Create an environment using Sample application.</p> <p><code>bash     Name: vprofile-app     Capacity: LoadBalanced         Min: 2         Max: 4     Security: Choose existing key-pair usedin previous steps     Tags:          Name:Project         Value: vprofile</code></p>"},{"location":"aws-continuous-delivery/#step-10-create-rds-mysql-database","title":"\ud83d\uddc4\ufe0f Step-10-Create-RDS-MySQL-Database","text":"<ul> <li>Create an RDS service with the details below.</li> <li>Don\u2019t forget the click View credential details to note down your password.</li> </ul> <p><code>bash     Engine: MySQL     version: 5.7     Free-Tier     DB Identifier: vprofile-cicd-mysql     credentials: admin     Auto generate password (will take note of pwd once RDS is created)     db.t2.micro     Create new SecGrp:      * Name: vprofile-cicd-rds-mysql-sg     Additional Configurations:      * initial db name: accounts</code></p>"},{"location":"aws-continuous-delivery/#step-11-update-rds-security-group","title":"\ud83d\udd12 Step-11-Update-RDS-Security-Group","text":"<p>Go to instances, find BeanStalk instance and copy its Secgrp ID. Update RDS SecGrp Inbound rules to allow access for Beanstalk instances on port 3306.</p>"},{"location":"aws-continuous-delivery/#step-12-beanstalk-instance-to-connect-rds-to-deploy-schemas","title":"\ud83d\uddc4\ufe0f Step-12-Beanstalk-instance-to-connect-RDS-to-deploy-schemas","text":"<ul> <li> <p>Although, SSH into your beanstalk instance to make changes is not a good practice as its better to create a new EC2 and perform these tasks, but for this project, we will make an exception.</p> </li> <li> <p>Go to Beanstalk SecGrp group, and change access to port 22 from Anywhere to MyIP. Install mysql client in this instance to be able to connect RDS. We also need to install git since we will clone our source code and get scripts to create schema in our database.</p> </li> </ul> <p><code>bash     sudo -i     yum install mysql git -y     mysql -h &lt;RDS_endpoint&gt; -u &lt;RDS_username&gt; -p&lt;RDS_password&gt;     show databases;     git clone https://github.com/rumeysakdogan/vprofileproject-all.git     cd vprofileproject-all/     git checkout cd-aws     cd src/main/resources     mysql -h &lt;RDS_endpoint&gt; -u &lt;RDS_username&gt; -p&lt;RDS_password&gt; accounts &lt; db_backup.sql     mysql -h &lt;RDS_endpoint&gt; -u &lt;RDS_username&gt; -p&lt;RDS_password&gt;     use accounts;     show tables;</code></p> <p>Now go back to Beanstalk environment and under Configuration -&gt; load balancer -&gt; Processes , update Health check path to /login. Then apply changes.</p>"},{"location":"aws-continuous-delivery/#step-13-update-code-with-pom-settingxml","title":"\ud83d\udcbb Step-13-Update-Code-with-pom-setting.xml","text":"<ul> <li> <p>Go to CodeCommit, select cd-aws branch. We will do the same updates that we did in ci-aws branch to the pom &amp; settings.xml files. We can directory select file and Edit in CodeCommit, then commit our changes.</p> </li> <li> <p>For pom.xml, add the correct url from your code artifact connection steps:</p> </li> </ul> <p><code>bash     &lt;repository&gt;             &lt;id&gt;codeartifact&lt;/id&gt;             &lt;name&gt;codeartifact&lt;/name&gt;         &lt;url&gt;https://visualpath-392530415763.d.codeartifact.us-east-1.amazonaws.com/maven/maven-central-store/&lt;/url&gt;         &lt;/repository&gt;</code> for settings.xml, update below parts with correct url from code artifact.</p> <p><code>bash     &lt;profiles&gt;     &lt;profile&gt;         &lt;id&gt;default&lt;/id&gt;         &lt;repositories&gt;         &lt;repository&gt;             &lt;id&gt;codeartifact&lt;/id&gt;         &lt;url&gt;https://visualpath-392530415763.d.codeartifact.us-east-1.amazonaws.com/maven/maven-central-store/&lt;/url&gt;         &lt;/repository&gt;         &lt;/repositories&gt;     &lt;/profile&gt;     &lt;/profiles&gt;     &lt;activeProfiles&gt;             &lt;activeProfile&gt;default&lt;/activeProfile&gt;         &lt;/activeProfiles&gt;     &lt;mirrors&gt;     &lt;mirror&gt;         &lt;id&gt;codeartifact&lt;/id&gt;         &lt;name&gt;visualpath-maven-central-store&lt;/name&gt;         &lt;url&gt;https://visualpath-392530415763.d.codeartifact.us-east-1.amazonaws.com/maven/maven-central-store/&lt;/url&gt;         &lt;mirrorOf&gt;*&lt;/mirrorOf&gt;     &lt;/mirror&gt;     &lt;/mirrors&gt;</code></p>"},{"location":"aws-continuous-delivery/#step-14-build-job-setup","title":"\ud83d\udd27 Step-14-Build-Job-Setup","text":"<ul> <li>Go to CodeBuild and change Source for Vprofile-Build &amp; Vprofile-build-Artifact projects. Currently these projects are triggered from ci-aws branch, we will change branch to cd-aws.</li> </ul> <p>Create \u201cBuildAndRelease\u201d Build Project</p> <p>Then we create a new project called Build Project for deploying artifacts to BeanStalk.</p> <p><code>bash     Name: Vprofile-BuildAndRelease     Repo: CodeCommit     branch: cd-aws     Environment     *Managed image: Ubuntu     *Standard     Image 5.0     We will use existing role from previous Build project which has access to SSM Parameter Store     Insert build commands:      * From source code we will get spec file under `aws-files/buildAndRelease_buildspec.yml`.     Logs:     *LogGroup:vprofile-cicd-logs     *StreamnameBuildAndReleaseJob</code></p> <p>We need to create 3 new parameters as used in BuilAndRelease_buildspec.yml file in SSM Parameter store. we have noted these values from RDS creation step, we will use them now.</p> <p><code>bash     RDS-Endpoint: String     RDSUSER: String     RDSPASS: SecureString</code></p> <p>Let\u2019s run the project to know if successful!</p> <p></p> <p>Create \u201cSoftwareTesting\u201d Build Project</p> <p>In this Build Project, we will run our Selenium Automation scripts and store the artifacts in S3 bucket.</p> <p>First, we will create an S3 bucket.</p> <p><code>bash     Name: vprofile-cicd-testoutput-rd (give a unique name)     Region: it should be the same region we create our pipeline</code></p> <p>Next, create a new Build project for Selenium Automation Tests. Create a new Build project with details below:</p> <p><code>bash     Name: SoftwareTesting     Repo: CodeCommit     branch: seleniumAutoScripts     Environment:     * Windows Server 2019     * Runtime: Base     * Image: 1.0     We will use existing role from previous Build project which has access to SSM Parameter Store     Insert build commands:      * From source code we will get spec file under `aws-files/win_buildspec.yml`.     * We need to update url part to our Elastic Beanstalk URL.     Artifacts:     *Type: S3     * Bucketname: vprofile-cicd-testoutput-rd     * Enable semantic versioning     Artifcats packaging: zip     Logs:     *LogGroup: vprofile-cicd-logs     *Streamname:</code></p>"},{"location":"aws-continuous-delivery/#step-15-create-pipeline","title":"\u26d3\ufe0f Step-15-Create-Pipeline","text":"<p>Create CodePipeline with name vprofile-cicd-pipeline</p> <p>```bash     Source:     * CodeCommit     * vprofile-code-repo     * cd-aws     * Amazon CloudWatch Events</p> <pre><code>Build\n* BuildProvider: CodeBuild\n* ProjectName: Vprofile-BuildAndRelease\n* Single Build\n\nDeploy\n* Deploy provider: Beanstalk\n* application: vprofile-app\n* Environment: vprofile-app-env\n</code></pre> <p>```</p> <p><code>bash     We will Disable transitions and Edit pipeline to add more stages.</code></p> <p>Add this stage in our codepipeline after \u201cSource\u201d:</p> <p><code>bash     Name: CodeAnalysis     Action provider: CodeBuild     Input artifacts: SourceArtifact     Project name: Vprofile-Build</code></p> <p>Add second stage after CodeAnalysis:</p> <p><code>bash     Name: BuildAndStore     Action provider: CodeBuild     Input artifacts: SourceArtifact     Project name: Vprofile-Build-artifact     OutputArtifact: BuildArtifact</code></p> <p>Add third stage after BuildAndStore:</p> <p><code>bash     Name: DeployToS3     Action provider: Amazon S3     Input artifacts: BuildArtifact     Bucket name: vprofile98-build-artifact     Extract file before deploy</code></p> <p>Edit the ouput artifacts of Build and Deploy stages. Go to Build stage Edit Stage. Change Output artifact name as BuildArtifactToBean.</p> <p>Go to Deploy stage, Edit stage. We change InputArtifact to BuildArtifactToBean.</p> <p>Last Stage will be added after Deploy stage:</p> <p><code>bash     Name: Software Testing     Action provider: CodeBuild     Input artifacts: SourceArtifact     ProjectName: SoftwareTesting</code></p> <p>Save and Release change. This will start our CodePipeline.</p>"},{"location":"aws-continuous-delivery/#step-16-sns-notification","title":"\ud83d\udd14 Step-16-SNS-Notification","text":"<p>Select your pipeline. Click Notify, then Manage Notification. We will create a new notification.</p> <p><code>bash     vprofile-aws-cicd-pipeline-notification     Select all     Notification Topic: use same topic from CI pipeline</code></p>"},{"location":"aws-continuous-delivery/#step-17-validatetest","title":"\ud83e\uddea Step-17-Validate&amp;Test","text":"<p>Time to test our pipeline.</p> <p></p> <p>We can check the app from browser with Beanstalk endpoint to view result.</p>"},{"location":"aws-continuous-delivery/#license","title":"\ud83d\udcc4 License","text":"<p>This project is licensed under the MIT License.</p>"},{"location":"aws-control-tower/","title":"Control Tower","text":""},{"location":"aws-control-tower/#overview","title":"Overview","text":"<p>We\u2019re going to be setting up Control Tower in our AWS Organisation.</p> <p>You will be setting this up from your \u201cmanagement\u201d account. In a production environment, your management account would contain nothing but your Organization, Control Tower, Users (ideally SSO), and Billing. All resources, buckets, logs, etc should be in sub-accounts.</p> <p>For this demo you will need four to five different email accounts. If you use Gmail or Google Workspace, you can use \u201cplus addresses\u201d, which means if your email address is <code>jeffbezos@gmail.com</code>, you can use <code>jeffbezos+aws@gmail.com</code>, or <code>jeffbezos+sandbox@gmail.com</code>, or any other variation, and they will all route to your primary email. </p> <p>Alternatively you can use a service like 10 Minute Mail (https://10minutemail.com/) to get a free temporary email address. </p> <p>As you create each account, you should login to the root user for that account, reset the password, and note it down. You will need the root user password for the last step when you close the accounts.</p> <p>Another important point to note, at the end of this demo we are going to be closing AWS accounts. AWS documentation says that when an account is closed, the root user email address cannot be reused, so make sure you don\u2019t use an email address that you will want to use with AWS in the future.</p> <p>https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_close.html#:~:text=The root user's email address can't be reused if you close an account</p> <p>We will be creating this environment in the ap-southeast-2 region, so all links to the console will be there. Make sure you change region if you\u2019re deploying elsewhere.</p>"},{"location":"aws-control-tower/#instructions","title":"Instructions","text":""},{"location":"aws-control-tower/#stage-1-create-an-organization-and-sub-account","title":"Stage 1 - Create an Organization and sub-account","text":"<p>In your primary (management) account, head to the Organizations console: https://us-east-1.console.aws.amazon.com/organizations/v2/home?region=us-east-1#</p> <p>Click on Create an organization</p> <p>Now we\u2019ll add a sub-account to our organization, click on Add an AWS account</p> <p></p> <p>Leave \u201cCreate an AWS account\u201d selected.</p> <p>Enter your account name, we\u2019ll use \u201cSandbox\u201d.</p> <p>Enter your root user email address you would like to use for this new account. Note, this email must not be used with an AWS account already.</p> <p>Remember: If you use Gmail or Google Workspace, you can use \u201cplus addresses\u201d. </p> <p>Leave IAM role name as default.</p> <p>You should now have two accounts in your Organization</p> <p></p> <p>If you want to login to your new Sandbox account, you can head to the AWS Console login page and reset your password for your new root user. This is optional, the rest of these steps can be done without logging into your new account as root.</p> <p>You now have an AWS Organization with a management account and a sub-account. In the real world you might have dozens of accounts for different projects, environments (dev, testing, prod, etc), or for different teams. You can then organize these accounts into OUs (Organizational Units) based on team, project, or whatever you like.</p> <p>You can also set up SCPs (Service Control Policies) to enforce things such as mandatory backups, restricted regions, disabled services, etc. We will be doing this via Control Tower later in the guide.</p>"},{"location":"aws-control-tower/#stage-2-set-up-control-tower","title":"Stage 2 - Set up Control Tower","text":"<p>Head to the Control Tower console: https://ap-southeast-2.console.aws.amazon.com/controltower/home/landing?region=ap-southeast-2</p> <p>Click Set up landing zone</p> <p>Set your Home Region to your preferred region. This is where things like Cloudformation Stack Sets, logs, buckets, etc will be provisioned by Control Tower by default. This doesn\u2019t affect where you can provision resources. I\u2019m going to use \u201cAsia Pacific (Sydney)\u201d.</p> <p>Leave Region deny setting set to \u201cNot enabled\u201d, we don\u2019t want to restrict any regions right now.</p> <p>Click Next</p> <p>Control Tower sets up a new OU in your organization; \u201cSecurity\u201d used for log archives and security audits. You can change this OU name if you like, but we\u2019ll leave it as default.</p> <p>There is also a config box to set up a new OU called \u201cSandbox\u201d, which you would put your sandbox, dev, testing, etc accounts.</p> <p>Leave both of these as default and click Next</p> <p>On the next page, you will be prompted to create two new accounts, one for Log archiving, and one for Auditing (CloudTrail trail buckets, etc).</p> <p>You need to create these two accounts, so you will need two more email addresses. </p> <p>Note: You can use your Sandbox account you created earlier for one of these, but not both (they need to be separate accounts). You also can\u2019t use the management account for either of these. So at the bare minimum, you need one new account. AWS obviously recommends creating two new accounts and not sharing logs or audit with any resources.</p> <p>I\u2019m going to create two new accounts with plus addresses, so we will have four accounts total.</p> <p>On the next page, Control Tower will ask you if you wish to enable organization level CloudTrail, which enables CloudTrail for all of your accounts and aggregates the storage to your Audit account created on the previous page. Leave this set to Enabled.</p> <p>Leave the rest of the page as default and click Next</p> <p>Check the confirmation box, and then click Set up landing zone</p> <p></p> <p>Setting up Control Tower will take a while (in my experience, 20-30 minutes).</p>"},{"location":"aws-control-tower/#stage-3-creating-and-switching-to-an-iam-user","title":"Stage 3 - Creating and switching to an IAM user","text":"<p>For the next steps you cannot use the root account, so if you\u2019re logged in as root, we\u2019re going to create a new user in the IAM Identity Center (previously \u201cSSO\u201d), assign the necessary permissions, and add this user to our sub-account(s).</p> <p>Head to the IAM Identity Center console: https://ap-southeast-2.console.aws.amazon.com/singlesignon/home?region=ap-southeast-2#!/instances/users</p> <p>On the Users page, click on Add user</p> <p></p> <p>Set whatever you like for the Username, I\u2019ll use <code>jeffbezos</code></p> <p>Under Password, select \u201cGenerate a one-time password that you can share with this user\u201d</p> <p>Set the two Email addresses to (you guessed it) a new, unused email address.</p> <p>Fill out the First name and Last name with any values you like.</p> <p></p> <p>Click Next</p> <p>Add this user to all the groups listed. In production this should obviously be locked down to only the required groups, but for this demo, these permissions are fine.</p> <p></p> <p>Click Next</p> <p>Click Add user</p> <p>Note down the AWS access portal URL, your username, and your generated password.</p> <p>You will need this Access Portal URL multiple times throughout this guide.</p> <p></p> <p>Now we need to give your user access to your sub-account, so using Identity Center you can login to that account (which is required for the next step).</p> <p>Head to AWS Accounts, select your sub-account (not your management account), and click Assign users or groups</p> <p></p> <p>Switch to the Users tab, select your newly created user, and click Next</p> <p></p> <p>Select all Permission Sets, and click Next</p> <p></p> <p>Click Submit</p> <p>Now, head to that access portal URL, and login with your newly created account. You will be prompted to change your password.</p> <p>Once logged in, you will see a list of all your accounts that your new user has been added to. Click on your sub-account, then Management Console next to the role <code>AWSAdministratorAccess</code></p> <p></p> <p>Once you\u2019ve logged into your sub-account, head to the IAM console: https://us-east-1.console.aws.amazon.com/iamv2/home?region=ap-southeast-2#/roles</p> <p>Go to Roles and click Create Role</p> <p>Under Trusted entity type change this \u201cAWS account\u201d. Click on \u201cAnother AWS account\u201d, and paste the account ID of the management account. You can get this from the Identity Center page you were on previously.</p> <p></p> <p>Click Next</p> <p>Search for and attach the policy \u201cAdministratorAccess\u201d</p> <p></p> <p>Click Next</p> <p>Set the Role name to <code>AWSControlTowerExecution</code>. </p> <p>Click Create role</p> <p>This is the role that Control Tower will use when making changes in sub-accounts. When you create an account via Control Tower, it will automatically create this role using Account Factory, but adding an existing account requires this step to be done manually.</p> <p>So far this sub-account is in the AWS Organization, but is not managed by Control Tower. We\u2019ll do that next.</p>"},{"location":"aws-control-tower/#stage-4-enrolling-accounts-to-control-tower","title":"Stage 4 - Enrolling accounts to Control Tower","text":"<p>Log back into the management account using the Identity Center portal from earlier.</p> <p></p> <p>Head to the Control Tower console, and go to Organization: https://ap-southeast-2.console.aws.amazon.com/controltower/home/organization?region=ap-southeast-2</p> <p>You will see only your management, log, and audit accounts are enrolled in the Control Tower organization.</p> <p></p> <p>Click on your Sandbox account (or whichever account you created earlier), and click Actions \u2192 Enroll</p> <p></p> <p>Select any OU (there should only be 1), and click Enroll account</p> <p></p> <p>Click Enroll account in the confirmation box that pops up.</p> <p>The account status should now be \u201cEnrolling\u201d, this will take a few minutes.</p> <p></p> <p>Once that\u2019s done, you now have Control Tower set up, with a management account, an auditing account, a logging account, and a sub-account (where you would add your resources).</p>"},{"location":"aws-control-tower/#stage-5-enabling-a-configuration-control-for-your-organization","title":"Stage 5 - Enabling a configuration control for your organization","text":"<p>Head to the Control Tower console: https://ap-southeast-2.console.aws.amazon.com/controltower/home/controls?region=ap-southeast-2</p> <p>Click on All controls</p> <p>Click on the Find controls search bar and select \u201cGuidance\u201d</p> <p></p> <p>Then select \u201cEquals\u201d</p> <p></p> <p>Then select \u201cElective\u201d</p> <p></p> <p>Mandatory controls are already enabled, and we just want to browse what we can enable.</p> <p>There\u2019s three different types of control implementations; Config rule (used to detect), CloudFormation guard rules (used to prevent during CloudFormation creation), and Service Control Policy\u2019s (used to prevent at the API level).</p> <p>For this demo, we\u2019re going to set up an SCP, for S3, to prevent changes to the Encryption on buckets.</p> <p>Find the control called <code>[AWS-GR_AUDIT_BUCKET_ENCRYPTION_ENABLED] Disallow Changes to Encryption Configuration for Amazon S3 Buckets</code>, select it, and click Enable control</p> <p></p> <p>On the next page, select the OU you want this control to apply to. We\u2019ll apply this to the \u201cSandbox\u201d OU, as that\u2019s where our sub-account is.</p> <p></p> <p>Remember, in a production environment you might have dozens of nested OUs for different teams, departments, environments, etc, so this level of granularity can be useful.</p> <p>Click Enable control on OU</p> <p>Now let\u2019s test it out. Head back to the IAM Identity Center portal, and log into your sub-account (\u201dSandbox\u201d)</p> <p></p> <p>Head to the S3 console: https://s3.console.aws.amazon.com/s3/home?region=ap-southeast-2</p> <p>Go to Buckets and click Create bucket</p> <p></p> <p>Choose any Bucket name you like, and leave the rest of the options as default.</p> <p>Click Create bucket</p> <p>Note: You might get an error at the top of the S3 console saying you don\u2019t have permissions to <code>PutEncryption</code>, that is expected because of the control we have enabled.</p> <p>Once that\u2019s done, go into your newly created bucket, go to the Properties tab, and click Edit next to Default encryption</p> <p></p> <p>We\u2019ll try and change the encryption from the default S3 managed encryption key, to a KMS managed key.</p> <p></p> <p>Click Save changes</p> <p>And as expected, we got a permissions error.</p> <p></p> <p>This next part is optional, but we\u2019re now going to disable the S3 encryption control we enabled, and see if we can make that encryption change.</p> <p>Log back into the management account, using the Identity Center portal.</p> <p>Find the control that we enabled, it should be this one: https://ap-southeast-2.console.aws.amazon.com/controltower/home/controls/AWS-GR_AUDIT_BUCKET_ENCRYPTION_ENABLED?region=ap-southeast-2</p> <p>Under the OUs enabled tab, select the Sandbox OU that we chose earlier, and click Disable control</p> <p></p> <p>Click Disable in the pop up confirmation window.</p> <p>Now if we log back into the sub-account, go back to S3, back into the bucket, and try changing the encryption again, it should work.</p> <p></p> <p>Note: SCPs can take a couple of minutes to enable/disable, so if this doesn\u2019t work straight away, try again after a few minutes.</p>"},{"location":"aws-control-tower/#stage-6-creating-a-service-catalog-product","title":"Stage 6 - Creating a service catalog product","text":"<p>We\u2019re going to add a product to the Service Catalog in our management account. This will then be deployed in Stage 7, to a new account, using account factory. </p> <p>This is just a simple, AWS provided, Cloudformation based, product that creates an S3 bucket. In the real world you would most likely create your own CloudFormation stacks, or get products from AWS Marketplace to deploy more complex applications.</p> <p>First we need to create the required IAM role that Control Tower will use. Head to the IAM console: https://us-east-1.console.aws.amazon.com/iamv2/home?region=ap-southeast-2#/roles</p> <p>Go to Roles and click Create Role</p> <p>Under Trusted entity type change this \u201cAWS account\u201d. Leave \u201cThis account (\\&lt;account ID&gt;)\u201d selected.</p> <p>Click Next</p> <p>Search for and attach the policy \u201cAWSServiceCatalogAdminFullAccess\u201d</p> <p></p> <p>Click Next</p> <p>Set the Role name to <code>AWSControlTowerBlueprintAccess</code>. </p> <p>Click Create role</p> <p>Head to the Service Catalog console: https://ap-southeast-2.console.aws.amazon.com/servicecatalog/home?region=ap-southeast-2#home</p> <p>Go to the Getting started library</p> <p></p> <p>Search for <code>Amazon S3 Public Bucket with Read Only Access</code>, select it, and click Add to portfolio</p> <p></p> <p>On the next page, under Select portfolio, select the \u201cAWS Control Tower Account Factory Portfolio\u201d</p> <p></p> <p>Add to portfolio</p>"},{"location":"aws-control-tower/#stage-7-creating-an-account-with-account-factory","title":"Stage 7 - Creating an account with account factory","text":"<p>Head to the Control Tower console: https://ap-southeast-2.console.aws.amazon.com/controltower/home/dashboard?region=ap-southeast-2</p> <p>Go to Account factory and click Create account</p> <p></p> <p>On the next page, set the Account email to (you guessed it) a new email address. </p> <p>Set the Display name to anything you like.</p> <p>Under Access configuration, you can use the same email as the Account email above.</p> <p>Once that\u2019s filled out, expand the Account factory customization pane.</p> <p>Set the Account that contains your AWS Service Catalog products to the management account ID, and click Validate</p> <p></p> <p>It\u2019s possible to have a separate AWS account to store all of these product blueprints, but for this demo we\u2019re not going to do that, we\u2019re just going to use the management account.</p> <p>Under Select a product, choose the S3 product we created earlier.</p> <p></p> <p>Select Product version \u201cv1.0\u201d</p> <p>Under Deployment Regions select \u201cHome Region\u201d</p> <p>Click Create account</p> <p>If you go to Organization you will see your account being set up, and enrolling in your Organization and Control Tower</p> <p></p> <p>After a few minutes, this should change from \u201cEnrolling\u201d to \u201cEnrolled\u201d and move to the \u201cSandbox\u201d OU.</p> <p>Head back to the IAM Identity Center console: https://us-east-1.console.aws.amazon.com/iamv2/home?region=ap-southeast-2#/organization/accounts</p> <p>Go to the AWS Accounts page, select your newly created account, and click Assign users or groups</p> <p></p> <p>Go to the Users tab, select your IAM user, and click Next</p> <p></p> <p>Select all Permission sets, and click Next</p> <p></p> <p>Click Submit</p> <p>Now, head back to the Identity Center portal, select your new account, and login</p> <p></p> <p>Head to the CloudFormation console: https://ap-southeast-2.console.aws.amazon.com/cloudformation/home?region=ap-southeast-2#/stacks</p> <p>You will see a number of Stacks that have been set up by Control Tower, including our Service Catalog product</p> <p></p> <p>Now head to the S3 console: https://s3.console.aws.amazon.com/s3/buckets?region=ap-southeast-2</p> <p>You will see our bucket that was created from the Service Catalog product</p> <p></p> <p>Again, this is a very basic example of what is possible with Service Catalog, and usually it would be used for much more complex applications.</p>"},{"location":"aws-control-tower/#stage-8-clean-up","title":"Stage 8 - Clean up","text":"<p>This clean up is going to be a bit different, because we are going to be unmanaging (un-enrolling) accounts in Control Tower, deleting the Organization, and deleting AWS accounts. </p> <p>Obviously you should be very careful here to only delete the accounts you created for this demo.</p> <p>Log back in to your management account as the root user. We\u2019re going to be deleting our IAM users from Identity Centre, so we need to be using the root account.</p> <p>Head to the Service Catalog console: https://ap-southeast-2.console.aws.amazon.com/servicecatalog/home?region=ap-southeast-2#admin-products</p> <p>Go to the Product list page, and click on \u201cAmazon S3 Public Bucket with Read Only Access\u201d</p> <p>Under the Portfolios tab, select each portfolio, and click Disassociate product from portfolio</p> <p></p> <p>Go to the Product list page, select the \u201cAmazon S3 Public Bucket with Read Only Access\u201d product, click Actions \u2192 Remove from organization</p> <p></p> <p>Enter \u201cdelete\u201d in the confirmation box</p> <p>Head to the IAM Identity Center console: https://ap-southeast-2.console.aws.amazon.com/singlesignon/home?region=ap-southeast-2#!/instances/users</p> <p>On the Users page, select the IAM user you created earlier, and click Delete users</p> <p></p> <p>Click Delete user in the confirmation box</p> <p>Head to the Control Tower console: https://ap-southeast-2.console.aws.amazon.com/controltower/home/organization?region=ap-southeast-2</p> <p>Under Organization, select the accounts you wish to remove from Control Tower, and click Actions \u2192 Unmanage</p> <p></p> <p>Enter \u201cUNMANAGE\u201d in the confirmation box, and click Unmanage account</p> <p>Head to the Organizations console: https://us-east-1.console.aws.amazon.com/organizations/v2/home/accounts</p> <p>Select the accounts you wish to remove from the Organization, and click Actions \u2192 Remove from organization</p> <p></p> <p>Click Remove account in the confirmation box</p> <p>Once all accounts have been removed from the Organization, you can delete the Organization by going to Settings and then clicking Delete organization</p> <p></p> <p>Enter your oranization ID, and click Delete organization.</p> <p>Head to the IAM console: https://us-east-1.console.aws.amazon.com/iam/home?region=ap-southeast-2</p> <p>Go to the Roles page, search for and select <code>AWSControlTowerExecution</code> and click Delete</p> <p></p> <p>Enter the role name in the confirmation box, and click Delete</p> <p>Go to the Roles page, search for and select <code>AWSControlTowerBlueprintAccess</code> and click Delete</p> <p></p> <p>Enter the role name in the confirmation box, and click Delete</p> <p>Now, login to the root user account for each account you created in this guide. Click on the menu in the top right, then Account, then scroll down to the very bottom of the page</p> <p></p> <p>Click on Close Account</p> <p>Then in the confirmation box, click on Close Account again</p>"},{"location":"aws-dms-database-migration/","title":"AWS DMS Database Migration","text":""},{"location":"aws-dms-database-migration/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-dynamodb-lambda-trigger/","title":"AWS Dynamodb Lambda Trigger","text":""},{"location":"aws-dynamodb-lambda-trigger/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-efs/","title":"AWS EFS","text":""},{"location":"aws-efs/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-elastic-disaster-recovery/","title":"AWS Elastic Disaster Recovery","text":""},{"location":"aws-elastic-disaster-recovery/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-global-accelerator/","title":"AWS Global Accelerator","text":""},{"location":"aws-global-accelerator/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-iam-scp-permissions-boundary/","title":"AWS IAM Scp Permissions Boundary","text":""},{"location":"aws-iam-scp-permissions-boundary/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-lambda-s3-events/","title":"AWS Lambda S3 Events","text":""},{"location":"aws-lambda-s3-events/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-lambda-xray/","title":"AWS Lambda Xray","text":""},{"location":"aws-lambda-xray/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-lex-lambda-rds/","title":"AWS Lex Lambda RDS","text":""},{"location":"aws-lex-lambda-rds/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-macie/","title":"AWS Macie","text":""},{"location":"aws-macie/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-patch-manager/","title":"AWS Patch Manager","text":""},{"location":"aws-patch-manager/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-pet-rekognition-ecr/","title":"AWS Pet Rekognition ECR","text":""},{"location":"aws-pet-rekognition-ecr/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-static-web-hosting/","title":"Hosting a Static Website Using Amazon S3","text":"<p>Automate the Process of Hosting a Static Website Using Amazon S3 and AWS Developer Tools.</p>"},{"location":"aws-static-web-hosting/#project-overview","title":"\ud83d\ude80 Project Overview","text":"<p>For this project we will have a static website hosted on S3 and will be utilizing CodePipeline to monitor and automatically deploy changes made from our CodeCommit repository where our index.html is hosted. Then we\u2019ll setup CloudFront as a CDN that will redirect HTTP requests to HTTPS.</p>"},{"location":"aws-static-web-hosting/#problem-statement","title":"\ud83d\udd27 Problem Statement","text":"<p>Inefficient manual deployment processes and lack of content delivery network (CDN) optimization hinder the seamless deployment and accessibility of a static website hosted on Amazon S3. Additionally, the absence of automated monitoring and deployment mechanisms further complicates the update process, leading to delays and potential inconsistencies in website content. To address these challenges, our project aims to automate the deployment process using AWS services such as CodePipeline and CodeCommit, while also enhancing website performance and security by implementing CloudFront as a CDN to redirect HTTP requests to HTTPS.</p>"},{"location":"aws-static-web-hosting/#techonology-stack","title":"\ud83d\udcbd Techonology Stack","text":"<p>\u25cf CDN: AWS Cloudfront</p> <p>\u25cf Storage: AWS S3 for file storage.</p> <p>\u25cf CI/CD: Automate deployment using AWS Code Pipeline, AWS CodeBuild</p>"},{"location":"aws-static-web-hosting/#architecture-diagram","title":"\ud83d\udccc Architecture Diagram","text":""},{"location":"aws-static-web-hosting/#project-requirements","title":"\ud83c\udf1f Project Requirements","text":"<p>Your team has asked you to create a way to automate the deployment of a website. Currently your developers have to go through the process manually to test each new update to their code. You\u2019ll need to provide the static site URL to the developers and also make a modification to the code in the GitHub repo to verify the pipeline is working.</p> <p>https://github.com/cloudspaceacademy/aws-static-web-hosting.git</p> <ol> <li>Create a new repository in GitHub or CodeCommit and load the attached HTML.</li> <li>Create and configure a S3 bucket to host your static website.</li> <li>Create a CI/CD pipeline using the AWS Codepipeline service .</li> <li>Set your repo as the Source Stage of the Codepipeline that is triggered when an update is made to a GitHub repo.</li> <li>For the deploy stage select your S3 bucket.</li> <li>Deploy the pipeline and verify that you can reach the static website.</li> <li>Make an update to the code in your github to verify that the codepipeline is triggered. This can be as simple as a change to the Readme file because any change to the files should trigger the workflow.</li> </ol> <p>Note: you can skip the Build stage for this project.</p> <p>Your app is very popular all around the world but some users are complaining about slow load times in some Regions. You have been asked to add CloudFront as a CDN for your static website. CloudFront should allow caching of your static webpage and only allow HTTPS traffic to your site.</p>"},{"location":"aws-static-web-hosting/#instructions","title":"\ud83d\ude80 Instructions","text":"<p>Create New Repository and Clone it.</p> <p>First we need to create a repository.</p> <p>Navigate to GitHub -&gt; Repositories -&gt; Create Repository and give it a name.</p> <p></p> <p>Use the Clone URL to clone it to your local system.</p> <p>Add your files to your local repository, commit your changes, and push your changes.</p> <p></p> <p>File has been pushed from our local repo to CodeCommit.</p> <p>Create S3 Bucket</p> <p>Navigate to S3 -&gt; Create Bucket.</p> <p>Uncheck \u201cBlock all Public Access\u201d and acknowledge.</p> <p>Navigate to your bucket -&gt; Properties -&gt; Edit Static website hosting</p> <p>Enable Static website hosting and add your index document</p> <p>Now we need to create a bucket policy. Got to Permissions and edit the bucket policy.</p> <p>The following will allow everyone to access the bucket using the GetObject command,</p> <p></p> <p>Setup Pipeline.</p> <p>Navigate to CodePipeline -&gt; Create pipeline provide a name and click next.</p> <p>Source Provider = AWS CodeCommit</p> <p>Repository name = \u201cSelect your repo from the list\u201d</p> <p>Branch Name = Master</p> <p></p>"},{"location":"aws-systems-manager/","title":"AWS Systems Manager","text":""},{"location":"aws-systems-manager/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-video-on-demand/","title":"AWS Video On Demand","text":""},{"location":"aws-video-on-demand/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-vpc-flow-logs/","title":"AWS Vpc Flow Logs","text":""},{"location":"aws-vpc-flow-logs/#coming-soon","title":"Coming Soon !","text":""},{"location":"aws-waf/","title":"AWS Waf","text":""},{"location":"aws-waf/#overview","title":"Overview","text":"<p>We\u2019re going to be creating an EC2 instance running WordPress, we\u2019ll put an Application Load Balancer in front of it, and then associate an AWS WAF WebACL with that load balancer. </p> <p>We\u2019ll then explore the different rules and actions we can configure, as well as setting up WAF logging to S3 and viewing those logs.</p> <p>We will be creating this environment in the ap-southeast-4 (Melbourne) region, so all links to the console will be there. Make sure you change region if you\u2019re deploying elsewhere.</p>"},{"location":"aws-waf/#instructions","title":"Instructions","text":""},{"location":"aws-waf/#stage-1-creating-the-wordpress-instance","title":"Stage 1 - Creating the WordPress instance","text":"<p>First, we need to get the AMI of the latest version of WordPress. These AMIs are created by Bitnami and are made available for free. Head to this website to get the latest AMI for your region: https://bitnami.com/stack/wordpress/cloud/aws/amis</p> <p>Scroll down to the region you\u2019re using, and copy the AMI ID</p> <p></p> <p>Head to the EC2 console: https://ap-southeast-4.console.aws.amazon.com/ec2/home</p> <p>Go to Instances and click Launch instances</p> <p></p> <p>Set the Name to \u201cwordpress\u201d </p> <p>Under Application and OS Images (Amazon Machine Image) enter the AMI you copied earlier and press Enter</p> <p>In the window that pops up, go to the Community AMIs tab, and click Select next to the AMI result</p> <p></p> <p>Leave the Instance type as default (it should be t2.micro or t3.micro, which is free tier eligible)</p> <p>Set the Key pair (login) to \u201cProceed without a key pair (Not recommended)\u201d. We won\u2019t need to access this instance.</p> <p>Under Network settings, click Edit</p> <p></p> <p>We\u2019re going to use the default VPC, so you just need to make sure that is the VPC selected, and Auto-assign public IP is enabled</p> <p></p> <p>Under Firewall (security groups), leave \u201cCreate security group\u201d selected, and change the Security group name to \u201cwordpress-waf\u201d</p> <p>Under Security group rule 1, change the Type to HTTP</p> <p></p> <p>Leave everything else as default, and click Launch instance</p> <p>Once that\u2019s done, head back to the EC2 console: https://ap-southeast-4.console.aws.amazon.com/ec2/home</p> <p>Go to Instances, and copy the public IPv4 address of your new instance</p> <p></p> <p>Visit that IP in your browser, and you should see the default WordPress home page. You may need to wait a couple of minutes for your instance to finish booting. Also, make sure you\u2019re visiting http://, https:// won\u2019t work, and we won\u2019t be using HTTPS for this demo. <p></p>"},{"location":"aws-waf/#stage-2-creating-a-target-group","title":"Stage 2 - Creating a target group","text":"<p>Head to the EC2 console: https://ap-southeast-4.console.aws.amazon.com/ec2/home</p> <p>Go to Target Groups and click Create target group</p> <p></p> <p>Leave the Target Type as \u201cInstances\u201d</p> <p>Set the Target group name to \u201cwordpress\u201d</p> <p></p> <p>Leave everything else as default, and click Next</p> <p>Select the instance we created in the previous step, and click Include as pending below</p> <p></p> <p>Click Create target group</p>"},{"location":"aws-waf/#stage-3-creating-a-application-load-balancer","title":"Stage 3 - Creating a application load balancer","text":"<p>Head to the EC2 console: https://ap-southeast-4.console.aws.amazon.com/ec2/home</p> <p>Go to Load Balancers and click Create load balancer</p> <p></p> <p>Select \u201cApplication Load Balancer\u201d</p> <p></p> <p>Set the Load balancer name to \u201cwordpress-lb\u201d</p> <p>Select all the subnets under \u201cNetwork mapping\u201d, and make sure the VPC is set to default (<code>-</code>)</p> <p></p> <p>Under Security Groups, add the \u201cwordpress-waf\u201d security group we created in stage 1</p> <p></p> <p></p> <p>Under Listeners and routing, change the \u201cdefault action\u201d for HTTP to forward to the target group we created in the previous step</p> <p></p> <p>Click Create load balancer</p> <p>Once that\u2019s done, go back to Load Balancers and copy the DNS name for your newly created load balancer</p> <p></p> <p>Visit that URL in your browser (again, HTTP only). You should see your WordPress home page still.</p> <p>Note: ALB\u2019s can take a couple of minute to create and become ready</p> <p></p>"},{"location":"aws-waf/#stage-3-creating-an-s3-bucket-for-waf-logs","title":"Stage 3 - Creating an S3 bucket for WAF logs","text":"<p>Head to the S3 console: https://s3.console.aws.amazon.com/s3/buckets</p> <p>Go to Buckets and click Create bucket</p> <p></p> <p>S3 buckets for WAF logs must begin with <code>aws-waf-logs-</code>, so in my case I\u2019m going to use the bucket name <code>aws-waf-logs-demo-waf</code>. Remember S3 bucket names are unique so this may be taken, just pick another bucket name that begins with <code>aws-waf-logs-</code></p> <p>Make sure the region is set to the same region your EC2 instance was created in.</p> <p>Leave everything else as is, and click Create bucket</p>"},{"location":"aws-waf/#stage-4-setting-up-the-waf","title":"Stage 4 - Setting up the WAF","text":"<p>Head to the WAF console: https://us-east-1.console.aws.amazon.com/wafv2/homev2</p> <p>In each step in this stage, make sure your region remains set to the region your instance is deployed in. For me that is \u201cAsia Pacific (Melbourne)\u201d</p> <p></p> <p>First, we\u2019re going to create an IP Set, which is basically just a list of IP addresses grouped together, these are commonly used for allow-listed IPs (e.g. your home or office IPs, your developers, etc). IP Sets can contain up to 10,000 CIDR ranges, which makes allowing or blocking large numbers of networks very easy.</p> <p>In a new tab, open the following URL: https://checkip.amazonaws.com/</p> <p></p> <p>This will give you your IP address as seen by Amazon. This is useful if your work PC has a split-horizon VPN for example, where only AWS traffic is routed over the VPN.</p> <p>Copy this IP address down for the next step.</p> <p>Go to IP sets and click Create IP Set</p> <p></p> <p>Set the IP Set name to \u201chome-ip\u201d</p> <p>In the IP addresses box, add the IP you copied earlier followed by <code>/32</code>, so in my case <code>34.129.222.183/32</code></p> <p></p> <p>Click Create IP set</p> <p>Go to Regex pattern sets and click Create regex pattern set</p> <p></p> <p>Set the Regex pattern set name to \u201cno-wp-files\u201d</p> <p>In the Regular expressions box, enter:</p> <pre><code>(wp\\-login\\.php)$\n(.*wp\\-config.*)\n(xmlrpc\\.php)\n</code></pre> <p></p> <p>Click Create regex pattern set</p> <p>Go to Web ACLs and click Create web ACL</p> <p></p> <p>Set the Name to \u201cwordpress-acl\u201d</p> <p>Under Resource type select \u201cRegional resources\u201d</p> <p>Under Associated AWS resources click Add AWS resources</p> <p></p> <p>Select \u201cApplication Load Balancer\u201d (different regions will have different options), and select the ALB we created earlier</p> <p></p> <p>Click Add</p> <p>Click Next</p> <p>Under Rules, click Add rules and then Add managed rule groups</p> <p></p> <p>Expand AWS managed rule groups and you will see a list of WAF rule groups that are supplied and maintained by AWS.</p> <p>Under Free rule groups select the following</p> <p></p> <p>Each rule group has a description showing what kind of attacks the rule group helps protect against. </p> <p>Considering we\u2019re running a WordPress application, we definitely want the WordPress rules, and WordPress is built using PHP, so we want the PHP application rules as well. Our database for WordPress is MySQL, so the SQL database rule set will help protect against SQL Injection attacks (among other things)</p> <p>The \u201cCore rule set\u201d contains the most rules, and protects against common attack methods provided by the open source OWASP organisation (https://owasp.org/)</p> <p>Note each rule group has a \u201ccapacity\u201d value. Each Web ACL has a maximum capacity of 1500 \u201cWebACL Capacity Units\u201d (WCUs), and each rule we add uses up some of those units. \u201cCore rule set\u201d contains the most rules, so therefore is the most expensive at 700 WCUs. This limit is in place to prevent traffic inspection from taking too long (among presumably other reasons on AWS\u2019 side). </p> <p>You\u2019ll also see some other 3rd party rule groups provided by external security organisations. These are usually paid subscriptions that you subscribe to via the AWS Marketplace. We won\u2019t be using these in this demo.</p> <p>Click Add rules</p> <p></p> <p>On the next page, you can see it is telling us we\u2019ve used 1100/1500 WCUs. We\u2019re going to add more rules later, so this is fine.</p> <p>Under Default web ACL action for requests that don't match any rules leave this set to \u201cAllow\u201d. In our case, we only want traffic that matches one of our rules to be blocked, but if this was reversed and we had an application we only wanted to be accessed by specific IP addresses for example, we would change this to \u201cBlock\u201d.</p> <p>Click Next</p> <p>On the next page, we can change the priority of our rules. Similar to Network ACLs, rules within a Web ACL are executed in order of top to bottom. For now, we\u2019ll leave these as is.</p> <p></p> <p>Click Next</p> <p>On the next page, make sure Request sampling options is enabled, and click Next</p> <p></p> <p>On the final page, click Create web ACL</p> <p>On the next page, click on your newly created Web ACL</p> <p></p> <p>Go to the Logging and metrics tab and click Enable under Logging</p> <p></p> <p>Change the Logging destination to \u201cS3 bucket\u201d, and select your S3 bucket from the dropdown. There should only be one because the dropdown will only show buckets beginning with <code>aws-waf-logs-</code></p> <p></p> <p>In a production environment, you might want to redact certain fields such as an authorisation header, a query string, the HTTP method, etc. But for this demo we will leave these unselected.</p> <p>Click Save</p>"},{"location":"aws-waf/#stage-5-testing-our-waf","title":"Stage 5 - Testing our WAF","text":"<p>Open your ALB URL in a new tab (this will be something like http://wordpress-lb-1275728828.ap-southeast-4.elb.amazonaws.com/ that we visited earlier).</p> <p>Your WordPress homepage should still be viewable. You can visit the sample blog entry (<code>/sample-page/</code>), and the admin login (<code>/wp-admin</code>)</p> <p>Let\u2019s try and use a very basic SQL Injection attack on our website and see if the WAF stops us, go to the page: <code>/wp-login.php?user=1+ORDER+BY+10</code></p> <p>You should see a \u201c403 Forbidden\u201d message</p> <p></p> <p>This means our WAF rules are working as expected.</p> <p>If you head back to the WAF console: https://us-east-1.console.aws.amazon.com/wafv2/homev2/</p> <p>Go to Web ACLs and into the <code>wordpress-acl</code> we created (if your ACL doesn\u2019t show up, make sure the region is set correctly)</p> <p></p> <p>Then under Sampled requests, if you search for your IP address from earlier you should see a few of the requests you made, and their Action (allow or block) and the rule that caused any block actions</p> <p></p> <p>Sampled requests can take a few minutes to appear, and not all requests will appear (only certain requests are sampled and displayed).</p> <p>You will possibly also see various other requests from random other IP addresses that scan the internet, usually looking for vulnerable websites.</p> <p></p>"},{"location":"aws-waf/#stage-6-adding-custom-rules","title":"Stage 6 - Adding custom rules","text":"<p>We don\u2019t want our WordPress login page or XML-RPC page being accessed by anyone, so we\u2019re going to use our regex pattern we created earlier to block any requests to <code>/wp-login.php</code> or <code>/xmlrpc.php</code></p> <p>Head to the WAF console: https://us-east-1.console.aws.amazon.com/wafv2/homev2</p> <p>Click on your WordPress Web ACL</p> <p></p> <p>Go to the Rules tab then click Add rules then Add my own rules and rule groups</p> <p></p> <p>Leave Rule Type as \u201cRule builder\u201d</p> <p>Set the Rule Name to <code>no-wp-files</code> (you can call this anything you like, but for the demo we\u2019ll use this)</p> <p>Leave the Type as \u201cRegular rule\u201d. We could set this to a \u201cRate-based rule\u201d to prevent bots from trying hundreds or thousands of different passwords on our <code>wp-login.php</code> page, but we won\u2019t for this demo.</p> <p>Set Inspect to \u201cURI Path\u201d</p> <p></p> <p>Set Match Type to \u201cMatches pattern from regex pattern set\u201d</p> <p></p> <p>Set Regex pattern set to the pattern set we created earlier (it should be the only one there)</p> <p></p> <p>Set Text transformation to \u201cLowercase\u201d</p> <p></p> <p>What this does is changes everything in the \u201cInspect\u201d field (in our case, the URI Path) to lowercase. This is useful if you wanted to look for <code>/wp-login.php</code>, <code>/WP-LOGIN.PHP</code>, or even <code>/Wp-LoGiN.PhP</code>, WAF would see all of these as <code>/wp-login.php</code>. There are (as you can see) multiple other transformations such as compressing white space, decoding URL characters, Base64 decoding strings, etc, and you can have multiple of these.</p> <p>Under Action, select \u201cBlock\u201d</p> <p>Click Add rule</p> <p>On the next page, we need to move our new rule to the top of the priority list. Remember rules are executed in order of top to bottom. </p> <p>Select your new rule, and keep clicking Move up until it\u2019s at the top of the list.</p> <p></p> <p>Now if we head back to our website, and try accessing the <code>/wp-login.php</code> page, or <code>/wp-admin</code>, we will get a 403 Forbidden message</p> <p></p> <p>We can also try <code>/xmlrpc.php</code>, which will give us a 403 Forbidden as well.</p> <p>And if we head back to our WAF console, and go to the Overview tab, after a few minutes you should see your IP address, the URI\u2019s you tried to access, and the action \u201cBlock\u201d, note the Metric name also shows the name of the rule that was used.</p> <p></p> <p>Now, because we trust our home IP, we\u2019re going to allow everything from that IP address. </p> <p>Go to the Rules tab and click Add rules then Add my own rules and rule groups</p> <p></p> <p>Set the Rule type to \u201cIP Set\u201d</p> <p>Set the Rule name to \u201callow-home\u201d</p> <p>Change the IP set to the \u201chome-ip\u201d IP Set we created earlier</p> <p>Leave IP address to use as the originating address as \u201cSource IP address\u201d</p> <p>Change Action to \u201cAllow\u201d</p> <p></p> <p>Click Add rule</p> <p>On the next page, move this newly created rule to the top of the list</p> <p></p> <p>Click Save</p> <p>Now if we visit the <code>/wp-login.php</code> page again, you should be able to see it.</p> <p>Note: If you can\u2019t, double check your IP address hasn\u2019t changed since earlier (https://checkip.amazonaws.com/) and if it has, go back to Stage 4 and create a new IP Set (or add your IP to the <code>home-ip</code> IP Set)</p> <p></p> <p>Now as a final rule, we only want users in our country to be able to visit our website, so we\u2019re going to geo block all other countries.</p> <p>Note: Geo blocking can be inaccurate and it\u2019s definitely not uncommon for an IP to be configured as being in the wrong country. If an ISP purchases a block of IP addresses from an RIR (regional Internet registry) or from another ISP, it will usually take time for all of the many Geo IP databases to be updated, and then requires AWS to start using this updated database in AWS WAF, so keep this in mind if geo blocking doesn\u2019t work for you.</p> <p>We need to do this in two stages, we need a rule that allows our country (in my case, Australia), which will go at the bottom of the rule priority list (because we still want visitors from Australia to be checked by all of our other rules like SQL Injection, not accessing <code>wp-login.php</code>, etc). Then we need another rule that blocks all countries that aren\u2019t Australia.</p> <p>Obviously you can change this to be whatever country / countries you like.</p> <p>Go to the Rules tab and click Add rules then Add my own rules and rule groups</p> <p></p> <p>Set the rule Name to \u201callow-au\u201d (or allow-) <p>Change the Inspect option to \u201cOriginates from a country in\u201d</p> <p>Change Country codes to  (Australia in my case) <p>Change the Action to Allow</p> <p></p> <p>Click Add rule</p> <p>On the next page, leave this rule last and click Save</p> <p>Go to the Rules tab and click Add rules then Add my own rules and rule groups</p> <p></p> <p>Set the rule Name to \u201cblock-earth\u201d</p> <p>Change If a request to \u201cdoesn\u2019t match the statement (NOT)\u201d</p> <p>Change the Inspect option to \u201cOriginates from a country in\u201d</p> <p>Change Country codes to  (Australia in my case) <p>Change the Action to Block</p> <p></p> <p>Click Add rule</p> <p>On the next page, move this rule up to the second position. We still want our home IP to have access at all times. This is useful if you\u2019re allow listing your companies IP address(es) for example, you don\u2019t want your staff to lose access if there was a geo blocking error.</p> <p></p> <p>Click Save</p> <p>Now we should be able visit our WordPress website still</p> <p></p> <p>But let\u2019s test it from another country. To do this, we will use a website speed test service called Pingdom: https://tools.pingdom.com/</p> <p>Enter your ALB URL, and change Test from to a country that isn\u2019t your country (e.g. I\u2019m in Australia, so I will choose to test from Germany)</p> <p></p> <p>Click Start test</p> <p>After a few seconds the test will complete. Scroll down to Response codes and you should only see <code>403</code></p> <p></p> <p>So the website loaded quickly, we got an \u201cA\u201d in Performance, but that is because AWS WAF returned essentially an empty page with the words \u201c403 Forbidden\u201d.</p> <p>If Pingdom offers tests from your country, try running the test again from there, in my case, Australia.</p> <p></p> <p>You can see there were no <code>403</code> responses, only 200\u2019s (and 302, which is fine).</p>"},{"location":"aws-waf/#stage-7-viewing-our-waf-logs","title":"Stage 7 - Viewing our WAF Logs","text":"<p>Head to the S3 console: https://s3.console.aws.amazon.com/s3/buckets</p> <p>Go to Buckets and click on your AWS Waf Logs bucket</p> <p></p> <p>The directory structure will be:</p> <p><code>/AWSLogs/[ACCOUNT ID]/WAFLogs/[REGION]/[WEB ACL]/[YEAR]/[MONTH]/[DAY]/[HOUR]/[5 MINUTE BLOCK]/[LOGS]</code></p> <p>Try to go into the directory that was created around the time you were going through the previous steps (using Pingdom), select one of (or the only) log file, and click Download</p> <p></p> <p>This will download a log file that is gzipped. MacOS and Linux systems can extract these natively, however Windows PCs will need a program such as 7-Zip (download here).</p> <p>On MacOS you can simply double click the <code>.gz</code> file, on Windows you will need to right click, go to <code>7-Zip</code> then <code>Extract here</code></p> <p></p> <p>This will give you a <code>.log</code> text file. Open it up in whichever text editor you\u2019re comfortable with, I\u2019ll use VSCode.</p> <p>Each log line contains information about that specific request. It can be a bit difficult to read, but you can use a website like https://jsonformatter.org/json-pretty-print to make the JSON log line more readable (note, paste one line at a time on this website). Otherwise, if you have a plugin for VSCode (or your text editor) that can \u201cpretty print\u201d JSON, you can use that.</p> <p>Here\u2019s an example of one of my log entries, the <code>action</code> is \u201cALLOW\u201d, and the <code>terminatingRuleId</code> is \u201callow-home\u201d. This is my home IP address that is allowed as the first rule in the Web ACL.</p> <pre><code>{\n    \"timestamp\": 1680696799239,\n    \"formatVersion\": 1,\n    \"webaclId\": \"arn:aws:wafv2:ap-southeast-4:123456789012:regional/webacl/wordpress-acl/1be732ad-bf31-44d9-8745-4da02eeaac18\",\n    \"terminatingRuleId\": \"allow-home\",\n    \"terminatingRuleType\": \"REGULAR\",\n    \"action\": \"ALLOW\",\n    \"terminatingRuleMatchDetails\": [],\n    \"httpSourceName\": \"ALB\",\n    \"httpSourceId\": \"123456789012-app/wordpress-lb/19541f9649c59058\",\n    \"ruleGroupList\": [],\n    \"rateBasedRuleList\": [],\n    \"nonTerminatingMatchingRules\": [],\n    \"requestHeadersInserted\": null,\n    \"responseCodeSent\": null,\n    \"httpRequest\": {\n        \"clientIp\": \"34.129.222.183\",\n        \"country\": \"AU\",\n        \"headers\": [\n            {\n                \"name\": \"Host\",\n                \"value\": \"wordpress-lb-1275728828.ap-southeast-4.elb.amazonaws.com\"\n            },\n            {\n                \"name\": \"Connection\",\n                \"value\": \"keep-alive\"\n            },\n            {\n                \"name\": \"Cache-Control\",\n                \"value\": \"max-age=0\"\n            },\n            {\n                \"name\": \"Upgrade-Insecure-Requests\",\n                \"value\": \"1\"\n            },\n            {\n                \"name\": \"User-Agent\",\n                \"value\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36\"\n            },\n            {\n                \"name\": \"Accept\",\n                \"value\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\"\n            },\n            {\n                \"name\": \"Accept-Encoding\",\n                \"value\": \"gzip, deflate\"\n            },\n            {\n                \"name\": \"Accept-Language\",\n                \"value\": \"en-AU,en-GB;q=0.9,en-US;q=0.8,en;q=0.7\"\n            },\n            {\n                \"name\": \"Cookie\",\n                \"value\": \"wordpress_test_cookie=WP%20Cookie%20check\"\n            }\n        ],\n        \"uri\": \"/wp-login.php\",\n        \"args\": \"\",\n        \"httpVersion\": \"HTTP/1.1\",\n        \"httpMethod\": \"GET\",\n        \"requestId\": \"1-642d65df-2bcb4716046498c504b1432a\"\n    }\n}\n</code></pre> <p>Below that, we can see the Pingdom request from German that was blocked by our \u201cblock-earth\u201d rule, note the country code under \u201chttpRequest\u201d is \u201cDE\u201d (Germany)</p> <pre><code>{\n    \"timestamp\": 1680696828189,\n    \"formatVersion\": 1,\n    \"webaclId\": \"arn:aws:wafv2:ap-southeast-4:123456789012:regional/webacl/wordpress-acl/1be732ad-bf31-44d9-8745-4da02eeaac18\",\n    \"terminatingRuleId\": \"block-earth\",\n    \"terminatingRuleType\": \"REGULAR\",\n    \"action\": \"BLOCK\",\n    \"terminatingRuleMatchDetails\": [],\n    \"httpSourceName\": \"ALB\",\n    \"httpSourceId\": \"123456789012-app/wordpress-lb/19541f9649c59058\",\n    \"ruleGroupList\": [],\n    \"rateBasedRuleList\": [],\n    \"nonTerminatingMatchingRules\": [],\n    \"requestHeadersInserted\": null,\n    \"responseCodeSent\": null,\n    \"httpRequest\": {\n        \"clientIp\": \"35.156.182.26\",\n        \"country\": \"DE\",\n        \"headers\": [\n            {\n                \"name\": \"Host\",\n                \"value\": \"wordpress-lb-1275728828.ap-southeast-4.elb.amazonaws.com\"\n            },\n            {\n                \"name\": \"Connection\",\n                \"value\": \"keep-alive\"\n            },\n            {\n                \"name\": \"Pragma\",\n                \"value\": \"no-cache\"\n            },\n            {\n                \"name\": \"Cache-Control\",\n                \"value\": \"no-cache\"\n            },\n            {\n                \"name\": \"User-Agent\",\n                \"value\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/61.0.3163.100 Chrome/61.0.3163.100 Safari/537.36 PingdomPageSpeed/1.0 (pingbot/2.0; +http://www.pingdom.com/)\"\n            },\n            {\n                \"name\": \"Accept\",\n                \"value\": \"image/webp,image/apng,image/*,*/*;q=0.8\"\n            },\n            {\n                \"name\": \"Referer\",\n                \"value\": \"http://wordpress-lb-1275728828.ap-southeast-4.elb.amazonaws.com/\"\n            },\n            {\n                \"name\": \"Accept-Encoding\",\n                \"value\": \"gzip, deflate\"\n            },\n            {\n                \"name\": \"Accept-Language\",\n                \"value\": \"en-US,en;q=0.8\"\n            }\n        ],\n        \"uri\": \"/favicon.ico\",\n        \"args\": \"\",\n        \"httpVersion\": \"HTTP/1.1\",\n        \"httpMethod\": \"GET\",\n        \"requestId\": \"1-642d65fc-1fbafd367ee9c2b671146b3f\"\n    },\n    \"labels\": [\n        {\n            \"name\": \"awswaf:clientip:geo:country:DE\"\n        },\n        {\n            \"name\": \"awswaf:clientip:geo:region:DE-HE\"\n        }\n    ]\n}\n</code></pre> <p>You can also see all of our request headers are there, as well as the URI, query strings, etc. Remember these can be redacted from logs files if needed.</p>"},{"location":"aws-waf/#stage-8-optional-custom-response-bodies","title":"Stage 8 - Optional: Custom response bodies","text":"<p>This can be useful if you would prefer a branded or nicer looking response from AWS WAF, especially if a request is being blocked (403).</p> <p>Head to the WAF console: https://us-east-1.console.aws.amazon.com/wafv2/homev2</p> <p>Click on your WordPress Web ACL</p> <p></p> <p>Go to Custom response bodies and click Create custom response body&lt;/kbd <p></p> <p>Set the Response body object name to \u201cfriendly-block\u201d</p> <p>Set the Content type to HTML</p> <p>Set the Response body to:</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;title&gt;Forbidden&lt;/title&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;div style=\"text-align:center;\"&gt;\n      &lt;h1 style=\"color: pink;\"&gt;Forbidden&lt;/h1&gt;\n      &lt;img src=\"https://img.freepik.com/free-photo/red-white-cat-i-white-studio_155003-13189.jpg\" alt=\"A red and white cat\"&gt;\n    &lt;/div&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>Note: This image may move or be deleted, feel free to replace the image file in <code>&lt;img src=</code> with another image of your choosing.</p> <p>Click Save</p> <p>Go to Rules, select the <code>allow-home</code> rule, and click Edit</p> <p></p> <p>For demo purposes, we\u2019re going to change this rule to block, so that we can see our custom response.</p> <p>Change the Action to \u201cblock\u201d, and then expand Custom response</p> <p>Select Enable</p> <p>Set Response code to 403</p> <p>Set Choose how you would like to specify the response body to \u201cfriendly-block\u201d</p> <p></p> <p>Click Save rule</p> <p>Now if we refresh our WordPress website, we\u2019ll see a much nicer forbidden message</p> <p></p> <p>Obviously in a production environment this would likely be company branded, and you would apply it to the other rules that you expect to block users. But for this demo, it was much easier to apply it to our home IP rule just so we could see it.</p>"},{"location":"aws-waf/#stage-9-clean-up","title":"Stage 9 - Clean up","text":"<p>Head to the WAF console: https://us-east-1.console.aws.amazon.com/wafv2/homev2</p> <p>Click on your WordPress Web ACL</p> <p></p> <p>Go to the Associated AWS resources tab, select your <code>wordpress-lb</code> ALB, and click Disassociate</p> <p></p> <p>Type \u201cremove\u201d in the confirmation box and click Disassociate</p> <p>Go back to Web ACLs, select your Web ACL and click Delete</p> <p></p> <p>Enter \u201cdelete\u201d in the confirmation box and click Delete</p> <p>Go to IP Sets, select your <code>home-ip</code> IP Set and click Delete</p> <p></p> <p>Enter \u201cdelete\u201d in the confirmation box and click Delete</p> <p>Go to Regex pattern sets, select your <code>no-wp-files</code> pattern set, and click Delete</p> <p></p> <p>Enter \u201cdelete\u201d in the confirmation box and click Delete</p> <p>Head to the S3 console: https://s3.console.aws.amazon.com/s3/buckets</p> <p>Go to Buckets and select your <code>aws-waf-logs-</code> bucket, and click Empty</p> <p></p> <p>In the confirmation window, enter \u201cpermanently delete\u201d and click Empty</p> <p>Go to Buckets and select your <code>aws-waf-logs-</code> bucket, and click Delete</p> <p></p> <p>In the confirmation window, enter your bucket name and click Delete</p> <p>Head to the EC2 console: https://ap-southeast-4.console.aws.amazon.com/ec2</p> <p>Go to Load Balancers, select your <code>wordpress-lb</code> ALB, click Actions then Delete load balancer</p> <p></p> <p>Enter \u201cconfirm\u201d in the confirmation box, and click Delete</p> <p>Go to Target Groups, select your <code>wordpress</code> target group, click Actions then Delete</p> <p></p> <p>Click Yes, delete in the confirmation box.</p> <p>Go to Instances, select your <code>wordpress</code> instance, click Instance state then Terminate instance</p> <p>Be careful and make sure you\u2019re deleting the instance we created at the beginning of the demo.</p> <p></p> <p>Click Terminate in the confirmation box.</p> <p>Go to Security Groups, select the <code>wordpress-waf</code> security group, click Actions then Delete security groups</p> <p></p> <p>Click Delete in the confirmation box</p> <p>Note: If you get an error similar to the following, wait a few minutes for your ALB and EC2 instance to finish terminating, then try again</p> <p></p>"},{"location":"dockerbeginning/","title":"Flask Calculator Web Application","text":"<p>Introduction</p> <p>This documentation provides an overview of the Flask calculator web application and guides you on how to set up and run the project. This simple web application allows users to perform basic arithmetic calculations such as addition, subtraction, multiplication, and division.</p> <p>Tools Requirements</p> <p>Python | installation guide Click Here </p> <p>Docker | installation guide Click Here</p>"},{"location":"dockerbeginning/#getting-started","title":"Getting Started","text":"<p>Open terminal and run this command to clone the repository of Flask Calculator Web Application.</p> <p>Command:</p> <pre><code>git clone https://github.com/cloudspaceacademy/Flask-Calculator-app.git\n</code></pre> <p>Now navigate to the directory Flask-Calculator-app. You will see a cloudspace directory now open that directory using the terminal. </p> <p>You can simply use this command:</p> <p><code>cd Flask-Calculator-app &amp;&amp; cd cloudspace</code></p> <p>Now run  <code>ls</code>  command and Now what are the scenarios there you will see app.py,Dockerfile,requirements.txt file and a directory named templates.</p> <p>Let's elaborate why we use that Dockerfile for this application and how we write that Dockerfile.</p>"},{"location":"dockerbeginning/#why-do-we-use-docker","title":"Why do we use docker?","text":"<p>Docker provides a way to package, distribute, and run applications consistently across various environments. It simplifies the process of creating, deploying, and maintaining applications while improving portability, scalability, and resource efficiency. The Dockerfile is at the heart of this process, defining how the container image is built and what software is included, making it a valuable tool for modern software development and deployment.</p> <p>Now let's elaborate how we write that Dockerfile to package our application.</p>"},{"location":"dockerbeginning/#dockerfile","title":"Dockerfile","text":"<pre><code>FROM python:3.9-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\n\nRUN pip install -r requirements.txt\n\nCOPY . .\n\nEXPOSE 5000\n\nCMD [\"python\", \"app.py\"] \n</code></pre> <p>Now just break down all the code and elaborate why they use for </p> <p>FROM python:3.9-slim</p> <p>This line specifies the base image for our Docker container. In this case, it's using the official Python 3.9 slim image. The \"slim\" variant provides a smaller image size by excluding some non-essential components, making it suitable for many Python applications.</p> <p>WORKDIR /app</p> <p>This line sets the working directory inside the container to /app. This is where the application code and files will be placed, and it's the directory where commands will be executed by default. It's a good practice to organize your application within a dedicated directory.</p> <p>COPY requirements.txt .</p> <p>This line copies the requirements.txt file from the host system into the container. The . at the end specifies the destination path, which is the current directory within the container (i.e., /app).</p> <p>RUN pip install -r requirements.txt</p> <p>This line uses the RUN instruction to execute a command within the container. In this case, it runs pip install -r requirements.txt to install all the Python packages listed in the requirements.txt file. This step ensures that the necessary dependencies are installed in the container.</p> <p>COPY . .</p> <p>This line copies all the files and directories from the current directory (the directory where the Dockerfile is located) on our host system to the /app directory in the container. This includes our application code (app.py) and any other files or directories in the project folder.</p> <p>EXPOSE 5000</p> <p>This line informs Docker that the container will listen on port 5000. While this line is not necessary for port mapping, it serves as a form of documentation to indicate which ports the container expects to use. It's good practice for clarity.</p> <p>CMD [\"python\", \"app.py\"]</p> <p>This line defines the default command to execute when the container is started. It runs the Flask application by executing python app.py. The Flask development server will start, and our application will be served on port 5000 as specified earlier.</p> <p>Now lets learn how to run the application by using just two docker commands from your local machine. </p>"},{"location":"dockerbeginning/#running-the-application-with-docker","title":"Running the Application with Docker","text":""},{"location":"dockerbeginning/#building-a-docker-image","title":"Building a Docker Image","text":"<p>To build a Docker image, you use the docker build command. </p> <p><code>docker build -t flask-calculator .</code></p> <p>docker build -t flask-calculator .  Run this command where the Dockerfile is located. Our Dockerfile is located in the root directory cloudspace so that we use ( . )  </p> <p>Now break down the code </p> <p>docker build: This is the Docker command for building an image.</p> <p>-t flask-calculator: This part specifies the image name and tag. In this example, the image is named \"flask-calculator,\" and it has no specific tag. The tag is used to version your images; if you don't specify one, it defaults to \"latest.\"</p> <p>.: The dot (.) at the end of the command tells Docker to use the current directory as the build context. It means that Docker will look in the current directory for a Dockerfile to use as the basis for building the image.</p>"},{"location":"dockerbeginning/#docker-image-used-for","title":"Docker image used for :","text":"<p>A Docker image is a lightweight, standalone, executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, system libraries, and settings. Images are used as the basis for creating Docker containers.</p>"},{"location":"dockerbeginning/#running-a-docker-container","title":"Running a Docker Container","text":"<p>Once we have built our Docker image, we can create and run containers from it using the docker run command. </p> <p><code>docker run -p 5000:80 -d flask-calculator</code></p> <p>Now break down the code </p> <p>docker run: This is the Docker command for creating and running a container.</p> <p>-p 5000:80: This part maps port 5000 from our host machine to port 80 inside the container. It allows us to access our Flask application running in the container on port 5000.</p> <p>-d: this defines the container will run in detached mode, which means it runs in the background.</p> <p>flask-calculator: This is the name of the Docker image you want to use to create the container. The image name corresponds to the one you specified when building the image.</p>"},{"location":"dockerbeginning/#docker-container-used-for","title":"Docker Container used for :","text":"<p>A Docker container is a runnable instance of a Docker image. It is an isolated environment where our application runs. Containers share the same OS kernel as the host system but are isolated from each other. They are ephemeral and can be started, stopped, and removed as needed.</p>"},{"location":"dockerbeginning/#access-the-application","title":"Access the application","text":"<p>Now you will see the app is running from your local machine on port 5000. Open your web browser and go to localhost:5000 to access the Flask calculator web application within the Docker container.</p>"},{"location":"dockercomposebeginning/","title":"Flask Calculator Web Application","text":"<p>Introduction</p> <p>This documentation provides an overview of the Flask calculator web application and guides you on how to set up and run the project. The Flask calculator is a straightforward web application designed for basic arithmetic calculations, including addition, subtraction, multiplication, and division.</p> <p>Tools Requirements</p> <p>Python | installation guide Click Here </p> <p>Docker | installation guide Click Here</p>"},{"location":"dockercomposebeginning/#getting-started","title":"Getting Started","text":"<p>Open your terminal and clone the Flask Calculator Web Application repository using the following command:</p> <pre><code>git clone https://github.com/cloudspaceacademy/Flask-Calculator-app.git\n</code></pre> <p>Navigate to the \"Flask-Calculator-app\" directory By running this command :</p> <p><code>cd Flask-Calculator-app</code></p> <p>Inside the \"Flask-Calculator-app\" directory, you will find a \"cloudspace\" directory. Enter this directory using the following command:</p> <p><code>cd cloudspace</code></p> <p>In Previous session \"Docker Beginner\" you have learned about how to run this \"Flask Calculator Web Application\" using Dockerfile.Now we are gonna learn how to run this application using docker-compose.yml file.</p> <p>In Previous session \"Docker Beginner\" you have learned about Dockerfile.How To write a Dockerfile,why its used for,how to run the application using Dockerfile etc.Now let's jump up to docker-compose.yml file.</p>"},{"location":"dockercomposebeginning/#docker-composeyml-file-used-for","title":"docker-compose.yml file used for","text":"<p>A docker-compose.yml file is used to define a multi-container Docker application. It allows you to define and configure multiple services, networks, and volumes in a single file. Docker Compose simplifies the process of orchestrating complex applications by providing a way to manage multiple containers as a single application stack.</p> <p>Simply Dockerfile is used for single service or to run the application using one container and docker-compose.yml file is used for multiple services or to packaging and run the application using multi containers.</p>"},{"location":"dockercomposebeginning/#docker-composeyml","title":"docker-compose.yml","text":"<pre><code>version: '3.8'\n\nservices:\n  flask-calculator:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - \"8080:80\"\n\n</code></pre> <p>Let's Break down the code </p> <p>version: '3.8':Specifies the version of the Docker Compose file syntax. In this case, it's using version 3.8.</p> <p>services:Defines the services that make up the Docker application.</p> <p>flask-calculator:The name of the service. In this case, it's named flask-calculator.</p> <p>build:Specifies how to build the Docker image for the service.</p> <p>context: . :Specifies the build context, which is the path to the directory containing the Dockerfile and any files needed for the build. In this case, it's set to the current directory (.)</p> <p>dockerfile: Dockerfile:Specifies the name of the Dockerfile to use for building the image. In this case, it's named Dockerfile.</p> <p>ports:Specifies the ports to expose from the container. - \"8080:80\":</p> <p>Maps port 80 from the container to port 8080 on the host machine. The format is host_port:container_port.</p>"},{"location":"dockercomposebeginning/#running-the-application-with-docker-compose","title":"Running the Application with Docker Compose","text":"<p>To run the application run this command where the docker-compose.yml file is located.</p> <p>Command :</p> <p><code>docker-compose up -d</code> </p> <p>Break down Codes:</p> <p>docker-compose: The Docker Compose command-line tool.</p> <p>up: This command is used to create and start containers based on the configurations specified in the docker-compose.yml file.</p> <p>-d: Stands for \"detached mode.\" When you run Docker containers in detached mode, the containers run in the background, and you get your terminal prompt back. This allows you to continue using the same terminal session for other tasks.</p> <p>Now Run <code>docker ps</code> command to see the running container id that you have just created.</p>"},{"location":"dockercomposebeginning/#access-the-application","title":"Access the application","text":"<p>Now you will see the app is running from your local machine on port 8080. Open your web browser and go to localhost:8080 to access the Flask calculator web application within the Docker container.</p>"},{"location":"microservices-bookStore/","title":"Building a BookStore leveraging MicroServices.","text":"<p>Step by Step BookStore DevOps Project.</p>"},{"location":"microservices-bookStore/#project-overview","title":"\ud83d\ude80 Project Overview","text":"<p>Our DevOps initiative will focus on constructing an AWS environment, setting up a Kubernetes cluster through Amazon Elastic Kubernetes Service (EKS), and implementing an efficient process for continuous integration and deployment.</p> <p>To lay the groundwork, we'll utilize the Bookinfo demonstration application. This application, consisting of multiple services, serves as an illustration of the intricacies inherent in a contemporary Microservices architecture.</p> <ul> <li> <p>GitHub Actions: CI/CD platform integrated with GitHub for automating workflows.</p> </li> <li> <p>AWS: Cloud services provider for hosting applications and managing infrastructure.</p> </li> <li> <p>EKS: Amazon Elastic Kubernetes Service for deploying, managing, and scaling containerized applications.</p> </li> <li> <p>ArgoCD: Declarative GitOps continuous delivery tool for Kubernetes.</p> </li> <li> <p>Terraform: Infrastructure as Code (IaC) tool for provisioning and managing AWS resources.</p> </li> <li> <p>ECR: Amazon Elastic Container Registry for securely storing and managing Docker images.</p> </li> <li> <p>Grafana: Monitoring and visualization platform for metrics.</p> </li> <li> <p>Prometheus: Open-source monitoring and alerting toolkit.</p> </li> </ul> <p>We Will try to cover more Technologies and concept in this Article as possible.</p>"},{"location":"microservices-bookStore/#problem-statement","title":"\ud83d\udd27 Problem Statement","text":"<p>In today's rapidly evolving tech landscape, mastering DevOps tools and technologies is a top priority for IT professionals seeking to streamline workflows, foster collaboration, and expedite project delivery. Many individuals dedicate significant time and effort to completing courses on essential tools such as Terraform, ArgoCD, Istio, Kubernetes, and AWS, equipping themselves with the theoretical knowledge needed to revolutionize their development processes.</p> <p>However, what often remains unaddressed is the significant challenge that arises once the courses are completed and the real-world integration journey begins. The struggle of connecting the dots between these powerful tools and effectively implementing them into a cohesive DevOps pipeline can be both daunting and perplexing.</p> <p>This is a narrative that many of us have encountered firsthand \u2013 the initial excitement of acquiring new skills, followed by the frustration of translating those skills into tangible results within our projects. This workshop aims to address this gap by offering a comprehensive guide not only on the 'how' of using these tools but also on the 'how' within the context of a holistic DevOps approach.</p>"},{"location":"microservices-bookStore/#techonology-stack","title":"\ud83d\udcbd Techonology Stack","text":"<p>\u25cf Application Integration: Simple Notification Service (SNS)</p> <p>\u25cf Management &amp; Governance: CloudWatch.</p> <p>\u25cf Security, Identity &amp; Compliance: Secret Manager, SonarCloud(SonarQube)</p> <p>\u25cf CI/CD: Automate deployment using AWS Code Pipeline, AWS CodeBuild, AWS CodeCommit, AWS CodeArtifact</p>"},{"location":"microservices-bookStore/#architecture-diagram","title":"\ud83d\udccc Architecture Diagram","text":"<p>Microservices Diagram:</p> <p></p> <p>Technology Stack Diagram:</p> <p></p>"},{"location":"microservices-bookStore/#getting-started","title":"\ud83d\udea6 Getting Started","text":""},{"location":"microservices-bookStore/#prerequisites","title":"Prerequisites","text":"<p>Before you get started, make sure you have the following prerequisites in place:</p> <ul> <li>AWS account.</li> <li>AWS CLI.</li> <li>Docker.</li> <li>Git for cloning the repository.</li> <li>Any modern code editor (e.g., Visual Studio Code, Sublime Text, etc.)</li> </ul> <p>To begin, you need an AWS account. If you don't have one, head to the AWS website and sign up for an account.</p> <p>We need IAM user Access Key and Secret Key to be used with Terraform</p> <p></p> <p>Never disclose your Access Keys to anyone, and consistently utilize Secrets Managers.</p>"},{"location":"microservices-bookStore/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ul> <li>Step-1: Clone the repository</li> <li>Step-2: Terraform Workflow</li> <li>Step-3: Terraform Cloud Env Vars</li> <li>Step-4: Install Required CLIs</li> <li>Step-5: Update Workflows with ECR URL</li> <li>Step-6: Update GitHub Repo with AWS Secrets</li> <li>Step-7: Deploy the Microservices Manifests</li> <li>Step-8: Istio Proxy uses Envoy</li> <li>Step-9: Test our BookStore Application</li> <li>Step-10: Monitoring</li> </ul>"},{"location":"microservices-bookStore/#step-1-clone-the-repository","title":"\u2728 Step-1-Clone-the-repository","text":"<ul> <li>Please clone the project repository to your local machine. (You will need to be added to the CloudSpace organization before you can clone this.)</li> </ul> <p><code>bash    git clone https://github.com/waleedmagdy/devops_project.git</code></p>"},{"location":"microservices-bookStore/#step-2-terraform-workflow","title":"\ud83c\udf1f Step-2-Terraform-Workflow","text":"<ul> <li>In this workshop, we are using Terraform Cloud to let Terraform runs in a consistent and reliable environment.</li> </ul> <ul> <li>First create an account on Terraform Cloud if you don\u2019t have one.</li> </ul> <p>Terraform Cloud Sign up (Terraform Cloud has a Free License so no need to worry about pricing)</p> <ul> <li>Create your first organization and then Set up a workspace in Terraform Cloud. This will help manage your infrastructure as code and enable collaboration.</li> </ul> <p></p> <ul> <li> <p>Choose Version Control Workflow to work with your repository on Github which we will choose to do here.</p> </li> <li> <p>If you want to work with Terraform from your Terminal you can go for CLI-driven Workflow.</p> </li> </ul> <p>Version Control Workflow &gt; Connect to Github &gt; choose the repository &gt; configure Setting</p> <p></p> <ul> <li>In Advanced options configure the Terraform Working Directory terraform as our Terraform code is inside terraform directory</li> </ul> <p>Before talking about the Terraform files, let's take time to read about Terraform \u2014 Best Practices  terraform-best-practices and Terraform \u2014 Best Practices</p> <p>Learn and Pick the right Terraform code Structure you need to follow.</p> <p>Now let\u2019s talk about the Terraform Directory before running our first plan and apply.</p> <p>terraform.tf</p> <p>This Terraform configuration block includes settings for Terraform Cloud (formerly known as Terraform Enterprise) and configures the AWS provider. Let's break down the code step by step:</p> <p>1. Terraform Cloud Configuration:</p> <pre><code>        terraform {\n        cloud {\n            organization = \"devops-project-org\"\n\n            workspaces {\n            name = \"devops-project-workspace\"\n            }\n        }\n        }  \n</code></pre> <p>Let's breakdown the above code:</p> <p>In this part of the code, you are configuring Terraform Cloud settings:</p> <p>organization: The name of the Terraform Cloud organization is set to \"devops-project-org\".</p> <p>workspaces: Within the organization, a workspace is configured with the name \"devops-project-workspace\". A workspace in Terraform Cloud is an isolated environment for managing infrastructure.</p> <p>2. AWS Provider Configuration:</p> <pre><code>    provider \"aws\" {\n    region = \"us-east-1\"\n    }  \n</code></pre> <p>Let's breakdown the above code:</p> <p>This part of the code configures the AWS provider using the provider block:</p> <p>aws: The name of the provider is \"aws\", indicating that this block configures resources from Amazon Web Services (AWS).</p> <p>region: The AWS region is set to \"us-east-1\", which means resources created using this provider will be located in the US East (North Virginia) region.</p> <p>vpc.tf</p> <p>This Terraform code snippet is used to create a Virtual Private Cloud (VPC) in Amazon Web Services (AWS) using the terraform-aws-modules/vpc/aws module. Let's break down the code step by step:</p> <p>1. Module Declaration:</p> <pre><code>    module \"vpc\" {\n    source = \"terraform-aws-modules/vpc/aws\"\n    }    \n</code></pre> <p>Here, you are declaring a Terraform module named \"vpc\" using the module source terraform-aws-modules/vpc/aws. This module is available in the Terraform registry and is designed to create a VPC with AWS resources.</p> <p>2. Module Parameters:</p> <pre><code>    name = \"my-vpc\"\n    cidr = \"10.0.0.0/16\"  \n</code></pre> <p>These parameters define the basic configuration of the VPC:</p> <p>name: The name of the VPC will be set to \"my-vpc\".    cidr: The IP range for the VPC is set to \"10.0.0.0/16\".</p> <p>3. Availability Zones and Subnets:</p> <pre><code>    azs             = [\"us-east-1a\", \"us-east-1b\", \"us-east-1c\"]\n    private_subnets = [\"10.0.1.0/24\", \"10.0.2.0/24\", \"10.0.3.0/24\"]\n    public_subnets  = [\"10.0.101.0/24\", \"10.0.102.0/24\", \"10.0.103.0/24\"]  \n</code></pre> <p>These parameters specify the availability zones and subnets for the VPC:</p> <p>azs: The list of Availability Zones where the subnets will be created.</p> <p>private_subnets: The list of private subnet CIDR blocks.</p> <p>public_subnets: The list of public subnet CIDR blocks.</p> <p>4. NAT and VPN Gateways:</p> <pre><code>    enable_nat_gateway = true\n    enable_vpn_gateway = true  \n</code></pre> <p>These settings enable NAT and VPN gateways for the VPC:</p> <p>enable_nat_gateway: NAT gateways will be created for the private subnets.</p> <p>enable_vpn_gateway: A VPN gateway will be created for the VPC.</p> <p>5. Tags:</p> <pre><code>    enable_nat_gateway = true\n    enable_vpn_gateway = true  \n</code></pre> <p>This block assigns tags to the resources created by the module. Tags are metadata that provide additional information about resources. Here, two tags are added: \"Terraform\" with the value \"true\" and \"Environment\" with the value \"dev\".</p> <p>ecr.tf:</p> <p>This Terraform code snippet creates an Amazon Elastic Container Registry (ECR) repository and defines an output to display the repository URL. Let's break down the code step by step:</p> <p>1. ECR Repository Resource:</p> <pre><code>    resource \"aws_ecr_repository\" \"my_repo\" {\n    name = \"my-ecr-repo\"\n    image_tag_mutability = \"MUTABLE\"\n} \n</code></pre> <p>In this part of the code, you are creating an AWS ECR repository named \"my-ecr-repo\" using the aws_ecr_repository resource. The parameters you've set are:</p> <p>name: The name of the ECR repository is set to \"my-ecr-repo\".</p> <p>image_tag_mutability: The mutability of image tags is set to \"MUTABLE\", which means you can overwrite tags on images.</p> <p>2. Output for Repository URL:</p> <pre><code>  output \"repository_url\" {\n  value = aws_ecr_repository.my_repo.repository_url\n}  \n</code></pre> <p>This part of the code defines an output named \"repository_url\" that will display the URL of the ECR repository. The value of this output is set to the repository URL of the aws_ecr_repository.my_repo resource.</p> <p>eks.tf:</p> <p>This Terraform code is used to create an Amazon Elastic Kubernetes Service (EKS) cluster using the terraform-aws-modules/eks/aws module. Let's break down the code step by step:</p> <p>1. Module Declaration:</p> <pre><code>    module \"eks\" {\n    source  = \"terraform-aws-modules/eks/aws\"\n    version = \"~&gt; 19.0\"\n}    \n</code></pre> <p>This declares a Terraform module named \"eks\" and specifies the source from which to fetch the EKS module (terraform-aws-modules/eks/aws) along with a version constraint.</p> <p>2. Cluster Configuration: </p> <pre><code>    cluster_name    = \"my-cluster\"\n    cluster_version = \"1.27\"\n    cluster_endpoint_public_access  = true  \n</code></pre> <p>These parameters configure the EKS cluster:</p> <p>cluster_name: The name of the EKS cluster will be set to \"my-cluster\".   cluster_version: The Kubernetes version of the cluster will be \"1.27\".   cluster_endpoint_public_access: The Kubernetes API server endpoint will have public access.</p> <p>3. Cluster Addons:</p> <pre><code>    cluster_addons = {\n    coredns = { most_recent = true }\n    kube-proxy = { most_recent = true }\n    vpc-cni = { most_recent = true }\n    }     \n</code></pre> <p>This block configures cluster addons like CoreDNS, kube-proxy, and vpc-cni to use the most recent versions.</p> <p>4. VPC and Subnet Configuration:</p> <pre><code>    vpc_id                   = module.vpc.vpc_id\n    subnet_ids               = module.vpc.private_subnets\n    control_plane_subnet_ids = module.vpc.public_subnets     \n</code></pre> <p>These parameters specify the Virtual Private Cloud (VPC) and subnet details for the EKS cluster using outputs from another module (likely named \"vpc\").</p> <p>5. Managed Node Group Configuration:</p> <pre><code>eks_managed_node_group_defaults = {\n  instance_types = [\"m6i.large\", \"m5.large\", \"m5n.large\", \"t3.large\"]\n}\n\neks_managed_node_groups = {\n  green = {\n    use_custom_launch_template = false\n    min_size     = 1\n    max_size     = 10\n    desired_size = 1\n    instance_types = [\"t3.large\"]\n    capacity_type  = \"SPOT\"\n  }\n}   \n</code></pre> <p>This section configures an EKS managed node group named \"green\" with specific instance types, sizes, and capacity type (SPOT).</p> <p>6. Fargate Profiles:</p> <pre><code>    fargate_profiles = {\n    default = {\n        name = \"default\"\n        selectors = [ { namespace = \"default\" } ]\n    }\n    }    \n</code></pre> <p>This section defines a Fargate profile named \"default\" that targets the \"default\" namespace.</p> <p>aws-auth Configuration: This section defines how IAM roles, users, and accounts will be mapped to Kubernetes RBAC roles for cluster access.</p> <p>7. Tags:</p> <p>```bash     tags = {     Environment = \"dev\"     Terraform   = \"true\"     }      </p> <pre><code> Tags are assigned to the created resources for organization and identification purposes.\n## \ud83d\ude80 Step-3-Terraform-Cloud-Env-Vars\n\nWe need to configure our organization with our Access Key and Secret Key and you can do it specific for the workspace or globally for the organization.\n\nWe will do it globally now for the organization by creating Variable Set\n\nunder the organization setting go to Variable sets and Create new one\n\n![alt diagram](assets/images/microservices-bookstore/terra1.jpeg)\n\n\n   **1. Plan and Apply Terraform Code**:\n\n   Now we are ready to start the Plan\n\n![alt diagram](assets/images/microservices-bookstore/plan.jpeg)\n\n   Review the Plan resources and then **Confirm &amp; Apply**\n\n   **2. Check AWS Resources Creation**:\n\n   Verify in the AWS Management Console that your defined resources have been created as intended.\n\n   **3. Deploy EKS-Manage EC2 Instance**:\n\n   Of course it doesn\u2019t have to be an EC2 instance you can use your Terminal.\n\n   We will setup this instance to manage our EKS cluster from.\n\n\n   - Deploy ubuntu instance\n\n   - Install tools like **kubectl** (Kubernetes command-line tool), **aws-cli** (AWS Command Line Interface), and any other utilities you might need.\n\n   - Install aws-cli\n\n```bash\nsudo apt install unzip  \ncurl \"&lt;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip&gt;\" -o \"awscliv2.zip\" \nunzip awscliv2.zip\nsudo ./aws/install\naws --version  \n</code></pre> <ul> <li>Install kubectl</li> </ul> <pre><code>curl -LO \"&lt;https://dl.k8s.io/release/$&gt;(curl -L -s &lt;https://dl.k8s.io/release/stable.txt&gt;)/bin/linux/amd64/kubectl\" \nKubectl\ncurl -LO \"&lt;https://dl.k8s.io/$&gt;(curl -L -s &lt;https://dl.k8s.io/release/stable.txt&gt;)/bin/linux/amd64/kubectl.sha256\"\necho \"$(cat kubectl.sha256) kubectl\" | sha256sum --check\nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\nkubectl version --client   \n</code></pre> <ul> <li> <p>Configure AWS Credentials</p> </li> <li> <p>Run aws configure and provide your AWS Access Key ID, Secret Access Key, default region, and output format.</p> </li> <li> <p>Configure kubectl</p> </li> </ul> <pre><code>aws eks update-kubeconfig --name my-cluster --region us-east-1  \n</code></pre> <ul> <li>Test kubectl by running</li> </ul> <pre><code>kubectl get nodes\n\nconnect: connection refused    \n</code></pre> <p>\"Not working\" \ud83d\ude15</p> <p>You need to troubleshoot why kubectl client can\u2019t talk with the EKS endpoint</p> <p>Hint: Something is blocking the requests to the EKS endpoint </p> <p>Now after You can talk to our EKS, you should add this fix to our Terraform code.</p> <p>Now you can run</p> <pre><code>    kubectl get nodes\n\n    ##OUTPUT##\n\n    NAME                         STATUS   ROLES    AGE   VERSION\n    ip-10-0-1-149.ec2.internal   Ready    &lt;none&gt;   64s   v1.27.3-eks-a5565ad           \n</code></pre>"},{"location":"microservices-bookStore/#step-4-install-required-clis","title":"\ud83d\udcbd Step-4-Install-Required-CLIs","text":"<p>We installed the AWS CLI, kubectl Now we need to install istioctl and argo CLI and install the required k8s resources.</p> <ul> <li>istioctl</li> </ul> <pre><code>    curl -L &lt;https://istio.io/downloadIstio&gt; | sh - \n    cd istio-1.18.2/\n    export PATH=$PWD/bin:$PATH\n    istioctl install --set profile=demo -y \n</code></pre> <p>istioctl is a command-line utility provided by Istio for installing and interacting with Istio deployments.</p> <p>install is the subcommand used to install Istio components.</p> <p>-set profile=demo specifies the installation profile. The \"demo\" profile includes a set of Istio components suitable for demonstration purposes.</p> <ul> <li>argo CLI</li> </ul> <pre><code>    curl -sSL -o argocd-linux-amd64 &lt;https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64&gt;   \n    sudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd\n    rm argocd-linux-amd64\n    sudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd \n</code></pre> <ul> <li>Argo CD install</li> </ul> <pre><code>    kubectl create namespace argocd \n    kubectl apply -n argocd -f &lt;https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml&gt;\n    kubectl patch svc argocd-server -n argocd -p '{\"spec\": {\"type\": \"LoadBalancer\"}}'        \n</code></pre> <p>kubectl patch svc argocd-server -n argocd -p '{\"spec\": {\"type\": \"LoadBalancer\"}}' This command patches (updates) the Argo CD server service to change its service type to a LoadBalancer. This modification makes the Argo CD server accessible from outside the Kubernetes cluster via a load balancer's public IP address or DNS.</p> <ul> <li>Argo CD intial admin secret</li> </ul> <p><code>bash     argocd admin initial-password -n argocd</code> </p> <p>It\u2019s not a best practice to do it from the Terminal but I will give you a hint \u201cTerrafrom\u201d</p>"},{"location":"microservices-bookStore/#step-5-update-workflows-with-ecr-url","title":"\ud83d\udd27 Step-5-Update-Workflows-with-ECR-URL","text":"<p>Modify your continuous integration workflows to include the ECR repository URL for Container image storage.</p> <p></p> <p>Under .github/workflows/ you will find the Github Actions we will use to build/push our container images let\u2019s break one workflow down.</p> <p>details_workflow.yml :</p> <p>Workflow Name and Trigger:name: Details Service Build, Push and Deploy: Describes the name of the workflow.</p> <p>on: Specifies the events that trigger the workflow.</p> <p>workflow_dispatch: Allows manual triggering of the workflow.</p> <p>push: Triggers the workflow when code changes are pushed to the repository's specified paths.</p> <p>Environment Variables:env: Defines environment variables that will be available to the workflow steps.</p> <p>ECR_REGISTRY: Specifies the Amazon ECR (Elastic Container Registry) registry URL.</p> <p>ECR_REPOSITORY: Specifies the ECR repository name.</p> <p>Jobs:jobs: Contains one or more jobs to be executed in sequence.</p> <p>build_and_push_image: Describes a job named \"build_and_push_image\" that runs on an Ubuntu environment.</p> <p>runs-on: Specifies the type of runner environment.</p> <p>steps: Lists the individual steps within the job.</p> <p>Steps:A series of steps, each with a specific name, purpose, and associated actions.</p> <p>uses refers to pre-built GitHub Actions that perform specific tasks.</p> <p>Here's what each step does:</p> <p>Checkout code: Retrieves the repository's code using the actions/checkout GitHub Action.</p> <p>Configure AWS credentials: Configures AWS credentials to access the ECR registry.</p> <p>Login to Amazon ECR: Uses the aws-actions/amazon-ecr-login GitHub Action to log in to the ECR registry.</p> <p>Get short SHA: Retrieves the short SHA hash of the latest Git commit.</p> <p>Build and push Docker image: Builds a Docker image and pushes it to the specified ECR repository.</p> <p>Update Kubernetes Deployment Image: Updates the image tag in a Kubernetes deployment YAML file to match the built Docker image.</p> <p>Commit and Push Changes: Commits the changes made to the Kubernetes deployment YAML file and pushes them to the repository.</p>"},{"location":"microservices-bookStore/#step-6-update-github-repo-with-aws-secrets","title":"\ud83d\ude80 Step-6-Update-GitHub-Repo-with-AWS-Secrets","text":"<p>under Setting &gt; Secrets and Variables &gt; Actions</p> <p></p> <p>Run Workflows</p> <p>Let\u2019s the party begins</p> <ul> <li> <p>as we are using one repository we need our Github Workflow to update the new image</p> </li> <li> <p>we should configure the Actions to be able to Read and Write to it\u2019s repository</p> </li> <li> <p>under Setting &gt; Actions &gt; General</p> </li> </ul> <p></p> <p>The Workflows Will run if there is a push inside the services directories or manually, I will run them Manually Now.</p> <p></p> <p>You will find that I only added the Update Kubernetes Deployment Image part to details_workflow.yml</p> <p>You need to complete the other Workflows</p> <p>You need to check the manifests/kubernetes image part to mach it with the Workflows</p> <p>Check ECR Repo</p> <p></p> <p>Argo CD</p> <ul> <li>add the repository to argo cd</li> <li>I will do it VIA SSH</li> <li>add the public ssh key to you Github account setting</li> <li>add the private ssh key to the argocd repository connect page</li> </ul> <p></p> <ul> <li>deploy Namespaces to the cluster staging and monitoring</li> <li>under manifests/networking/namespaces/ Add NEW APP in the argocd homepage</li> <li>follow the configuration</li> <li>Path: manifests/networking/namespaces/</li> <li>you can keep the Namespace field blank</li> </ul>"},{"location":"microservices-bookStore/#step-7-deploy-the-microservices-manifests","title":"\ud83d\udcbc Step-7-Deploy-the-Microservices-Manifests","text":"<ul> <li> <p>under argocd/apps/services you will find Application CRD for argocd app to deploy our manifest resources to Kubernetes</p> </li> <li> <p>argocd homepage create NEW APP</p> </li> <li> <p>Application Name: app-services</p> </li> <li> <p>Project Name: default</p> </li> <li> <p>Sync Policy: Automatic</p> </li> <li> <p>[x] PRUNE RESOURCES</p> </li> <li> <p>[x] SELF HEAL</p> </li> <li> <p>[x] AUTO-CREATE NAMESPACE</p> </li> <li> <p>Repository URL:</p> </li> <li> <p>Path: argocd/apps /services/</p> </li> <li> <p>Cluster URL:</p> </li> <li> <p>Namespace: staging</p> </li> </ul> <p></p> <p>You can check the Argo CD home page also you can check the resources in the EKS on AWS Console</p> <p></p> <p>If you check any POD in staging Namespace you will find that each one has two Containers</p> <p></p>"},{"location":"microservices-bookStore/#step-8-istio-proxy-uses-envoy","title":"\ud83d\udd12 Step-8-Istio-Proxy-uses-Envoy","text":"<p>Envoy proxies are deployed as sidecars to services, logically augmenting the services with Envoy\u2019s many built-in features, for example:</p> <ul> <li> <p>Dynamic service discovery</p> </li> <li> <p>Load balancing</p> </li> <li> <p>TLS termination</p> </li> <li> <p>HTTP/2 and gRPC proxies</p> </li> <li> <p>Circuit breakers</p> </li> <li> <p>Health checks</p> </li> <li> <p>Staged rollouts with %-based traffic split</p> </li> <li> <p>Fault injection</p> </li> <li> <p>Rich metrics</p> </li> </ul> <p>Istio Gateways and VirtualServices:</p> <ul> <li> <p>Istio Gateway: An Istio Gateway is a configuration resource that describes how external traffic (e.g., traffic from outside the Kubernetes cluster) is brought into the service mesh and how it's routed to services. It acts as an entry point into the mesh for incoming traffic. Gateways can be used to manage different protocols, such as HTTP, HTTPS, or TCP, and they can handle traffic based on hostnames, paths, and ports.</p> </li> <li> <p>Hosts and Ports: A Gateway is configured with a set of hosts and ports that it listens on. These could be domain names (for HTTP/HTTPS) or IP addresses and port numbers (for TCP).</p> </li> <li> <p>TLS Termination: Gateways can perform TLS termination, meaning they can handle SSL/TLS encryption and decryption for incoming traffic.</p> </li> <li> <p>Virtual Services: Gateways are often associated with VirtualServices to define how incoming traffic should be forwarded to specific services.</p> </li> <li> <p>Istio VirtualService: A VirtualService is a configuration resource that defines how traffic should be routed within the service mesh. It allows you to control the routing of traffic based on criteria like URI paths, headers, and more. VirtualServices are associated with one or more Istio Services and are often used in conjunction with Gateways to control how external traffic is routed to services.</p> </li> <li> <p>Destination Rules: VirtualServices can refer to DestinationRules, which define how traffic should be load-balanced between different versions of a service (canary deployments, blue-green deployments, etc.).</p> </li> <li>Traffic Splitting: VirtualServices can split traffic between different versions of services based on weights or other criteria.</li> <li>Match Conditions: VirtualServices define match conditions that determine which traffic is affected by the rules defined within them.</li> <li>Fault Injection: VirtualServices can also be used to inject faults or delays into requests for testing purposes.</li> </ul> <p>Deploy Gateways and VirtualServices</p> <ul> <li>under manifests/networking/gateways Create Argo CD app to deploy them</li> </ul> <p></p>"},{"location":"microservices-bookStore/#step-9-test-our-bookstore-application","title":"\ud83d\uddc4\ufe0f Step-9-Test-our-BookStore-Application","text":"<ul> <li>From the previous step you can browse to istio-ingressgateway url/productpage</li> <li>To get the url</li> </ul> <p><code>bash     kubectl get services -n istio-system     NAME                   TYPE           CLUSTER-IP       EXTERNAL-IP                                                               PORT(S)                                                                      AGE     istio-ingressgateway   LoadBalancer   172.20.27.197    ae271cd157c214ab888061809021225a-1922516608.us-east-1.elb.amazonaws.com   15021:32042/TCP,80:30092/TCP,443:31659/TCP,31400:31529/TCP,15443:32377/TCP   148m</code>    It will be the elb/dns under External-IP. We will open the application on our browser using that same link.</p> <p></p>"},{"location":"microservices-bookStore/#step-10-monitoring","title":"\ud83d\udcbb Step-10-Monitoring","text":"<p>Under argocd/apps/observability Create NEW APP in monitoring Namespace</p> <p></p> <p>We have Prometheus (Metrics Datastore), Loki (Logging), Jaeger (Tracing) In a short words</p> <ul> <li> <p>Logging: Recording events and activities for troubleshooting.</p> </li> <li> <p>Metrics: Measuring performance with numbers and graphs.</p> </li> <li> <p>Tracing: Following data flow to find performance issues.</p> </li> </ul> <p>Grafana</p> <p>one of a many dashboard you can import and a lot to explore</p> <p></p> <p>Kiali</p> <p>A comprehensive monitoring tool for Istio Service Mesh and also there is a lot to explore.</p> <p></p> <p>The dashboard can give you a Live fast response to any issue the could happen to any of your Microservice</p> <p></p>"},{"location":"microservices-bookStore/#license","title":"\ud83d\udcc4 License","text":"<p>This project is licensed under the MIT License.</p>"},{"location":"serverless-on-aws/","title":"Lambda and S3Events DEMO","text":""},{"location":"serverless-on-aws/#video-guides-for-this-mini-project","title":"Video Guides for this Mini Project","text":"<ul> <li>PART1</li> <li>PART2</li> <li>PLAYLIST</li> </ul> <p>In this demo lesson you're going to create a simple event-driven image processing pipeline. The pipeline uses two S3 buckets, a source bucket and a processed bucket. When images are added to the source bucket a lambda function is triggered based on the PUT.  When invoked the lambda function receives the <code>event</code> and extracts the bucket and object information. Once those details are known, the lambda function, using the <code>PIL</code> module pixelates the image with <code>5</code> different variations (8x8, 16x16, 32x32, 48x48 and 64x64) and uploads them to the processed bucket.</p>"},{"location":"serverless-on-aws/#stage-1-create-the-s3-buckets","title":"Stage 1 - Create the S3 Buckets","text":"<p>Move to the S3 Console https://s3.console.aws.amazon.com/s3/home?region=us-east-1# We will be creating <code>2</code> buckets, both with the same name, but each suffixed with a functional title (see below) , all settings apart from region and bucket name can be left as default. Click <code>Create Bucket</code> and create a bucket in the format of unique-name-<code>source</code> in the <code>us-east-1</code> region Click <code>Create Bucket</code> and create a another bucket in the format of unique-name-<code>processed</code> also in the <code>us-east-1</code> region These names will need to be unique, but as an example  </p> <p>Bucket 1 : <code>dontusethisname-source</code> Bucket 2 : <code>dontusethisname-processed</code> </p>"},{"location":"serverless-on-aws/#stage-2-create-the-lambda-role","title":"Stage 2 - Create the Lambda Role","text":"<p>Move to the IAM Console https://console.aws.amazon.com/iamv2/home?#/home Click Roles, then Create Role For <code>Trusted entity type</code>, pick <code>AWS service</code> For the service to trust pick <code>Lambda</code>  then click <code>Next</code> , <code>Next</code> again For <code>Role name</code> put <code>PixelatorRole</code>  then Create the role  </p> <p>Click <code>PixelatorRole</code> Under <code>Permissions Policy</code> we need to add permissions and it will be an <code>inline policy</code> Click <code>JSON</code>  and delete the contents of the code box entirely. Load this link in a new tab (https://raw.githubusercontent.com/acantril/learn-cantrill-io-labs/master/00-aws-simple-demos/aws-lambda-s3-events/01_LABSETUP/policy/s3pixelator.json) Copy the entire contents into your clipboard and paste into the previous permissions policy code editor box Locate the words <code>REPLACEME</code> there should be <code>4</code> occurrences, 2 each for the source and processed buckets .. and for each of those one for the bucket and another for the objects in that bucket. Replace the term <code>REPLACEME</code> with the name you picked for your buckets above, in my example it is <code>dontusethisname</code> You should end with 4 lines looking like this, only with <code>YOUR</code> bucket names  </p> <pre><code>\"Resource\":[\n    \"arn:aws:s3:::dontusethisname-processed\",\n    \"arn:aws:s3:::dontusethisname-processed/*\",\n    \"arn:aws:s3:::dontusethisname-source/*\",\n    \"arn:aws:s3:::dontusethisname-source\"\n]\n</code></pre> <p>Locate the two occurrences of <code>YOURACCOUNTID</code>, you need to replace both of these words with your AWS account ID To get that, click the account dropdown at the top right  click the small icon to copy down the <code>Account ID</code> and replace the <code>YOURACCOUNTID</code> in the policy code editor. important if you use the 'icon' to copy this number, it will remove the <code>-</code> in the account number for you :) you need to paste <code>123456789000</code> rather than <code>1234-5678-9000</code> </p> <p>You should have something which looks like this, only with your account ID:  </p> <pre><code>{\n      \"Effect\": \"Allow\",\n      \"Action\": \"logs:CreateLogGroup\",\n      \"Resource\": \"arn:aws:logs:us-east-1:123456789000:*\"\n  },\n  {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n          \"logs:CreateLogStream\",\n          \"logs:PutLogEvents\"\n      ],\n      \"Resource\": [\n          \"arn:aws:logs:us-east-1:123456789000:log-group:/aws/lambda/pixelator:*\"\n      ]\n  }\n</code></pre> <p>Click <code>Review Policy</code> For name put <code>pixelator_access_inline</code>  and create the policy.  </p>"},{"location":"serverless-on-aws/#stage-3-pre-only-do-this-part-if-you-want-to-get-experience-of-creating-a-lambda-zip","title":"Stage 3 (pre) - ONLY DO THIS PART IF YOU WANT TO GET EXPERIENCE OF CREATING A LAMBDA ZIP","text":""},{"location":"serverless-on-aws/#this-guide-is-only-tested-on-macos-it-should-work-on-linux-windows-may-require-different-tools","title":"this guide is only tested on macOS, it should work on linux, windows may require different tools.","text":""},{"location":"serverless-on-aws/#if-in-doubt-skip-to-step-3-below","title":"if in doubt, skip to step 3 below","text":"<p>From the CLI/Terminal Create a folder my_lambda_deployment Move into that folder create a folder called lambda Move into that folder Create a file called <code>lambda_function.py</code> and paste in the code for the lambda <code>pixelator</code> function (https://raw.githubusercontent.com/acantril/learn-cantrill-io-labs/master/00-aws-simple-demos/aws-lambda-s3-events/01_LABSETUP/lambda/lambda_function.py) then save Download this file (https://files.pythonhosted.org/packages/f3/3b/d7bb231b3bc1414252e77463dc63554c1aeccffe0798524467aca7bad089/Pillow-9.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl) into that folder run <code>unzip Pillow-9.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl</code> and then <code>rm Pillow-9.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl</code> These are the Pillow module files ... required for image manipulation in Python 3.9 (which is what the lambda function will be using) From the same folder, run <code>zip -r ../my-deployment-package.zip .</code> which will create a lambda function zip, containing all these files in the parent directory.  </p> <p>This zip will be the same zip which i link below, so if you do have any issues with the lambda function, you can use the one i've pre-created.</p>"},{"location":"serverless-on-aws/#stage-3-create-the-lambda-function","title":"Stage 3 - Create the Lambda Function","text":"<p>Move to the lambda console (https://console.aws.amazon.com/lambda/home?region=us-east-1#/functions) Click <code>Create Function</code> We're going to be <code>Authoring from Scratch</code> For <code>Function name</code> enter <code>pixelator</code> for <code>Runtime</code> select <code>Python 3.9</code> For <code>Architecture</code> select <code>x86_64</code> For <code>Permissions</code> expand <code>Change default execution role</code> pick <code>Use an existing role</code> and in the <code>Existing role</code> dropdown, pick <code>PixelatorRole</code> Then <code>Create Function</code> Close down any <code>notifcation</code> dialogues/popups Click <code>Upload from</code> and select <code>.zip file</code> Either 1, download this zip to your local machine (https://github.com/acantril/learn-cantrill-io-labs/blob/master/00-aws-simple-demos/aws-lambda-s3-events/01_LABSETUP/my-deployment-package.zip, click Download) or 2, locate the .zip you created yourself in the <code>Stage 3(pre)</code> above - they will be identical On the lambda screen, click <code>Upload</code> locate and select that .zip, and then click the <code>Save</code> button This upload will take a few minutes, but once complete you might see something saying <code>The deployment package of your Lambda function \"pixelator\" is too large to enable inline code editing. However, you can still invoke your function.</code> which is OK :)  </p>"},{"location":"serverless-on-aws/#stage-4-configure-the-lambda-function-trigger","title":"Stage 4 - Configure the Lambda Function &amp; Trigger","text":"<p>Click <code>Configuration</code> tab and then <code>Environment variables</code> We need to add an environment variable telling the pixelator function which processed bucket to use, it will know the source bucket because it's told about that in the event data. Click <code>Edit</code> then <code>Add environment variable</code>, under Key put <code>processed_bucket</code> and for <code>Value</code> put the bucket name of your processed bucket.  As an example <code>dontusethisname-processed</code>  (but use your bucket name) Be <code>really really really</code> sure you put your <code>processed</code> bucket here and NOT your source bucket. if you use the source bucket here, the output images will be stored in the source bucket, this will cause the lambda function to run over and over again ... bad be super-sure to put your processed bucket Click <code>Save</code> </p> <p>Click <code>General configuration</code> then click <code>Edit</code> and change the timeout to <code>1</code> minutes and <code>0</code> seconds, then click <code>Save</code> </p> <p>Click <code>Add trigger</code> In the dropdown pick <code>S3</code> Under <code>Bucket</code> pick your source bucket ... AGAIN be really really sure this is your source bucket and NOT your destination bucket and NOT any other bucket. Only pick your SOURCE bucket here. You will need to check the <code>Recursive invocation</code> acknowledgment box, this is because this lambda function is invoked every time anything is added to the source bucket, if you configure this wrongly, or configure the environment variable above wrongly ... it will run the lambda function over and over again for ever.  Once checked, click <code>Add</code> </p>"},{"location":"serverless-on-aws/#stage-5-test-and-monitor","title":"Stage 5 - Test and Monitor","text":"<p>open a tab to the <code>cloudwatch logs</code> console (https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logsV2:log-groups) make sure you have two tabs open to the <code>s3 console</code> (https://s3.console.aws.amazon.com/s3/home?region=us-east-1)  In one tab open your <code>-source</code> bucket &amp; in the other open the `-processed' bucket  </p> <p>In the <code>-source</code> bucket tab, make sure to select the <code>Objects</code> tab and click <code>Upload</code> Add some files and click <code>Upload</code>  (use your own, or these https://github.com/acantril/learn-cantrill-io-labs/tree/master/00-aws-simple-demos/aws-lambda-s3-events/01_LABSETUP/media) Once finished, click <code>Close</code> Move to the <code>CloudWatch Logs</code> tab Click the <code>Refresh</code> icon, locate and click <code>/aws/lambda/pixelator</code> If there is a log stream in there, click the most recent one, if not, keep clicking the <code>Refresh</code> icon and then click the most recent log stream Expand the line which begins with <code>{'Records': [{'eventVersion':</code> and you can see all of the event information about the lambda invocation, you should see the object name listed in <code>'object': {'key'</code> ... Go to the S3 Console tab for the <code>-processed</code> bucket Click the <code>Refresh</code> icon Select each of the pixelated versions of the image ... you should have 5 (<code>8x8</code>, <code>16x16</code>, <code>32x32</code>, <code>48x48</code> and <code>64x64</code>) Click <code>Open</code> You browser will either open or save all of the images Open them one by one, starting with <code>8x8</code> and finally <code>64x64</code> in order ... notice how they are the same image, but less and less pixelated :)  </p>"},{"location":"serverless-on-aws/#stage-6-cleanup","title":"Stage 6 - Cleanup","text":"<p>Open the <code>pixelator</code> lambda function (https://console.aws.amazon.com/lambda/home?region=us-east-1#/functions/pixelator?tab=code) Delete the function Move to the IAM Roles console (https://console.aws.amazon.com/iamv2/home#/roles)  Click <code>PixelatorRole</code>, then <code>Delete</code> the role, then confirm the deletion. Go to the <code>S3 Console</code> (https://s3.console.aws.amazon.com/s3/home?region=us-east-1&amp;region=us-east-1) For each of the <code>source</code> and <code>processed</code> buckets do:</p> <ul> <li>Select the bucket.  </li> <li>Click <code>Empty</code>.  </li> <li>Type <code>permanently delete</code>, and <code>Empty</code>.  </li> <li>Close the dialogue and move back to the main S3 Console.  </li> <li>Make sure the bucket is still selected, click <code>Delete</code>.   </li> <li>Type the name of the bucket then delete the bucket.  </li> </ul> <p>That's all folks! :)  </p>"},{"location":"terraform-on-aws/","title":"Multi-Tier Architecture on AWS using Terraform","text":"<p>Deploy a scalable and resilient multi-tier architecture on AWS using Terraform.</p>"},{"location":"terraform-on-aws/#project-overview","title":"\ud83d\ude80 Project Overview","text":"<p>This project allows us to deploy a highly available, scalable, and secure multi-tier architecture on Amazon Web Services (AWS) using Terraform. The architecture consists of the following three tiers:</p> <ul> <li> <p>Web Tier: This tier handles incoming user requests and can be horizontally scaled for increased capacity. It typically includes web servers and a load balancer for distributing traffic.</p> </li> <li> <p>Application Tier: Application servers run our business logic and interact with the database tier. They can also be horizontally scaled to meet demand.</p> </li> <li> <p>Database Tier: The database stores and manages our application data. In this architecture, we use Amazon RDS for a managed database service.</p> </li> </ul>"},{"location":"terraform-on-aws/#architecture-diagram","title":"\ud83d\udccc Architecture Diagram","text":""},{"location":"terraform-on-aws/#getting-started","title":"\ud83d\udea6 Getting Started","text":""},{"location":"terraform-on-aws/#prerequisites","title":"Prerequisites","text":"<p>Before you get started, make sure you have the following prerequisites in place:</p> <ul> <li>Terraform installed.</li> <li>AWS IAM credentials configured.</li> <li>Git for cloning the repository.</li> </ul>"},{"location":"terraform-on-aws/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ul> <li>Features</li> <li>Web Tier</li> <li>Application Tier</li> <li>Database Tier</li> <li>Terraform Configuration</li> <li>Deployment</li> <li>Usage</li> <li>Contributing</li> <li>License</li> </ul>"},{"location":"terraform-on-aws/#features","title":"\u2728 Features","text":"<ul> <li>High Availability: The architecture is designed for fault tolerance and redundancy.</li> <li>Scalability: Easily scale the web and application tiers to handle varying workloads.</li> <li>Security: Security groups and network ACLs are configured to ensure a secure environment.</li> </ul>"},{"location":"terraform-on-aws/#web-tier","title":"\ud83c\udf1f Web Tier","text":"<p>The Web Tier is the entry point for incoming user requests. It typically includes:</p> <ul> <li>Load Balancer: Distributes traffic across multiple web servers.</li> <li>Auto Scaling: Automatically adjusts the number of web servers based on traffic.</li> <li>Security Groups: Controls incoming and outgoing traffic to the web servers.</li> </ul>"},{"location":"terraform-on-aws/#web-tier-configuration","title":"Web Tier Configuration","text":"<ul> <li>Launch Template Configuration</li> <li>Load Balancer Configuration</li> <li>Auto Scaling Configuration</li> <li>Security Group Configuration of Load balancer</li> <li>Security Group Configuration of Auto Scaling Group</li> </ul>"},{"location":"terraform-on-aws/#application-tier","title":"\ud83d\ude80 Application Tier","text":"<p>The Application Tier hosts the application servers responsible for running business logic and interacting with the database tier. Key components include:</p> <ul> <li>Application Servers: These run your application code and can be horizontally scaled.</li> <li>Load Balancer: Distributes traffic to the application servers.</li> <li>Auto Scaling: Automatically adjusts the number of web servers based on traffic.</li> <li>Security Groups: Controls incoming and outgoing traffic to the application servers.</li> </ul>"},{"location":"terraform-on-aws/#application-tier-configuration","title":"Application Tier Configuration","text":"<ul> <li>Launch Template Configuration</li> <li>Load Balancer Configuration</li> <li>Auto Scaling Configuration</li> <li>Security Group Configuration of Load balancer</li> <li>Security Group Configuration of Auto Scaling Group</li> </ul>"},{"location":"terraform-on-aws/#database-tier","title":"\ud83d\udcbd Database Tier","text":"<p>The Database Tier stores and manages our application data. We use Amazon RDS for a managed database service. Key components include:</p> <ul> <li>Amazon RDS: A managed database service for MySQL/PostgreSQL/SQL Server databases.</li> <li>Security Groups: Control incoming and outgoing traffic to the database.</li> </ul>"},{"location":"terraform-on-aws/#database-tier-configuration","title":"Database Tier Configuration","text":"<ul> <li>DB Subnet group Configuration</li> <li>Amazon RDS Configuration</li> <li>Security Group Configuration</li> </ul>"},{"location":"terraform-on-aws/#terraform-configuration","title":"\ud83d\udd27 Terraform Configuration","text":"<p>The Terraform configuration for this project is organized into different and resources to create the necessary AWS infrastructure components. Key resources include:</p> <ul> <li>Virtual Private Cloud (VPC)</li> <li>Subnets and Route Tables</li> <li>Security Groups and Network ACLs</li> <li>Load Balancers</li> <li>Auto Scaling Groups</li> <li>RDS Database Instances</li> </ul>"},{"location":"terraform-on-aws/#deployment","title":"\ud83d\ude80 Deployment","text":"<p>Follow these steps to deploy the architecture:</p> <ol> <li>Clone the repository:</li> </ol> <p><code>bash    git clone https://github.com/mathesh-me/multi-tier-architecture-using-terraform.git</code></p> <ol> <li>Make changes as per your needs.</li> <li>Initialize Terraform and apply the configuration:    <code>terraform init</code></li> <li>Review the changes and confirm.</li> </ol>"},{"location":"terraform-on-aws/#usage","title":"\ud83d\udcbc Usage","text":""},{"location":"terraform-on-aws/#scaling","title":"Scaling","text":"<ul> <li>To scale the Web or Application Tier, use Auto Scaling configurations provided in the respective Terraform files. Adjust the desired capacity to match your scaling requirements.</li> </ul>"},{"location":"terraform-on-aws/#database-management","title":"Database Management","text":"<ul> <li>Access the Amazon RDS instance in the Database Tier to manage your data.</li> </ul>"},{"location":"terraform-on-aws/#load-balancing","title":"Load Balancing","text":"<ul> <li>Configure the load balancer in the Web and Application Tiers to distribute traffic evenly.</li> </ul>"},{"location":"terraform-on-aws/#security-considerations","title":"Security Considerations","text":"<ul> <li>Review and customize the security groups and network ACLs to meet your specific security requirements.</li> </ul>"},{"location":"terraform-on-aws/#contributing","title":"\ud83e\udd1d Contributing","text":"<p>Contributions are Welcome! Please read my Contributing Guidelines to get started with contributing to this project.</p>"},{"location":"terraform-on-aws/#license","title":"\ud83d\udcc4 License","text":"<p>This project is licensed under the MIT License.</p>"},{"location":"assets/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE1%20-%20Provision%20and%20Discuss%20Architecture/","title":"Advanced Demo - Web Identity Federation","text":"<p>In this advanced demo series you will be implementing a simple serverless application which uses Web Identity Federation. The application runs using the following technologies</p> <ul> <li>S3 for front-end application hosting</li> <li>Google API Project as an ID Provider</li> <li>Cognito and IAM Roles to swap Google Token for AWS credentials</li> </ul> <p>The application runs from a browser, gets the user to login using a Google ID and then loads all images from a private S3 bucket into a browser using presignedURLs.  </p> <p>This advanced demo consists of 5 stages :-  </p> <ul> <li>STAGE 1 : Provision the environment and review tasks &lt;= THIS STAGE </li> <li>STAGE 2 : Create Google API Project &amp; Client ID  </li> <li>STAGE 3 : Create Cognito Identity Pool  </li> <li>STAGE 4 : Update App Bucket &amp; Test Application  </li> <li>STAGE 5 : Cleanup the account  </li> </ul> <p> </p>"},{"location":"assets/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE1%20-%20Provision%20and%20Discuss%20Architecture/#video-guide","title":"Video Guide","text":"<p>Stage1 - Video Guide</p>"},{"location":"assets/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE1%20-%20Provision%20and%20Discuss%20Architecture/#stage-1a-login-to-an-aws-account","title":"STAGE 1A - Login to an AWS Account","text":"<p>Login to an AWS account using a user with admin privileges and ensure your region is set to <code>us-east-1</code> <code>N. Virginia</code> Click HERE to auto configure the infrastructure the app requires  Check the  <code>The following resource(s) require capabilities: [AWS::IAM::ManagedPolicy, AWS::IAM::Role]</code> box Click <code>Create Stack</code> </p> <p>Wait for the STACK to move into the <code>CREATE_COMPLETE</code> state before continuing.  </p>"},{"location":"assets/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE1%20-%20Provision%20and%20Discuss%20Architecture/#stage-1b-verify-s3-bucket","title":"STAGE 1B - Verify S3 bucket","text":"<p>Open the S3 console https://s3.console.aws.amazon.com/s3/home?region=us-east-1   Open the bucket starting <code>webidf-appbucket</code>  It should have objects within it, including <code>index.html</code> and <code>scripts.js</code> Click the <code>Permissions</code> Tab Verify <code>Block all public access</code> is set to <code>Off</code> Click <code>Bucket Policy</code> Verify there is a bucket policy  </p>"},{"location":"assets/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE1%20-%20Provision%20and%20Discuss%20Architecture/#stage-1c-verify-privatebucket","title":"STAGE 1C - Verify privatebucket","text":"<p>Open the bucket starting <code>webidf-patchesprivatebucket-</code> Load the objects in the bucket so you are aware of the contents Verify there is no bucket policy and the bucket is entirely private.  </p>"},{"location":"assets/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE1%20-%20Provision%20and%20Discuss%20Architecture/#stage-1d-verify-cloudformation-distribution","title":"STAGE 1D - Verify CloudFormation Distribution","text":"<p>Move to the <code>CloudFront</code> consle https://us-east-1.console.aws.amazon.com/cloudfront/v3/home?region=us-east-1#/distributions Locate the distribution pointing at origin <code>webidf-appbucket-....</code>  and click Locate the <code>distribution domain name</code> Note down as the <code>WebApp URL</code> this name prefixed with https i.e if yours is <code>d1o4f0w1ew0exo.cloudfront.net</code> then your <code>WebApp URL</code> is <code>https://https://d1o4f0w1ew0exo.cloudfront.net</code> (note, the copy icon may copy the https:// for you)  </p>"},{"location":"assets/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE1%20-%20Provision%20and%20Discuss%20Architecture/#stage-1-finish","title":"STAGE 1 - FINISH","text":"<p>At this stage you have the base infrastructure in place including:-</p> <ul> <li>front end app bucket</li> <li>privatepatches bucket  </li> <li>CloudFront distribution proving caching and HTTPS capability</li> </ul> <p>In stage 2 you will create a google API project which will be the <code>ID Provider</code> for this serverless application.  </p>"},{"location":"assets/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE2%20-%20Create%20Google%20APIProject%20and%20Client%20ID/","title":"Advanced Demo - Web Identity Federation","text":"<p>In this advanced demo series you will be implementing a simple serverless application which uses Web Identity Federation. The application runs using the following technologies  </p> <ul> <li>S3 for front-end application hosting  </li> <li>Google API Project as an ID Provider  </li> <li>Cognito and IAM Roles to swap Google Token for AWS credentials  </li> </ul> <p>The application runs from a browser, gets the user to login using a Google ID and then loads all images from a private S3 bucket into a browser using presignedURLs.  </p> <p>This advanced demo consists of 5 stages :-  </p> <ul> <li>STAGE 1 : Provision the environment and review tasks   </li> <li>STAGE 2 : Create Google API Project &amp; Client ID &lt;= THIS STAGE </li> <li>STAGE 3 : Create Cognito Identity Pool  </li> <li>STAGE 4 : Update App Bucket &amp; Test Application  </li> <li>STAGE 5 : Cleanup the account  </li> </ul> <p> </p>"},{"location":"assets/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE2%20-%20Create%20Google%20APIProject%20and%20Client%20ID/#video-guide","title":"Video Guide","text":"<p>Stage2 - Video Guide</p>"},{"location":"assets/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE2%20-%20Create%20Google%20APIProject%20and%20Client%20ID/#stage-2a-create-google-api-project","title":"STAGE 2A - Create Google API PROJECT","text":"<p>Any application that uses OAuth 2.0 to access Google APIs must have authorization credentials that identify the application to Google's OAuth 2.0 server In this stage we need to create those authorization credentials.  </p> <p>You will need a valid google login, GMAIL will do. If you don't have one, you will need to create one as part of this process. Move to the Google Credentials page https://console.developers.google.com/apis/credentials   Either sign in, or create a google account  </p> <p>You will be moved to the <code>Google API Console</code>  You may have to set your country and agree to some terms and conditions, thats fine go ahead and do that.   Click the <code>Select a project</code> dropdown, and then click <code>NEW PROJECT</code>  For project name enter <code>PetIDF</code> Click <code>Create</code> </p>"},{"location":"assets/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE2%20-%20Create%20Google%20APIProject%20and%20Client%20ID/#stage-2b-configure-consent-screen","title":"STAGE 2B - Configure Consent Screen","text":"<p>Click <code>Credentials</code> Click <code>CONFIGURE CONSENT SCREEN</code>  because our application will be usable by any google user, we have to select external users Check the box next to <code>External</code> and click <code>CREATE</code> Next you need to give the application a name ... enter <code>PetIDF</code> in the <code>App Name</code> box.  enter your own email in <code>user support email</code> enter your own email in <code>Developer contact information</code> Click <code>SAVE AND CONTINUE</code>  Click <code>SAVE AND CONTINUE</code> Click <code>SAVE AND CONTINUE</code> Click <code>BACK TO DASHBOARD</code> </p>"},{"location":"assets/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE2%20-%20Create%20Google%20APIProject%20and%20Client%20ID/#stage-2c-create-google-api-project-credentials","title":"STAGE 2C - Create Google API PROJECT CREDENTIALS","text":"<p>Click <code>Credentials</code> on the menu on the left  Click <code>CREATE CREDENTIALS</code> and then <code>OAuth client ID</code>  In the <code>Application type download</code> select <code>Web Application</code>  Under Name enter <code>PetIDFServerlessApp</code> </p> <p>We need to add the <code>WebApp URL</code>, this is the distribution domain name of the cloudfront distribution (making sure it has https:// before it) Click <code>ADD URI</code> under <code>Authorized JavaScript origins</code>  Enter the endpoint URL, you need to enter the <code>Distribution DNS Name</code> of your CloudFront distribution (created by the 1-click deployment), you should add https:// at the start,  it should look something like this <code>https://d38sv1tnkmk8i6.cloudfront.net</code> but you NEED to use your own distributions DNS name DONT USE THIS ONE Click <code>CREATE</code> </p> <p>You will be presented with two pieces of information  </p> <ul> <li><code>Client ID</code> </li> <li><code>Client Secret</code> </li> </ul> <p>Note down the <code>Client ID</code> you will need it later. You wont need the <code>Client Secret</code> again. Once noted down safely, click <code>OK</code> </p>"},{"location":"assets/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE2%20-%20Create%20Google%20APIProject%20and%20Client%20ID/#stage-2-finish","title":"STAGE 2 - FINISH","text":"<ul> <li>template front end app bucket</li> <li>Configured Google API Project</li> <li>Credentials to access it</li> </ul>"},{"location":"assets/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE3%20-%20Create%20Cognito%20Identity%20Pool/","title":"Advanced Demo - Web Identity Federation","text":"<p>In this advanced demo series you will be implementing a simple serverless application which uses Web Identity Federation.  The application runs using the following technologies  </p> <ul> <li>S3 for front-end application hosting  </li> <li>Google API Project as an ID Provider  </li> <li>Cognito and IAM Roles to swap Google Token for AWS credentials  </li> </ul> <p>The application runs from a browser, gets the user to login using a Google ID and then loads all images from a private S3 bucket into a browser using presignedURLs.  </p> <p>This advanced demo consists of 5 stages :-  </p> <ul> <li>STAGE 1 : Provision the environment and review tasks   </li> <li>STAGE 2 : Create Google API Project &amp; Client ID  </li> <li>STAGE 3 : Create Cognito Identity Pool &lt;= THIS STAGE </li> <li>STAGE 4 : Update App Bucket &amp; Test Application  </li> <li>STAGE 5 : Cleanup the account  </li> </ul> <p> </p>"},{"location":"assets/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE3%20-%20Create%20Cognito%20Identity%20Pool/#video-guide","title":"Video Guide","text":"<p>Stage3 - Video Guide</p>"},{"location":"assets/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE3%20-%20Create%20Cognito%20Identity%20Pool/#stage-3a-create-a-cognito-identity-pool","title":"STAGE 3A - CREATE A COGNITO IDENTITY POOL","text":"<p>Move to the Cognito Console https://console.aws.amazon.com/cognito/home?region=us-east-1# On the menu on the left, select <code>Federated Identities</code> We're going to be creating a new identity pool If this is your first, the creation process will begin immediatly, if you already have any identity pools you'll have to click <code>federated identities</code> then click on <code>Create new identity pool</code>  In <code>Identity pool name</code> enter <code>PetIDFIDPool</code>  Expand <code>Authentication Providers</code> and click on <code>Google+</code>  In the <code>Google Client ID</code> box, enter the Google Client ID you noted down in the previous step. Click <code>Create Pool</code> </p>"},{"location":"assets/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE3%20-%20Create%20Cognito%20Identity%20Pool/#stage-3b-permissions","title":"STAGE 3B - Permissions","text":"<p>Expand <code>View Details</code>  This is going to create two IAM roles One for <code>Your authenticated identities</code> and another for your <code>Your unauthenticated identities</code>  For now, we're just going to click on <code>Allow</code> we can review the roles later.    </p> <p>You will be presented with your <code>Identity Pool ID</code>, note this down, you will need it later. Click to move back to the dashboard  </p>"},{"location":"assets/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE3%20-%20Create%20Cognito%20Identity%20Pool/#stage-3c-adjust-permissions","title":"STAGE 3C - Adjust Permissions","text":"<p>The serverless application is going to read images out of a private bucket created by the initial cloudformation template.   The bucket is called <code>patchesprivatebucket</code>  Move to the IAM Console https://console.aws.amazon.com/iam/home?region=us-east-1#/home   Click <code>Roles</code>  Locate and click on <code>Cognito_PetIDFIDPoolAuth_Role</code> Click on <code>Trust Relationships</code> See how this is assumable by <code>cognito-identity.amazonaws.com</code> With two conditions - <code>StringEquals</code> <code>cognito-identity.amazonaws.com:aud</code> <code>your congnito ID pool</code> - <code>ForAnyValue:StringLike</code> <code>cognito-identity.amazonaws.com:amr</code> <code>authenticated</code> This means to assume this role - you have to be authenticated by one of the ID providers defined in the cognito ID pool.    </p> <p>When you use WEDIDF with cognito, this role is assumed on your behalf by cognito, and its what generates temporary AWS credentials which are used to access AWS resources.  </p> <p>Click <code>permissions</code> .. this defines what these credentials can do.  </p> <p>The cloudformation template created a managed policy which can access the <code>privatepatches</code> bucket Click <code>Add permissions</code> and then <code>Attach policies</code> Type <code>PrivatePatches</code> in the search box and press <code>enter</code> Check the box next to <code>PrivatePatchesPermissions</code> and click <code>Attach Policies</code> </p>"},{"location":"assets/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE3%20-%20Create%20Cognito%20Identity%20Pool/#stage-3-finish","title":"STAGE 3 - FINISH","text":"<ul> <li>template front end app bucket  </li> <li>Configured Google API Project  </li> <li>Credentials to access it  </li> <li>Cognito ID Pool  </li> <li>IAM Roles for the ID Pool  </li> </ul>"},{"location":"assets/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE4%20-%20Update%20App%20Bucket%20and%20Test%20Application/","title":"Advanced Demo - Web Identity Federation","text":"<p>In this advanced demo series you will be implementing a simple serverless application which uses Web Identity Federation.   The application runs using the following technologies  </p> <ul> <li>S3 for front-end application hosting  </li> <li>Google API Project as an ID Provider  </li> <li>Cognito and IAM Roles to swap Google Token for AWS credentials  </li> </ul> <p>The application runs from a browser, gets the user to login using a Google ID and then loads all images from a private S3 bucket into a browser using presignedURLs.  </p> <p>This advanced demo consists of 5 stages :-  </p> <ul> <li>STAGE 1 : Provision the environment and review tasks   </li> <li>STAGE 2 : Create Google API Project &amp; Client ID  </li> <li>STAGE 3 : Create Cognito Identity Pool  </li> <li>STAGE 4 : Update App Bucket &amp; Test Application &lt;= THIS STAGE </li> <li>STAGE 5 : Cleanup the account  </li> </ul> <p> </p>"},{"location":"assets/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE4%20-%20Update%20App%20Bucket%20and%20Test%20Application/#video-guide","title":"Video Guide","text":"<p>Stage4 - Video Guide</p>"},{"location":"assets/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE4%20-%20Update%20App%20Bucket%20and%20Test%20Application/#stage-4a-download-indexhtml-and-scriptsjs-from-the-s3","title":"STAGE 4A - Download index.html and scripts.js from the S3","text":"<p>Move to the S3 Console https://s3.console.aws.amazon.com/s3/home?region=us-east-1   Open the <code>webidf-appbucket-</code> bucket  select <code>index.html</code> and click <code>Download</code> &amp; save the file locally select <code>scripts.js</code> and click <code>Download</code> &amp; save the file locally  </p>"},{"location":"assets/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE4%20-%20Update%20App%20Bucket%20and%20Test%20Application/#stage-4b-update-files-with-your-specific-connection-information","title":"STAGE 4B - Update files with your specific connection information","text":"<p>Open the local copy of <code>index.html</code> in a code editor.   Locate the <code>REPLACE_ME_GOOGLE_APP_CLIENT_ID</code> placeholder  Replace this with YOUR CLIENT ID Save <code>index.html</code> </p> <p>Open the local copy of <code>scripts.js</code> in a code editor.  Locate the IdentityPoolId: <code>REPLACE_ME_COGNITO_IDENTITY_POOL_ID</code> placeholder   Replace the <code>REPLACE_ME_COGNITO_IDENTITY_POOL_ID</code> part with your IDENTITY POOL ID you noted down in the previous step Locate the <code>Bucket: \"REPLACE_ME_NAME_OF_PATCHES_PRIVATE_BUCKET\"</code> placeholder. Replace <code>REPLACE_ME_NAME_OF_PATCHES_PRIVATE_BUCKET</code> with with bucket name of the <code>webidf-patchesprivatebucket-</code> bucket Save <code>scripts.js</code> </p>"},{"location":"assets/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE4%20-%20Update%20App%20Bucket%20and%20Test%20Application/#stage-4c-upload-files","title":"STAGE 4C - Upload files","text":"<p>Back on the S3 console, inside the <code>webidf-appbucket-</code> bucket.  Click <code>Upload</code>  Add the <code>index.html</code> and <code>scripts.js</code> files and click <code>Upload</code> </p>"},{"location":"assets/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE4%20-%20Update%20App%20Bucket%20and%20Test%20Application/#stage-4c-test-application","title":"STAGE 4C - Test application","text":"<p>Open the <code>WebApp URL</code> you noted down earlier, the <code>distribution domain name</code> of the cloudfront distribution This is the web app, with no access to any AWS resources right now Open your browser <code>web developer tools</code> (firefox tool-&gt;browser tools-&gt; web developer tools) it might be called browser console in other browsers, it will log any output from javascript running in your web browser. With the browser console open, Click <code>Sign In</code>  Sign in with your google account  </p> <p>When you click the Sign In button a few things happen:-  (watch the console)  </p> <ul> <li>You authenticate with the Google IDP  </li> <li>a Google Access token is returned  </li> <li>This token is provided as part of the API Call to Cognito  </li> <li>If successful this exchanges this for Temporary AWS credentials  </li> <li>These are used to list objects in the private bucket  </li> <li>for all objects, presignedURLs are generated and used to load the images in the browser.  </li> </ul> <p>Once signed in you should see 3 cat pictures loaded from a private S3 bucket  </p> <p>Click on each of them, notice the URL which is used? it's a presignedURL generated by the JS running in browser, using the API's which you can access using the cognito credentials.  </p> <p>All of this is done with no self-managed compute.</p>"},{"location":"assets/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE4%20-%20Update%20App%20Bucket%20and%20Test%20Application/#caching-issue","title":"Caching Issue","text":"<p>If you're having issues signing in, check that:   - you have uploaded the <code>index.html</code> and <code>scripts.js</code> with correct credentials   - your CloudFront distribution has finished deploying</p> <p>If you're still having issues signing in after that, the CloudFront distribution is caching the old <code>index.html</code> and <code>scripts.js</code> files. To fix this, you can invalidate the cache by following these steps:   - Go to the CloudFront console   - Select the CloudFront distribution that you created   - Click on the <code>Invalidations</code> tab   - Click on <code>Create invalidation</code>   - Enter <code>/index.html</code> and <code>/scripts.js</code> in the <code>Object Paths</code> field   - Click on <code>Create invalidation</code></p>"},{"location":"assets/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE4%20-%20Update%20App%20Bucket%20and%20Test%20Application/#stage-4-finish","title":"STAGE 4 - FINISH","text":"<p>At this point you have a fully functional simple serverless application, complete with :-</p> <ul> <li>template front end app bucket</li> <li>Configured Google API Project</li> <li>Credentials to access it</li> <li>Cognito ID Pool</li> <li>IAM Roles for the ID Pool</li> <li>HTML and JS configured to access the Google IDP and Cognito</li> </ul>"},{"location":"assets/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE5%20-%20Cleanup/","title":"Advanced Demo - Web Identity Federation","text":"<p>In this advanced demo series you will be implementing a simple serverless application which uses Web Identity Federation.   The application runs using the following technologies  </p> <p>This advanced demo consists of 5 stages :-</p> <ul> <li>STAGE 1 : Provision the environment and review tasks </li> <li>STAGE 2 : Create Google APIProject &amp; ClientID </li> <li>STAGE 3 : Create Cognito Identity Pool</li> <li>STAGE 4 : Update App Bucket &amp; Test Application </li> <li>STAGE 5 : Cleanup the account &lt;= THIS STAGE</li> </ul>"},{"location":"assets/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE5%20-%20Cleanup/#video-guide","title":"Video Guide","text":"<p>Stage5 - Video Guide</p>"},{"location":"assets/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE5%20-%20Cleanup/#stage-5a-delete-the-google-api-project-credentials","title":"STAGE 5A - Delete the Google API Project &amp; Credentials","text":"<p>https://console.developers.google.com/cloud-resource-manager  Select <code>PetIDF</code> and click <code>DELETE</code> Type in the ID of the project, which might have a slightly different name (shown above the text box) click <code>Shut Down</code> </p>"},{"location":"assets/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE5%20-%20Cleanup/#stage-5b-delete-the-cognito-id-pool","title":"STAGE 5B - Delete the Cognito ID Pool","text":"<p>Move to the cognito console https://console.aws.amazon.com/cognito/home?region=us-east-1 Click <code>Federated Identities</code> Click on <code>PetIDFIDPool</code> Click <code>Edit Identity Pool</code> Locate and expand <code>Delete identity pool</code> Click <code>Delete Identity Pool</code> Click <code>Delete Pool</code> </p>"},{"location":"assets/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE5%20-%20Cleanup/#stage-5c-delete-the-iam-roles","title":"STAGE 5c - Delete the IAM Roles","text":"<p>Move to the IAM Console https://console.aws.amazon.com/iam/home?region=us-east-1#/home Select <code>Roles</code> Select both <code>Cognito_PetIDF*</code> roles Click <code>Delete Role</code> Click <code>Yes Delete</code> </p>"},{"location":"assets/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE5%20-%20Cleanup/#stage-5c-delete-the-cloudformation-stack","title":"STAGE 5C - Delete the CloudFormation Stack","text":"<p>Move to the cloud formation console https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks?filteringText=&amp;filteringStatus=active&amp;viewNested=true&amp;hideStacks=false Select <code>WEBIDF</code>, click <code>Delete</code> then <code>Delete Stack</code> </p>"},{"location":"assets/aws-cognito-web-identity-federation/02_LABINSTRUCTIONS/STAGE5%20-%20Cleanup/#stage-5-finish","title":"STAGE 5 - FINISH","text":"<ul> <li>template front end app bucket</li> <li>Configured Google API Project</li> <li>Credentials to access it</li> <li>Cognito ID Pool</li> <li>IAM Roles for the ID Pool</li> </ul>"}]}